{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ì§€ì—­ë‚œë°© ì—´ìˆ˜ìš” ì˜ˆì¸¡: ì™„ì „í•œ ê³ ë„í™”ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”\n",
        "\n",
        "## ëª¨ë¸ë§ ì „ëµ\n",
        "- **êµ¬ì„±**: 3ê°œ ê·œëª¨ ê·¸ë£¹ Ã— 2ê°œ ê³„ì ˆ = 6ê°œ ëª¨ë¸\n",
        "- **ìŠ¤íƒœí‚¹**: Prophet + CatBoost + LSTM + Ridge ë©”íƒ€ëª¨ë¸\n",
        "- **ìµœì í™”**: ëª¨ë“  ëª¨ë¸ì— Optuna + 3-Fold CV\n",
        "- **ì´ ëª¨ë¸**: 18ê°œ + 6ê°œ ë©”íƒ€ëª¨ë¸ = 24ê°œ\n",
        "- **ì¬í˜„ì„±**: ì™„ì „í•œ ì‹œë“œ ê³ ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "setup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\n"
          ]
        }
      ],
      "source": [
        "# Google Colab í™˜ê²½ í™•ì¸ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\")\n",
        "    !pip install catboost prophet torch optuna statsmodels holidays pmdarima scikit-learn==1.3.0 --quiet\n",
        "    from google.colab import files, drive\n",
        "    print(\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# ì™„ì „í•œ ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "import holidays\n",
        "import json\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# Prophet/Stan ë¡œê·¸ ì™„ì „ ì°¨ë‹¨\n",
        "os.environ['CMDSTAN_LOGGER'] = 'ERROR'\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# íŠ¹ì • ë¡œê±°ë“¤ ë¹„í™œì„±í™”\n",
        "for logger_name in ['prophet', 'cmdstanpy', 'pystan']:\n",
        "    logging.getLogger(logger_name).setLevel(logging.CRITICAL)\n",
        "    logging.getLogger(logger_name).disabled = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "imports2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë””ë°”ì´ìŠ¤: cpu\n",
            "ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ! (ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± ë³´ì¥)\n"
          ]
        }
      ],
      "source": [
        "# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, KFold\n",
        "import catboost as cb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Prophet\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Prophet ì„¤ì¹˜ í•„ìš”\")\n",
        "    Prophet = None\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# ARIMA\n",
        "try:\n",
        "    from pmdarima import auto_arima\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "except ImportError:\n",
        "    print(\"pmdarima ì„¤ì¹˜ í•„ìš”\")\n",
        "    auto_arima = None\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ! (ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± ë³´ì¥)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Huber Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# Huber Loss í•¨ìˆ˜ ì •ì˜\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"\n",
        "    Huber Loss ê³„ì‚°\n",
        "    deltaë³´ë‹¤ ì‘ì€ ì˜¤ì°¨ì—ëŠ” ì œê³± ì†ì‹¤, í° ì˜¤ì°¨ì—ëŠ” ì„ í˜• ì†ì‹¤ ì ìš©\n",
        "    \"\"\"\n",
        "    residual = np.abs(y_true - y_pred)\n",
        "    condition = residual <= delta\n",
        "    squared_loss = 0.5 * residual**2\n",
        "    linear_loss = delta * residual - 0.5 * delta**2\n",
        "    return np.where(condition, squared_loss, linear_loss).mean()\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"RMSEì™€ Huber Lossë¥¼ ë™ì‹œì— ê³„ì‚°\"\"\"\n",
        "    # RMSE ê³„ì‚°\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    \n",
        "    # Huber Loss ê³„ì‚°\n",
        "    residual = np.abs(y_true - y_pred)\n",
        "    condition = residual <= delta\n",
        "    squared_loss = 0.5 * residual**2\n",
        "    linear_loss = delta * residual - 0.5 * delta**2\n",
        "    huber = np.where(condition, squared_loss, linear_loss).mean()\n",
        "    \n",
        "    return {'rmse': rmse, 'huber': huber}\n",
        "\n",
        "def huber_score(y_true, y_pred, delta=1.0):\n",
        "    \"\"\"ìµœì í™”ìš© Huber Score (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)\"\"\"\n",
        "    return evaluate_predictions(y_true, y_pred, delta)['huber']\n",
        "\n",
        "print(\"Huber Loss í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_load"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° íŒŒì¼ ë¡œë“œ\n",
        "if IN_COLAB:\n",
        "    print(\"íŒŒì¼ ì—…ë¡œë“œ ë°©ë²• ì„ íƒ:\")\n",
        "    print(\"1. ì§ì ‘ ì—…ë¡œë“œ\")\n",
        "    print(\"2. Google Drive\")\n",
        "\n",
        "    method = input(\"ì„ íƒ (1 ë˜ëŠ” 2): \")\n",
        "\n",
        "    if method == \"1\":\n",
        "        uploaded = files.upload()\n",
        "        files_list = list(uploaded.keys())\n",
        "        train_path = [f for f in files_list if 'train' in f.lower()][0]\n",
        "        test_path = [f for f in files_list if 'test' in f.lower()][0]\n",
        "    else:\n",
        "        drive.mount('/content/drive')\n",
        "        train_path = \"/content/drive/MyDrive/train_heat.csv\"\n",
        "        test_path = \"/content/drive/MyDrive/test_heat.csv\"\n",
        "else:\n",
        "    train_path = '/dataset/train_heating.csv' ## ê²½ë¡œ ìˆ˜ì • í•„ìš”\n",
        "    test_path = '/dataset/test_heating.csv' ## ê²½ë¡œ ìˆ˜ì • í•„ìš”\n",
        "\n",
        "print(f\"íŒŒì¼ ê²½ë¡œ ì„¤ì • ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 1. ê³ ë„í™”ëœ ë°ì´í„° (ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ ìƒì„±)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "preprocess_func"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_advanced(train_path, test_path):\n",
        "    print(\"ê³ ë„í™”ëœ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\")\n",
        "\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    def process_df_advanced(df):\n",
        "        if 'Unnamed: 0' in df.columns:\n",
        "            df = df.drop(columns=['Unnamed: 0'])\n",
        "        df.columns = [col.replace('train_heat.', '') for col in df.columns]\n",
        "\n",
        "        if df['tm'].dtype == 'object':\n",
        "            df['tm'] = pd.to_datetime(df['tm'])\n",
        "        else:\n",
        "            df['tm'] = pd.to_datetime(df['tm'], format='%Y%m%d%H')\n",
        "        \n",
        "        df['year'] = df['tm'].dt.year\n",
        "        df['month'] = df['tm'].dt.month\n",
        "        df['day'] = df['tm'].dt.day\n",
        "        df['hour'] = df['tm'].dt.hour\n",
        "        df['dayofweek'] = df['tm'].dt.dayofweek\n",
        "        df['dayofyear'] = df['tm'].dt.dayofyear\n",
        "\n",
        "        # âœ… wd (í’í–¥) ì œì™¸, ì‚¬ìš©í•  ì»¬ëŸ¼ë§Œ í¬í•¨\n",
        "        missing_cols = ['ta', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand'] # ì—´ìˆ˜ìš”ë„ ê²°ì¸¡ì¹˜ ìˆìŒ\n",
        "\n",
        "        # âœ… 1ë‹¨ê³„: ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ ìƒì„± (NaN ë³€í™˜ ì „ì—)\n",
        "        print(\"   ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ ìƒì„± ì¤‘...\")\n",
        "        for col in missing_cols:\n",
        "            if col in df.columns:\n",
        "                # -99ë¥¼ ê²°ì¸¡ì¹˜ë¡œ ì¸ì‹í•˜ì—¬ í”Œë˜ê·¸ ìƒì„±\n",
        "                missing_mask = (df[col] == -99)\n",
        "                df[f'{col}_missing'] = missing_mask.astype(int)\n",
        "        \n",
        "        # âœ… 2ë‹¨ê³„: ê²°ì¸¡ì¹˜ë¥¼ NaNìœ¼ë¡œ ë³€í™˜\n",
        "        for col in missing_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].replace(-99, np.nan)\n",
        "\n",
        "        # âœ… wd ì»¬ëŸ¼ ì²˜ë¦¬: -9.9 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ # SVR ë³´ê°„ì— í™œìš©\n",
        "        if 'wd' in df.columns:\n",
        "            df['wd'] = df['wd'].replace(-9.9, np.nan)\n",
        "            print(\"   wd (í’í–¥) ì»¬ëŸ¼ì˜ -9.9 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ë¨\")\n",
        "            \n",
        "        # ì¼ì‚¬ëŸ‰ ì•¼ê°„ì€ 0 ì²˜ë¦¬\n",
        "        if 'si' in df.columns:\n",
        "            night_mask = (df['hour'] < 8) | (df['hour'] > 18)\n",
        "            df.loc[night_mask & df['si'].isna(), 'si'] = 0\n",
        "\n",
        "        df = df.sort_values(['branch_id', 'tm'])\n",
        "        \n",
        "        # âœ… 3ë‹¨ê³„: ìƒì„±ëœ ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ í™•ì¸\n",
        "        missing_flag_cols = [col for col in df.columns if col.endswith('_missing')]\n",
        "        print(f\"   ìƒì„±ëœ ê²°ì¸¡ì¹˜ í”Œë˜ê·¸: {missing_flag_cols}\")\n",
        "        for col in missing_flag_cols:\n",
        "            missing_count = df[col].sum()\n",
        "            print(f\"     {col}: {missing_count:,}ê°œ ê²°ì¸¡ì¹˜\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    train_df = process_df_advanced(train_df)\n",
        "    test_df = process_df_advanced(test_df)\n",
        "\n",
        "    print(f\"   í›ˆë ¨: {train_df.shape}, í…ŒìŠ¤íŠ¸: {test_df.shape}\")\n",
        "    print(f\"   ê¸°ê°„: {train_df['tm'].min()} ~ {test_df['tm'].max()}\")\n",
        "\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "features"
      },
      "source": [
        "## 2-1. Feature ìƒì„± : ì‹œì¦Œë³„ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„± (ë„ë©”ì¸ íŠ¹í™”) _ ì˜¨ë„, í’ì†, ê°•ìˆ˜ëŸ‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "outlier_flags"
      },
      "outputs": [],
      "source": [
        "def create_weather_outlier_flags(train_df, test_df):\n",
        "    \"\"\"ì‹œì¦Œë³„ ê¸°ìƒë°ì´í„° ê¸°ë°˜ ì´ìƒì¹˜ í”Œë˜ê·¸ (TRAIN ê¸°ì¤€ ì ìš©)\"\"\"\n",
        "    print(\"ì‹œì¦Œë³„ ê¸°ìƒ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„± ì¤‘ (TRAIN ê¸°ì¤€)...\")\n",
        "    \n",
        "    # 1ë‹¨ê³„: TRAIN ë°ì´í„°ì—ì„œ ì‹œì¦Œë³„, ì§€ì‚¬ë³„ ì„ê³„ê°’ ê³„ì‚°\n",
        "    outlier_thresholds = {}\n",
        "    \n",
        "    for branch in train_df['branch_id'].unique():\n",
        "        branch_data = train_df[train_df['branch_id'] == branch]\n",
        "        outlier_thresholds[branch] = {}\n",
        "        \n",
        "        # ì‹œì¦Œë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì„ê³„ê°’ ê³„ì‚°\n",
        "        for season in [0, 1]:  # 0: ë¹„ë‚œë°©ì² , 1: ë‚œë°©ì² \n",
        "            season_data = branch_data[branch_data['heating_season'] == season]\n",
        "            \n",
        "            if len(season_data) > 10:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰\n",
        "                outlier_thresholds[branch][season] = {\n",
        "                    # ğŸŒ¡ï¸ ì˜¨ë„: í•˜ìœ„ 10% (ê·¹í•œ ì¶”ìœ„)\n",
        "                    'ta_q10': season_data['ta'].quantile(0.10),\n",
        "                    # ğŸ’¨ í’ì†: ìƒìœ„ 10% (ê°•í’)\n",
        "                    'ws_q90': season_data['ws'].quantile(0.90),\n",
        "                    # ğŸŒ§ï¸ ì¼ê°•ìˆ˜ëŸ‰: ìƒìœ„ 10% (í­ìš°)\n",
        "                    'rn_day_q90': season_data['rn_day'].quantile(0.90)\n",
        "                }\n",
        "                print(f\"   ì§€ì‚¬ {branch}, {'ë‚œë°©ì² ' if season else 'ë¹„ë‚œë°©ì² '}: ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\")\n",
        "    \n",
        "    # 2ë‹¨ê³„: ì„ê³„ê°’ì„ TRAINê³¼ TESTì— ì ìš©\n",
        "    def apply_weather_thresholds(df, thresholds):\n",
        "        df = df.copy()\n",
        "        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "        df['cold_extreme'] = 0      # ê·¹í•œ ì¶”ìœ„ (í•˜ìœ„ 10%)\n",
        "        df['strong_wind'] = 0       # ê°•í’ (ìƒìœ„ 10%)\n",
        "        df['heavy_rain'] = 0        # í­ìš° (ìƒìœ„ 10%)\n",
        "        \n",
        "        for branch in df['branch_id'].unique():\n",
        "            if branch in thresholds:\n",
        "                branch_mask = df['branch_id'] == branch\n",
        "                \n",
        "                # ì‹œì¦Œë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ ì ìš©\n",
        "                for season in [0, 1]:  # 0: ë¹„ë‚œë°©ì² , 1: ë‚œë°©ì² \n",
        "                    if season in thresholds[branch]:\n",
        "                        season_mask = branch_mask & (df['heating_season'] == season)\n",
        "                        season_thresholds = thresholds[branch][season]\n",
        "                        \n",
        "                        # ì˜¨ë„ ì´ìƒì¹˜ (ë‚®ì€ ì˜¨ë„)\n",
        "                        df.loc[season_mask, 'cold_extreme'] = (\n",
        "                            df.loc[season_mask, 'ta'] < season_thresholds['ta_q10']\n",
        "                        ).astype(int)\n",
        "                        \n",
        "                        # í’ì† ì´ìƒì¹˜ (ë†’ì€ í’ì†)\n",
        "                        df.loc[season_mask, 'strong_wind'] = (\n",
        "                            df.loc[season_mask, 'ws'] > season_thresholds['ws_q90']\n",
        "                        ).astype(int)\n",
        "                        \n",
        "                        # ê°•ìˆ˜ëŸ‰ ì´ìƒì¹˜ (ë§ì€ ë¹„)\n",
        "                        df.loc[season_mask, 'heavy_rain'] = (\n",
        "                            df.loc[season_mask, 'rn_day'] > season_thresholds['rn_day_q90']\n",
        "                        ).astype(int)\n",
        "                        \n",
        "        return df\n",
        "    \n",
        "    # TRAIN ì ìš©\n",
        "    train_result = apply_weather_thresholds(train_df, outlier_thresholds)\n",
        "    \n",
        "    # TEST ì ìš©\n",
        "    test_result = apply_weather_thresholds(test_df, outlier_thresholds)\n",
        "    \n",
        "    print(f\"   ê¸°ìƒ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„± ì™„ë£Œ: {len(outlier_thresholds)}ê°œ ì§€ì‚¬\")\n",
        "    \n",
        "    return train_result, test_result, outlier_thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-2. Feature ìƒì„± : ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "advanced_features"
      },
      "outputs": [],
      "source": [
        "def create_advanced_features(df, season_type=\"heating\"):\n",
        "    df = df.copy()\n",
        "    print(f\"{season_type} ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
        "    \n",
        "    # ë²”ì£¼í˜• ì‹œê°„ ë³€ìˆ˜ (ë¬¸ìì—´ë¡œ ëª…ì‹œì  ë³€í™˜)\n",
        "    df['hour_cat'] = df['hour'].astype(str)\n",
        "    df['month_cat'] = df['month'].astype(str)\n",
        "    df['weekday_name'] = df['dayofweek'].map(\n",
        "        lambda x: ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][x]\n",
        "    ).astype(str)\n",
        "    \n",
        "    # ìˆœí™˜ ì¸ì½”ë”©\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
        "    df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
        "    \n",
        "    # ì‹œì¦Œë³„ ì›” ìˆœí™˜ (sin, cos í¬í•¨)\n",
        "    if season_type == \"heating\":\n",
        "        heating_months = {10:0, 11:1, 12:2, 1:3, 2:4, 3:5, 4:6}\n",
        "        df['heating_month_order'] = df['month'].map(heating_months)\n",
        "        df['heating_month_sin'] = np.sin(2 * np.pi * df['heating_month_order'] / 7)\n",
        "        df['heating_month_cos'] = np.cos(2 * np.pi * df['heating_month_order'] / 7)\n",
        "    else:\n",
        "        non_heating_months = {5:0, 6:1, 7:2, 8:3, 9:4}\n",
        "        df['non_heating_month_order'] = df['month'].map(non_heating_months)\n",
        "        df['non_heating_month_sin'] = np.sin(2 * np.pi * df['non_heating_month_order'] / 5)\n",
        "        df['non_heating_month_cos'] = np.cos(2 * np.pi * df['non_heating_month_order'] / 5)\n",
        "    \n",
        "    # ë¸Œëœì¹˜ ID (ë¬¸ìì—´ë¡œ ë³€í™˜)\n",
        "    df['branch_id'] = df['branch_id'].astype(str)\n",
        "    \n",
        "    # ê³ ê¸‰ ê¸°ìƒ ë²”ì£¼í˜• ë³€ìˆ˜\n",
        "    df['temp_category'] = 'Normal'\n",
        "    df.loc[df['ta'] < -10, 'temp_category'] = 'VeryCold'\n",
        "    df.loc[(df['ta'] >= -10) & (df['ta'] < 0), 'temp_category'] = 'Cold'\n",
        "    df.loc[(df['ta'] >= 0) & (df['ta'] < 10), 'temp_category'] = 'Cool'\n",
        "    df.loc[(df['ta'] >= 10) & (df['ta'] < 25), 'temp_category'] = 'Normal'\n",
        "    df.loc[df['ta'] >= 25, 'temp_category'] = 'Hot'\n",
        "    df['temp_category'] = df['temp_category'].astype(str)\n",
        "    \n",
        "    if season_type == \"heating\":\n",
        "        df['cold_warning_level'] = 'Normal'\n",
        "        df.loc[df['ta'] <= -12, 'cold_warning_level'] = 'ColdAdvisory'\n",
        "        df.loc[df['ta'] <= -15, 'cold_warning_level'] = 'ColdWarning'\n",
        "        df['cold_warning_level'] = df['cold_warning_level'].astype(str)\n",
        "    \n",
        "    df['wind_category'] = 'Weak'\n",
        "    df.loc[df['ws'] >= 5.0, 'wind_category'] = 'Moderate'\n",
        "    df.loc[df['ws'] >= 10.0, 'wind_category'] = 'Strong'\n",
        "    df['wind_category'] = df['wind_category'].astype(str)\n",
        "    \n",
        "    # ê³µíœ´ì¼/í”¼í¬ì‹œê°„\n",
        "    kr_holidays = holidays.KR()\n",
        "    df['is_holiday'] = df['tm'].dt.date.apply(lambda x: x in kr_holidays)\n",
        "    df['holiday_type'] = df['is_holiday'].map({False: 'Weekday', True: 'Holiday'}).astype(str)\n",
        "    \n",
        "    df['peak_time'] = 'Normal'\n",
        "    df.loc[(df['hour'] >= 0) & (df['hour'] <= 6), 'peak_time'] = 'Dawn'\n",
        "    df.loc[(df['hour'] > 6) & (df['hour'] <= 11), 'peak_time'] = 'Morning'\n",
        "    df.loc[(df['hour'] > 11) & (df['hour'] <= 18), 'peak_time'] = 'Afternoon'\n",
        "    df.loc[(df['hour'] > 18) & (df['hour'] <= 23), 'peak_time'] = 'Evening'\n",
        "    df['peak_time'] = df['peak_time'].astype(str)\n",
        "    \n",
        "    # ê³ ê¸‰ ìˆ˜ì¹˜í˜• íŠ¹ì„±\n",
        "    df['HDD18'] = np.maximum(0, 18 - df['ta'])\n",
        "    # df['HDD20'] = np.maximum(0, 20 - df['ta'])\n",
        "    \n",
        "    # apparent_tempëŠ” ë‚œë°©ì‹œì¦Œì—ë§Œ ìƒì„±\n",
        "    if season_type == \"heating\":\n",
        "        def calculate_apparent_temp(ta, ws):\n",
        "            winter_at = 13.12 + 0.6215 * ta - 11.37 * (ws * 3.6)**0.16 + 0.3965 * ta * (ws * 3.6)**0.16\n",
        "            return winter_at\n",
        "        \n",
        "        df['apparent_temp'] = calculate_apparent_temp(df['ta'], df['ws'])\n",
        "        df['apparent_temp'] = df['apparent_temp'].fillna(0)\n",
        "    \n",
        "    for lag in [3, 6, 24]:\n",
        "        df[f'ta_lag_{lag}h'] = df.groupby('branch_id')['ta'].shift(lag).fillna(0) # ì´ˆê¸° ë¹„ì–´ìˆëŠ” ê°’ì€ 0ìœ¼ë¡œ ë°˜ì˜\n",
        "    \n",
        "    for window in [6, 12, 24]:\n",
        "        df[f'ta_ma_{window}h'] = df.groupby('branch_id')['ta'].transform(\n",
        "            lambda x: x.rolling(window, min_periods=1).mean()\n",
        "        )\n",
        "    \n",
        "    df['ta_diff_3h'] = df.groupby('branch_id')['ta'].diff(3)\n",
        "    df['ta_diff_6h'] = df.groupby('branch_id')['ta'].diff(6)\n",
        "    # diff ë³€ìˆ˜ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ìš°ê¸°\n",
        "    df['ta_diff_3h'] = df['ta_diff_3h'].fillna(0)\n",
        "    df['ta_diff_6h'] = df['ta_diff_6h'].fillna(0)\n",
        "    \n",
        "    daily_stats = df.groupby(['branch_id', df['tm'].dt.date]).agg({\n",
        "        'ta': ['min', 'max', 'mean']\n",
        "    }).round(2)\n",
        "    daily_stats.columns = ['daily_ta_min', 'daily_ta_max', 'daily_ta_mean']\n",
        "    daily_stats['daily_temp_range'] = daily_stats['daily_ta_max'] - daily_stats['daily_ta_min']\n",
        "    \n",
        "    df = df.merge(\n",
        "        daily_stats.reset_index(),\n",
        "        left_on=['branch_id', df['tm'].dt.date],\n",
        "        right_on=['branch_id', 'tm'],\n",
        "        how='left',\n",
        "        suffixes=('', '_daily')\n",
        "    )\n",
        "    \n",
        "    print(f\"   {season_type} ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {df.shape[1]}ê°œ ì»¬ëŸ¼\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-3. ê²°ì¸¡ì¹˜ ë³´ê°„ (Branchë³„ SVR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def apply_svr_interpolation(df, is_train=True):\n",
        "    \"\"\"SVR ê¸°ë°˜ ê³ ê¸‰ ë³´ê°„ í•¨ìˆ˜ (í´ë°± ì—†ìŒ, ì‹¤íŒ¨ì‹œ ì—ëŸ¬)\"\"\"\n",
        "    print(f\"SVR ë³´ê°„ ì ìš© ì¤‘ ({'TRAIN' if is_train else 'TEST'} ë°ì´í„°)...\")\n",
        "    \n",
        "    df_interpolated = df.copy()\n",
        "    \n",
        "    # ë³´ê°„ ëŒ€ìƒ ì»¬ëŸ¼ í™•ì¥ (is_trainì— ë”°ë¼ heat_demand í¬í•¨ ì—¬ë¶€ ê²°ì •)\n",
        "    base_cols = ['ta', 'hm', 'ws', 'wd', 'rn_day', 'rn_hr1', 'si', 'ta_chi']\n",
        "    if is_train:\n",
        "        interpolation_cols = base_cols + ['heat_demand']\n",
        "    else:\n",
        "        interpolation_cols = base_cols\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    available_cols = [col for col in interpolation_cols if col in numeric_cols]\n",
        "    \n",
        "    print(f\"   ë³´ê°„ ëŒ€ìƒ ì»¬ëŸ¼: {available_cols}\")\n",
        "    \n",
        "    # ì‹œê°„ íŠ¹ì„± ìƒì„± (SVRìš©)\n",
        "    df_interpolated['hour'] = df_interpolated['tm'].dt.hour\n",
        "    df_interpolated['day_of_year'] = df_interpolated['tm'].dt.dayofyear\n",
        "    df_interpolated['month'] = df_interpolated['tm'].dt.month\n",
        "    df_interpolated['dayofweek'] = df_interpolated['tm'].dt.dayofweek\n",
        "    \n",
        "    # ë¸Œëœì¹˜ë³„ SVR ë³´ê°„\n",
        "    for branch in tqdm(df['branch_id'].unique(), desc=\"ì§€ì‚¬ë³„ SVR ë³´ê°„\"):\n",
        "        branch_mask = df_interpolated['branch_id'] == branch\n",
        "        branch_data = df_interpolated[branch_mask].copy().sort_values('tm')\n",
        "        \n",
        "        if len(branch_data) < 24:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰\n",
        "            raise ValueError(f\"ì§€ì‚¬ {branch}: ë°ì´í„° ë¶€ì¡± ({len(branch_data)}ê°œ). ìµœì†Œ 24ê°œ í•„ìš”.\")\n",
        "            \n",
        "        for col in available_cols:\n",
        "            if col not in branch_data.columns:\n",
        "                continue\n",
        "                \n",
        "            missing_mask = branch_data[col].isna()\n",
        "            missing_count = missing_mask.sum()\n",
        "            total_count = len(branch_data)\n",
        "            \n",
        "            if missing_count == 0:  # ê²°ì¸¡ì¹˜ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                # í›ˆë ¨ìš© ë°ì´í„° (ê²°ì¸¡ì´ ì•„ë‹Œ ê²ƒë“¤)\n",
        "                train_mask = ~missing_mask\n",
        "                \n",
        "                if train_mask.sum() < 10:  # ìµœì†Œ 10ê°œ ì´ìƒì˜ í›ˆë ¨ ë°ì´í„° í•„ìš”\n",
        "                    raise ValueError(f\"ì§€ì‚¬ {branch}, ì»¬ëŸ¼ {col}: í›ˆë ¨ ë°ì´í„° ë¶€ì¡± ({train_mask.sum()}ê°œ). ìµœì†Œ 10ê°œ í•„ìš”.\")\n",
        "                \n",
        "                # íŠ¹ì„±: ì‹œê°„, ì—°ì¤‘ì¼, ì›”, ìš”ì¼\n",
        "                feature_cols = ['hour', 'day_of_year', 'month', 'dayofweek']\n",
        "                X_train = branch_data.loc[train_mask, feature_cols].values\n",
        "                y_train = branch_data.loc[train_mask, col].values\n",
        "                \n",
        "                # ì˜ˆì¸¡í•  ë°ì´í„°\n",
        "                X_pred = branch_data.loc[missing_mask, feature_cols].values\n",
        "                \n",
        "                if len(X_pred) == 0:\n",
        "                    continue\n",
        "                \n",
        "                # ìŠ¤ì¼€ì¼ë§\n",
        "                scaler_X = StandardScaler()\n",
        "                scaler_y = StandardScaler()\n",
        "                \n",
        "                X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "                y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "                \n",
        "                # SVR ëª¨ë¸ í›ˆë ¨\n",
        "                svr = SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.1)\n",
        "                svr.fit(X_train_scaled, y_train_scaled)\n",
        "                \n",
        "                # ì˜ˆì¸¡\n",
        "                X_pred_scaled = scaler_X.transform(X_pred)\n",
        "                y_pred_scaled = svr.predict(X_pred_scaled)\n",
        "                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "                \n",
        "                # ê²°ê³¼ í• ë‹¹\n",
        "                df_interpolated.loc[branch_mask & missing_mask, col] = y_pred\n",
        "                \n",
        "                print(f\"     ì§€ì‚¬ {branch}, {col}: {missing_count}ê°œ ê²°ì¸¡ì¹˜ SVR ë³´ê°„ ì™„ë£Œ\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ì§€ì‚¬ {branch}, ì»¬ëŸ¼ {col} SVR ë³´ê°„ ì‹¤íŒ¨:\")\n",
        "                print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                print(f\"   ê²°ì¸¡ì¹˜: {missing_count}/{total_count}ê°œ\")\n",
        "                print(f\"   í›ˆë ¨ ë°ì´í„°: {train_mask.sum() if 'train_mask' in locals() else 'N/A'}ê°œ\")\n",
        "                raise e  # âœ… í´ë°± ì—†ìŒ, ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "    \n",
        "    # ìµœì¢… ê²°ì¸¡ì¹˜ í™•ì¸ ë° ì²˜ë¦¬\n",
        "    for col in available_cols:\n",
        "        if col in df_interpolated.columns:\n",
        "            remaining_nan = df_interpolated[col].isna().sum()\n",
        "            if remaining_nan > 0:\n",
        "                print(f\"âš ï¸ {col}: SVR ë³´ê°„ í›„ì—ë„ {remaining_nan}ê°œ ê²°ì¸¡ì¹˜ ë‚¨ìŒ\")\n",
        "                # Forward/Backward fillë¡œ ìµœì¢… ì²˜ë¦¬\n",
        "                df_interpolated[col] = df_interpolated[col].ffill().bfill()\n",
        "                \n",
        "                # ì—¬ì „íˆ ê²°ì¸¡ì¹˜ê°€ ìˆìœ¼ë©´ ì—ëŸ¬\n",
        "                final_nan = df_interpolated[col].isna().sum()\n",
        "                if final_nan > 0:\n",
        "                    raise ValueError(f\"{col}: ëª¨ë“  ë³´ê°„ ë°©ë²• ì‹¤íŒ¨, {final_nan}ê°œ ê²°ì¸¡ì¹˜ ë‚¨ìŒ\")\n",
        "    \n",
        "    print(f\"   âœ… SVR ë³´ê°„ ì™„ë£Œ\")\n",
        "    return df_interpolated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "groups"
      },
      "source": [
        "## 3-1. ê·œëª¨ë³„ ê·¸ë£¹ ë¶„í•  ë° CV ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. heating_season ì»¬ëŸ¼ ì¶”ê°€ í•¨ìˆ˜\n",
        "def add_heating_season(df):\n",
        "    \"\"\"ë‚œë°© ì‹œì¦Œ ì»¬ëŸ¼ ì¶”ê°€\"\"\"\n",
        "    df = df.copy()\n",
        "    df['heating_season'] = 0  # ê¸°ë³¸ê°’: ë¹„ë‚œë°©\n",
        "    heating_months = [10, 11, 12, 1, 2, 3, 4]  # 10ì›”~4ì›”: ë‚œë°©ì‹œì¦Œ\n",
        "    df.loc[df['month'].isin(heating_months), 'heating_season'] = 1\n",
        "    return df\n",
        "\n",
        "# 2. ì‹œì¦Œë³„ ë°ì´í„° ë¶„í•  í•¨ìˆ˜\n",
        "def split_by_season_only(df):\n",
        "    \"\"\"ì‹œì¦Œë³„ë¡œë§Œ ë¶„í•  (2ê°œ ê·¸ë£¹)\"\"\"\n",
        "    groups = {}\n",
        "    \n",
        "    for season in [0, 1]:  # 0: ë¹„ë‚œë°©, 1: ë‚œë°©\n",
        "        season_name = 'heating' if season == 1 else 'non_heating'\n",
        "        season_data = df[df['heating_season'] == season].copy()\n",
        "        groups[season_name] = season_data\n",
        "                \n",
        "    return groups\n",
        "\n",
        "# 3. ì—°ë„ ê¸°ë°˜ CV ë¶„í•  í•¨ìˆ˜\n",
        "def create_year_based_cv_splits(df, group_name=\"\"):\n",
        "    \"\"\"ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±\"\"\"\n",
        "    print(f\"{group_name} ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\")\n",
        "    \n",
        "    # ì—°ë„ë³„ ë°ì´í„° ë¶„í¬ í™•ì¸\n",
        "    year_counts = df['year'].value_counts().sort_index()\n",
        "    print(f\"   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\")\n",
        "    for year, count in year_counts.items():\n",
        "        print(f\"     {year}ë…„: {count:,}ê°œ\")\n",
        "    \n",
        "    # 3-Fold CV: 2021, 2022, 2023ë…„ ê°ê° validationìœ¼ë¡œ ì‚¬ìš©\n",
        "    cv_splits = []\n",
        "    \n",
        "    for val_year in [2021, 2022, 2023]:\n",
        "        train_mask = df['year'] != val_year\n",
        "        val_mask = df['year'] == val_year\n",
        "        \n",
        "        train_indices = df[train_mask].index.tolist()\n",
        "        val_indices = df[val_mask].index.tolist()\n",
        "        \n",
        "        cv_splits.append((train_indices, val_indices))\n",
        "        \n",
        "        print(f\"   Fold {val_year}: í›ˆë ¨ {len(train_indices):,}ê°œ, ê²€ì¦ {len(val_indices):,}ê°œ\")\n",
        "    \n",
        "    return cv_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3-2. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1ï¸âƒ£ ê¸°ë³¸ ì „ì²˜ë¦¬...\n",
            "ê³ ë„í™”ëœ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\n",
            "   ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ ìƒì„± ì¤‘...\n",
            "   wd (í’í–¥) ì»¬ëŸ¼ì˜ -9.9 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ë¨\n",
            "   ìƒì„±ëœ ê²°ì¸¡ì¹˜ í”Œë˜ê·¸: ['ta_missing', 'ws_missing', 'rn_day_missing', 'rn_hr1_missing', 'hm_missing', 'si_missing', 'ta_chi_missing', 'heat_demand_missing']\n",
            "     ta_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     ws_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     rn_day_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     rn_hr1_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     hm_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     si_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     ta_chi_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     heat_demand_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "   ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ ìƒì„± ì¤‘...\n",
            "   wd (í’í–¥) ì»¬ëŸ¼ì˜ -9.9 ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ë¨\n",
            "   ìƒì„±ëœ ê²°ì¸¡ì¹˜ í”Œë˜ê·¸: ['ta_missing', 'ws_missing', 'rn_day_missing', 'rn_hr1_missing', 'hm_missing', 'si_missing', 'ta_chi_missing', 'heat_demand_missing']\n",
            "     ta_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     ws_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     rn_day_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     rn_hr1_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     hm_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     si_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     ta_chi_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "     heat_demand_missing: 0ê°œ ê²°ì¸¡ì¹˜\n",
            "   í›ˆë ¨: (289997, 63), í…ŒìŠ¤íŠ¸: (97147, 63)\n",
            "   ê¸°ê°„: 2021-01-01 01:00:00 ~ 2025-01-01 00:00:00\n",
            "2ï¸âƒ£ heating_season ì»¬ëŸ¼ ì¶”ê°€...\n",
            "3ï¸âƒ£ ê²°ì¸¡ì¹˜ SVR ë³´ê°„ ì ìš©...\n",
            "\n",
            "ğŸ”§ í›ˆë ¨ ë°ì´í„° SVR ë³´ê°„:\n",
            "SVR ë³´ê°„ ì ìš© ì¤‘ (TRAIN ë°ì´í„°)...\n",
            "   ë³´ê°„ ëŒ€ìƒ ì»¬ëŸ¼: ['ta', 'hm', 'ws', 'wd', 'rn_day', 'rn_hr1', 'si', 'ta_chi', 'heat_demand']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4518004e27b446a296677a20a1a0c6eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ì§€ì‚¬ë³„ SVR ë³´ê°„:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… SVR ë³´ê°„ ì™„ë£Œ\n",
            "\n",
            "ğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° SVR ë³´ê°„:\n",
            "SVR ë³´ê°„ ì ìš© ì¤‘ (TEST ë°ì´í„°)...\n",
            "   ë³´ê°„ ëŒ€ìƒ ì»¬ëŸ¼: ['ta', 'hm', 'ws', 'wd', 'rn_day', 'rn_hr1', 'si', 'ta_chi']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bf0304d11d744c58e8bdccb3725dd2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ì§€ì‚¬ë³„ SVR ë³´ê°„:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… SVR ë³´ê°„ ì™„ë£Œ\n",
            "4ï¸âƒ£ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„±...\n",
            "ì‹œì¦Œë³„ ê¸°ìƒ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„± ì¤‘ (TRAIN ê¸°ì¤€)...\n",
            "   ì§€ì‚¬ A, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ B, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ C, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ D, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ E, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ F, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ G, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ H, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ I, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ J, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ K, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ L, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ M, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ N, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ O, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ P, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ Q, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ R, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ì§€ì‚¬ S, ë‚œë°©ì² : ì„ê³„ê°’ ê³„ì‚° ì™„ë£Œ\n",
            "   ê¸°ìƒ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„± ì™„ë£Œ: 19ê°œ ì§€ì‚¬\n",
            "5ï¸âƒ£ ì‹œì¦Œë³„ ê·¸ë£¹ ë¶„í• ...\n",
            "6ï¸âƒ£ ê·¸ë£¹ë³„ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„±...\n",
            "  ğŸ”¥ heating ê·¸ë£¹ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "   heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì™„ë£Œ: 68ê°œ ì»¬ëŸ¼\n",
            "heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "   heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì™„ë£Œ: 68ê°œ ì»¬ëŸ¼\n",
            "  â„ï¸ non_heating ê·¸ë£¹ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "non_heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "   non_heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì™„ë£Œ: 71ê°œ ì»¬ëŸ¼\n",
            "non_heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì¤‘...\n",
            "   non_heating ì‹œì¦Œ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„± ì™„ë£Œ: 71ê°œ ì»¬ëŸ¼\n",
            "\n",
            "ğŸ“Š ê·¸ë£¹ë³„ ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°:\n",
            "   heating     : í›ˆë ¨ (289997, 68), í…ŒìŠ¤íŠ¸ (97147, 68)\n",
            "   non_heating : í›ˆë ¨ (0, 71), í…ŒìŠ¤íŠ¸ (0, 71)\n",
            "\n",
            "âœ… ëª¨ë“  ì „ì²˜ë¦¬ ì™„ë£Œ!\n",
            "7ï¸âƒ£ ê·¸ë£¹ë³„ CV ë¶„í•  ë¯¸ë¦¬ë³´ê¸°:\n",
            "heating ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\n",
            "   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\n",
            "     2021ë…„: 96,653ê°œ\n",
            "     2022ë…„: 96,672ê°œ\n",
            "     2023ë…„: 96,672ê°œ\n",
            "   Fold 2021: í›ˆë ¨ 193,344ê°œ, ê²€ì¦ 96,653ê°œ\n",
            "   Fold 2022: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "   Fold 2023: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "   heating: 3ê°œ fold ìƒì„±ë¨\n",
            "     âœ… heating: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ ê²°ì¸¡ì¹˜ í•´ê²°ë¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. ê¸°ë³¸ ì „ì²˜ë¦¬ (ê³µí†µ)\n",
        "print(\"1ï¸âƒ£ ê¸°ë³¸ ì „ì²˜ë¦¬...\")\n",
        "train_df, test_df = load_and_preprocess_advanced(train_path, test_path)\n",
        "\n",
        "# 2. heating_season ì»¬ëŸ¼ ì¶”ê°€\n",
        "print(\"2ï¸âƒ£ heating_season ì»¬ëŸ¼ ì¶”ê°€...\")\n",
        "train_df = add_heating_season(train_df)\n",
        "test_df = add_heating_season(test_df)\n",
        "\n",
        "# 3. âœ… ê²°ì¸¡ì¹˜ ë³´ê°„ ë¨¼ì €! (íŒŒìƒë³€ìˆ˜ ìƒì„± ì „)\n",
        "print(\"3ï¸âƒ£ ê²°ì¸¡ì¹˜ SVR ë³´ê°„ ì ìš©...\")\n",
        "print(\"\\nğŸ”§ í›ˆë ¨ ë°ì´í„° SVR ë³´ê°„:\")\n",
        "train_df = apply_svr_interpolation(train_df, is_train=True)\n",
        "\n",
        "print(\"\\nğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° SVR ë³´ê°„:\")\n",
        "test_df = apply_svr_interpolation(test_df, is_train=False)\n",
        "\n",
        "# 4. ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„± (ë³´ê°„ í›„)\n",
        "print(\"4ï¸âƒ£ ì´ìƒì¹˜ í”Œë˜ê·¸ ìƒì„±...\")\n",
        "train_df, test_df, weather_thresholds = create_weather_outlier_flags(train_df, test_df)\n",
        "\n",
        "# 5. ê·¸ë£¹ë³„ë¡œ ë¨¼ì € ë¶„í• \n",
        "print(\"5ï¸âƒ£ ì‹œì¦Œë³„ ê·¸ë£¹ ë¶„í• ...\")\n",
        "train_groups = split_by_season_only(train_df)\n",
        "test_groups = split_by_season_only(test_df)\n",
        "\n",
        "# 6. ê° ê·¸ë£¹ë³„ë¡œ ì‹œì¦Œì— ë§ëŠ” íŠ¹ì„± ìƒì„± (ë³´ê°„ ì™„ë£Œëœ ë°ì´í„°ë¡œ)\n",
        "print(\"6ï¸âƒ£ ê·¸ë£¹ë³„ ê³ ë„í™”ëœ íŠ¹ì„± ìƒì„±...\")\n",
        "\n",
        "# heating ê·¸ë£¹ íŠ¹ì„± ìƒì„±\n",
        "print(\"  ğŸ”¥ heating ê·¸ë£¹ íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
        "train_groups['heating'] = create_advanced_features(train_groups['heating'], \"heating\")\n",
        "test_groups['heating'] = create_advanced_features(test_groups['heating'], \"heating\")\n",
        "\n",
        "# non_heating ê·¸ë£¹ íŠ¹ì„± ìƒì„±  \n",
        "print(\"  â„ï¸ non_heating ê·¸ë£¹ íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
        "train_groups['non_heating'] = create_advanced_features(train_groups['non_heating'], \"non_heating\")\n",
        "test_groups['non_heating'] = create_advanced_features(test_groups['non_heating'], \"non_heating\")\n",
        "\n",
        "print(f\"\\nğŸ“Š ê·¸ë£¹ë³„ ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°:\")\n",
        "for group_name in ['heating', 'non_heating']:\n",
        "    print(f\"   {group_name:12s}: í›ˆë ¨ {train_groups[group_name].shape}, í…ŒìŠ¤íŠ¸ {test_groups[group_name].shape}\")\n",
        "\n",
        "print(f\"\\nâœ… ëª¨ë“  ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "\n",
        "# 7. ê·¸ë£¹ë³„ CV ë¶„í•  ìƒì„± ë° ê²°ê³¼ í™•ì¸\n",
        "print(\"7ï¸âƒ£ ê·¸ë£¹ë³„ CV ë¶„í•  ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "for group_name, group_data in train_groups.items():\n",
        "    if len(group_data) > 100:\n",
        "        cv_splits = create_year_based_cv_splits(group_data, group_name)\n",
        "        print(f\"   {group_name}: {len(cv_splits)}ê°œ fold ìƒì„±ë¨\")\n",
        "        \n",
        "        # ê²°ì¸¡ì¹˜ ìµœì¢… í™•ì¸\n",
        "        weather_cols = ['ta', 'hm', 'ws', 'rn_day', 'rn_hr1', 'si', 'ta_chi', 'apparent_temp']\n",
        "        total_missing = 0\n",
        "        for col in weather_cols:\n",
        "            if col in group_data.columns:\n",
        "                missing = group_data[col].isna().sum()\n",
        "                total_missing += missing\n",
        "                if missing > 0:\n",
        "                    print(f\"     âš ï¸ {col}: {missing}ê°œ ê²°ì¸¡ì¹˜ ë‚¨ìŒ\")\n",
        "        \n",
        "        if total_missing == 0:\n",
        "            print(f\"     âœ… {group_name}: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ ê²°ì¸¡ì¹˜ í•´ê²°ë¨\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_processed_data(train_groups, test_groups, weather_thresholds):\n",
        "   \"\"\"ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥\"\"\"\n",
        "   os.makedirs(save_dir, exist_ok=True)\n",
        "   \n",
        "   # ê° ê·¸ë£¹ë³„ë¡œ CSV ì €ì¥\n",
        "   for group_name in train_groups.keys():\n",
        "       train_groups[group_name].to_csv(f\"/train_{group_name}.csv\", index=False)\n",
        "       test_groups[group_name].to_csv(f\"/test_{group_name}.csv\", index=False)\n",
        "   \n",
        "   # weather_thresholds ì €ì¥\n",
        "   with open(f\"/weather_thresholds.pickle\", 'wb') as f:\n",
        "       pickle.dump(weather_thresholds, f)\n",
        "   \n",
        "   print(f\"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "save_processed_data(train_groups, test_groups, weather_thresholds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ! ë°ì´í„° ìˆì„ ë•Œ, ì—¬ê¸°ë¶€í„° ë°”ë¡œ ëŒë¦¬ë©´ ë¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "def load_processed_data(save_dir=\"/Users/jisupark_1/workspace/star_track_python/PRJ_Meteo/dataset/\"):\n",
        "   \"\"\"ì €ì¥ëœ ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ë¡œë“œ\"\"\"\n",
        "   try:\n",
        "       train_groups = {\n",
        "           'heating': pd.read_csv(f\"{save_dir}/train_heating.csv\"),\n",
        "           'non_heating': pd.read_csv(f\"{save_dir}/train_non_heating.csv\")\n",
        "       }\n",
        "       \n",
        "       test_groups = {\n",
        "           'heating': pd.read_csv(f\"{save_dir}/test_heating.csv\"),\n",
        "           'non_heating': pd.read_csv(f\"{save_dir}/test_non_heating.csv\")\n",
        "       }\n",
        "       \n",
        "       with open(f\"{save_dir}/weather_thresholds.pickle\", 'rb') as f:\n",
        "           weather_thresholds = pickle.load(f)\n",
        "       \n",
        "       print(f\"âœ… ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
        "       return train_groups, test_groups, weather_thresholds\n",
        "   \n",
        "   except FileNotFoundError:\n",
        "       print(\"âŒ ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "       return None, None, None\n",
        "\n",
        "# 1. ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì‹œë„\n",
        "train_groups, test_groups, weather_thresholds = load_processed_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7ï¸âƒ£ ê·¸ë£¹ë³„ CV ë¶„í•  ë¯¸ë¦¬ë³´ê¸°:\n",
            "heating ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\n",
            "   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\n",
            "     2021ë…„: 96,653ê°œ\n",
            "     2022ë…„: 96,672ê°œ\n",
            "     2023ë…„: 96,672ê°œ\n",
            "   Fold 2021: í›ˆë ¨ 193,344ê°œ, ê²€ì¦ 96,653ê°œ\n",
            "   Fold 2022: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "   Fold 2023: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "   heating: 3ê°œ fold ìƒì„±ë¨\n",
            "     âœ… heating: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ ê²°ì¸¡ì¹˜ í•´ê²°ë¨\n",
            "\n",
            "non_heating ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\n",
            "   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\n",
            "     2021ë…„: 69,768ê°œ\n",
            "     2022ë…„: 69,768ê°œ\n",
            "     2023ë…„: 69,768ê°œ\n",
            "   Fold 2021: í›ˆë ¨ 139,536ê°œ, ê²€ì¦ 69,768ê°œ\n",
            "   Fold 2022: í›ˆë ¨ 139,536ê°œ, ê²€ì¦ 69,768ê°œ\n",
            "   Fold 2023: í›ˆë ¨ 139,536ê°œ, ê²€ì¦ 69,768ê°œ\n",
            "   non_heating: 3ê°œ fold ìƒì„±ë¨\n",
            "     âœ… non_heating: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ ê²°ì¸¡ì¹˜ í•´ê²°ë¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"7ï¸âƒ£ ê·¸ë£¹ë³„ CV ë¶„í•  ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "for group_name, group_data in train_groups.items():\n",
        "    if len(group_data) > 100:\n",
        "        cv_splits = create_year_based_cv_splits(group_data, group_name)\n",
        "        print(f\"   {group_name}: {len(cv_splits)}ê°œ fold ìƒì„±ë¨\")\n",
        "        \n",
        "        # ê²°ì¸¡ì¹˜ ìµœì¢… í™•ì¸\n",
        "        weather_cols = [# ê¸°ë³¸ ì‹œê°„ ë³€ìˆ˜ ì¶”ê°€\n",
        "            'hour', 'month', 'day',\n",
        "            # ê¸°ìƒ ë³€ìˆ˜\n",
        "            'ta', 'hm', 'ws', 'rn_day', 'rn_hr1', 'si', 'ta_chi',  # rn_hr1, ta_chi ì¶”ê°€\n",
        "            # íŒŒìƒ ë³€ìˆ˜\n",
        "            'HDD18', 'apparent_temp',  # HDD20 ì œê±°\n",
        "            # ìˆœí™˜ ì¸ì½”ë”©\n",
        "            'hour_sin', 'hour_cos', 'month_sin', 'month_cos', \n",
        "            'dayofweek_sin', 'dayofweek_cos',\n",
        "            # ì‹œê³„ì—´ íŠ¹ì„±\n",
        "            'ta_lag_3h', 'ta_lag_6h', 'ta_lag_24h', \n",
        "            'ta_ma_6h', 'ta_ma_12h', 'ta_ma_24h',\n",
        "            'ta_diff_3h', 'ta_diff_6h', \n",
        "            # ì¼ë³„ í†µê³„\n",
        "            'daily_ta_min', 'daily_ta_max', 'daily_ta_mean', 'daily_temp_range']\n",
        "        total_missing = 0\n",
        "        for col in weather_cols:\n",
        "            if col in group_data.columns:\n",
        "                missing = group_data[col].isna().sum()\n",
        "                total_missing += missing\n",
        "                if missing > 0:\n",
        "                    print(f\"     âš ï¸ {col}: {missing}ê°œ ê²°ì¸¡ì¹˜ ë‚¨ìŒ\")\n",
        "        \n",
        "        if total_missing == 0:\n",
        "            print(f\"     âœ… {group_name}: ëª¨ë“  ê¸°ìƒë³€ìˆ˜ ê²°ì¸¡ì¹˜ í•´ê²°ë¨\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_features"
      },
      "source": [
        "## 4. ëª¨ë¸ë³„ í”¼ì³ ì •ì˜\n",
        "\n",
        "Prophet\n",
        "\n",
        "- ì‹œê°„ ë³€ìˆ˜ì™€ ê¸°ë³¸ ê¸°ìƒ ë³€ìˆ˜ ì¤‘ì‹¬\n",
        "- ë„ˆë¬´ ë§ì€ ë³€ìˆ˜ë³´ë‹¤ëŠ” í•µì‹¬ íŠ¹ì„±ì— ì§‘ì¤‘\n",
        "\n",
        "CatBoost\n",
        "\n",
        "- ë²”ì£¼í˜• ë³€ìˆ˜ì™€ í”Œë˜ê·¸ ë³€ìˆ˜ë¥¼ ìµœëŒ€í•œ í™œìš©\n",
        "- ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ í”Œë˜ê·¸ë¡œ ë°ì´í„° í’ˆì§ˆ ì •ë³´ ì œê³µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_definition"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ëª¨ë¸ë³„ í”¼ì³ ì •ì˜ ì™„ë£Œ:\n",
            "==================================================\n",
            "prophet_heating     : 15ê°œ í”¼ì³\n",
            "  basic          : 6ê°œ\n",
            "  seasonal       : 9ê°œ\n",
            "prophet_non_heating : 14ê°œ í”¼ì³\n",
            "  basic          : 5ê°œ\n",
            "  seasonal       : 9ê°œ\n",
            "catboost_heating    : 49ê°œ í”¼ì³\n",
            "  numerical      : 26ê°œ\n",
            "  categorical    : 9ê°œ\n",
            "  flags          : 10ê°œ\n",
            "  seasonal       : 4ê°œ\n",
            "catboost_non_heating: 47ê°œ í”¼ì³\n",
            "  numerical      : 26ê°œ\n",
            "  categorical    : 8ê°œ\n",
            "  flags          : 10ê°œ\n",
            "  seasonal       : 3ê°œ\n"
          ]
        }
      ],
      "source": [
        "def define_model_features():\n",
        "    # Prophetì€ ìì²´ ì‹œê³„ì—´ ë¶„í•´ ëŠ¥ë ¥ì´ ìˆìŒ\n",
        "    prophet_features = {\n",
        "        'basic': [\n",
        "            'ta', 'hm', 'ws', 'HDD18', 'apparent_temp'\n",
        "        ],\n",
        "        'seasonal': [\n",
        "            'hour_sin', 'hour_cos', 'month_sin', 'month_cos', \n",
        "            'dayofweek_sin', 'dayofweek_cos',\n",
        "            'ta_lag_3h', 'ta_lag_6h'  # ì§§ì€ lagë§Œ (Prophet ìì²´ ì‹œê³„ì—´ ì²˜ë¦¬ ë³´ì™„) # ma, diffëŠ” ë¶ˆí•„ìš” (Prophetì´ ìì²´ ì²˜ë¦¬)\n",
        "        ]\n",
        "    }\n",
        "    # CatBoost: ëª¨ë“  íŠ¹ì„± í™œìš© (ë²”ì£¼í˜• + ì‹œê³„ì—´ + í”Œë˜ê·¸)\n",
        "    catboost_features = {\n",
        "        'numerical': [\n",
        "            'day', 'dayofyear',\n",
        "            # ê¸°ìƒ ë³€ìˆ˜\n",
        "            'ta', 'hm', 'ws', 'rn_day', 'rn_hr1', 'si', 'ta_chi',\n",
        "            # íŒŒìƒ ë³€ìˆ˜\n",
        "            'HDD18',\n",
        "            # ìˆœí™˜ ì¸ì½”ë”©\n",
        "            'hour_sin', 'hour_cos', #'month_sin', 'month_cos' ì œì™¸ (ì¤‘ë³µ)\n",
        "            'dayofweek_sin', 'dayofweek_cos',\n",
        "            # ëª¨ë“  ì‹œê³„ì—´ íŠ¹ì„± (CatBoostëŠ” ì§ì ‘ í•™ìŠµ)\n",
        "            'ta_lag_3h', 'ta_lag_6h', 'ta_lag_24h', \n",
        "            'ta_ma_6h', 'ta_ma_12h', 'ta_ma_24h',\n",
        "            'ta_diff_3h', 'ta_diff_6h', \n",
        "            # ì¼ë³„ í†µê³„\n",
        "            'daily_ta_min', 'daily_ta_max', 'daily_ta_mean', 'daily_temp_range'\n",
        "        ],\n",
        "        'categorical': [\n",
        "            'branch_id', 'hour_cat', 'month_cat', 'weekday_name', \n",
        "            'temp_category', 'wind_category', 'holiday_type', 'peak_time'\n",
        "        ],\n",
        "        'flags': [\n",
        "            # ê²°ì¸¡ì¹˜ í”Œë˜ê·¸ (ëª¨ë“  ê¸°ìƒ ë³€ìˆ˜)\n",
        "            'ta_missing', 'ws_missing', 'rn_day_missing', \n",
        "            'rn_hr1_missing', 'hm_missing', 'si_missing', 'ta_chi_missing',\n",
        "            # ì´ìƒì¹˜ í”Œë˜ê·¸ \n",
        "            'cold_extreme', 'strong_wind', 'heavy_rain'\n",
        "        ]\n",
        "    }\n",
        "     # LSTM: í•µì‹¬ íŠ¹ì„± + ì „ì²˜ë¦¬ëœ ì‹œê³„ì—´ (lag ì œì™¸)\n",
        "    lstm_features = {\n",
        "        'numerical': [\n",
        "            # ê¸°ë³¸ ì‹œê°„ ë³€ìˆ˜ ì¶”ê°€\n",
        "            'day', 'dayofyear',\n",
        "            # í•µì‹¬ ê¸°ìƒ ë³€ìˆ˜\n",
        "            'ta', 'hm', 'ws', 'rn_day', 'si',\n",
        "            # íŒŒìƒ ë³€ìˆ˜\n",
        "            'HDD18',\n",
        "            # ìˆœí™˜ ì¸ì½”ë”©\n",
        "            'hour_sin', 'hour_cos', #'month_sin', 'month_cos' ì œì™¸ (ì¤‘ë³µ)\n",
        "            'dayofweek_sin', 'dayofweek_cos',\n",
        "            # í•µì‹¬ ì´ë™ í‰ê· ë§Œ\n",
        "            'ta_ma_6h', 'ta_ma_12h', 'ta_ma_24h',\n",
        "            # ì¼ë³„ í†µê³„ (í•µì‹¬ë§Œ)\n",
        "            'daily_ta_mean', 'daily_temp_range' # lag, diff ì œê±° - LSTM sequenceë¡œ ëŒ€ì²´\n",
        "        ], \n",
        "        'categorical_encoded': ['branch_id']\n",
        "    }\n",
        "    \n",
        "    # heating ì‹œì¦Œë³„ íŠ¹ì„± ì¶”ê°€\n",
        "    prophet_heating = copy.deepcopy(prophet_features)\n",
        "    prophet_heating['basic'].append('apparent_temp')\n",
        "    prophet_heating['seasonal'].extend(['heating_month_order'])\n",
        "     # prophet_non_heatingë„ non_heating_month_order ì¶”ê°€\n",
        "    prophet_non_heating = copy.deepcopy(prophet_features)\n",
        "    prophet_non_heating['seasonal'].append('non_heating_month_order')\n",
        "\n",
        "    # ë‚œë°©ì‹œì¦Œìš© CatBoost (í•œíŒŒ ê²½ë³´ + ì‹œì¦Œë³„ ìˆœí™˜ ì¶”ê°€)\n",
        "    catboost_heating = copy.deepcopy(catboost_features)\n",
        "    catboost_heating['categorical'].append('cold_warning_level')\n",
        "    catboost_heating['seasonal'] = [\n",
        "        'apparent_temp', 'heating_month_order', \n",
        "        'heating_month_sin', 'heating_month_cos'\n",
        "    ]\n",
        "    # ë¹„ë‚œë°©ì‹œì¦Œìš© CatBoost (ì‹œì¦Œë³„ ìˆœí™˜ ì¶”ê°€)\n",
        "    catboost_non_heating = copy.deepcopy(catboost_features)\n",
        "    catboost_non_heating['seasonal'] = [\n",
        "        'non_heating_month_order', 'non_heating_month_sin', 'non_heating_month_cos'\n",
        "    ]\n",
        "\n",
        "    \n",
        "    return {\n",
        "        'prophet_heating': prophet_heating,\n",
        "        'prophet_non_heating': prophet_non_heating,  # apparent_temp ì—†ìŒ\n",
        "        'catboost_heating': catboost_heating,\n",
        "        'catboost_non_heating': catboost_non_heating\n",
        "        # 'lstm_heating': lstm_heating,\n",
        "        # 'lstm_non_heating': lstm_non_heating\n",
        "    }\n",
        "\n",
        "model_features = define_model_features()\n",
        "\n",
        "print(\"ëª¨ë¸ë³„ í”¼ì³ ì •ì˜ ì™„ë£Œ:\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, features in model_features.items():\n",
        "    total_features = sum(len(v) if isinstance(v, list) else 0 for v in features.values())\n",
        "    print(f\"{model_name:20s}: {total_features}ê°œ í”¼ì³\")\n",
        "    for feature_type, feature_list in features.items():\n",
        "        if isinstance(feature_list, list):\n",
        "            print(f\"  {feature_type:15s}: {len(feature_list)}ê°œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arima_models"
      },
      "source": [
        "## 5. ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (Huber Loss ì§€ì›)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Time Series Dataset í´ë˜ìŠ¤ ì¶”ê°€\n",
        "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y, sequence_length=24):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.sequence_length + 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.X[idx:idx+self.sequence_length],\n",
        "            self.y[idx+self.sequence_length-1]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_classes"
      },
      "source": [
        "## 6. Prophet, CatBoost, LSTM ëª¨ë¸ í´ë˜ìŠ¤ (Optuna ìµœì í™”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prophet ìµœì í™” ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "class ProphetOptimizedModel:\n",
        "    def __init__(self, season_type=\"heating\"):\n",
        "        self.models = {}\n",
        "        self.best_params = {}\n",
        "        self.season_type = season_type\n",
        "        \n",
        "        \n",
        "    def optimize_hyperparameters(self, df, cv_splits, target_col='heat_demand', n_trials=30):\n",
        "        \"\"\"ì—°ë„ ê¸°ë°˜ CVë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
        "        print(f\"Prophet Huber Loss í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... (trials: {n_trials})\")\n",
        "        \n",
        "        def objective(trial):\n",
        "            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
        "            changepoint_prior_scale = trial.suggest_float('changepoint_prior_scale', 0.001, 0.5, log=True)\n",
        "            seasonality_prior_scale = trial.suggest_float('seasonality_prior_scale', 0.1, 10, log=True)\n",
        "            holidays_prior_scale = trial.suggest_float('holidays_prior_scale', 0.1, 10, log=True)\n",
        "            seasonality_mode = trial.suggest_categorical('seasonality_mode', ['additive', 'multiplicative'])\n",
        "            \n",
        "            cv_scores = []\n",
        "            \n",
        "            # ì—°ë„ ê¸°ë°˜ 3-Fold CV\n",
        "            for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
        "                fold_predictions = []\n",
        "                fold_targets = []\n",
        "                \n",
        "                train_fold = df.iloc[train_idx]\n",
        "                val_fold = df.iloc[val_idx]\n",
        "                \n",
        "                # ì§€ì‚¬ë³„ ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡\n",
        "                for branch in df['branch_id'].unique():\n",
        "                    branch_train = train_fold[train_fold['branch_id'] == branch]\n",
        "                    branch_val = val_fold[val_fold['branch_id'] == branch]\n",
        "                    \n",
        "                    if len(branch_train) < 50 or len(branch_val) == 0:\n",
        "                        continue\n",
        "                    \n",
        "                    try:\n",
        "                        # Prophet ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ ë³´ê°„ëœ ë°ì´í„° ì‚¬ìš©)\n",
        "                        prophet_df = pd.DataFrame({\n",
        "                            'ds': pd.to_datetime(branch_train['tm']),\n",
        "                            'y': branch_train[target_col]\n",
        "                        })\n",
        "                        \n",
        "                        # ì‹œì¦Œë³„ í”¼ì³ ì„¤ì • ì‚¬ìš©\n",
        "                        feature_config = model_features[f'prophet_{self.season_type}']\n",
        "                        regressors = feature_config['basic'] + feature_config['seasonal']\n",
        "                        \n",
        "                        for reg in regressors:\n",
        "                            if reg in branch_train.columns:\n",
        "                                prophet_df[reg] = branch_train[reg].values\n",
        "                        \n",
        "                        # Prophet ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
        "                        model = Prophet(\n",
        "                            changepoint_prior_scale=changepoint_prior_scale,\n",
        "                            seasonality_prior_scale=seasonality_prior_scale,\n",
        "                            holidays_prior_scale=holidays_prior_scale,\n",
        "                            seasonality_mode=seasonality_mode,\n",
        "                            daily_seasonality=True,\n",
        "                            weekly_seasonality=True,\n",
        "                            yearly_seasonality=True\n",
        "                        )\n",
        "                        \n",
        "                        # íšŒê·€ë³€ìˆ˜ ì¶”ê°€\n",
        "                        for reg in regressors:\n",
        "                            if reg in prophet_df.columns and reg not in ['ds', 'y']:\n",
        "                                model.add_regressor(reg)\n",
        "                        \n",
        "                        model.fit(prophet_df)\n",
        "                        \n",
        "                        # ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„\n",
        "                        future_df = pd.DataFrame({\n",
        "                            'ds': pd.to_datetime(branch_val['tm'])\n",
        "                        })\n",
        "                        \n",
        "                        for reg in regressors:\n",
        "                            if reg in branch_val.columns:\n",
        "                                future_df[reg] = branch_val[reg].values\n",
        "                        \n",
        "                        # ì˜ˆì¸¡ ì‹¤í–‰\n",
        "                        forecast = model.predict(future_df)\n",
        "                        predictions = np.maximum(forecast['yhat'].values, 0)\n",
        "                        \n",
        "                        actual_values = branch_val[target_col].values\n",
        "                        valid_mask = ~np.isnan(actual_values) & ~np.isnan(predictions)\n",
        "                        \n",
        "                        if valid_mask.sum() > 0:\n",
        "                            fold_predictions.extend(predictions[valid_mask])\n",
        "                            fold_targets.extend(actual_values[valid_mask])\n",
        "                    \n",
        "                    except Exception as e:\n",
        "                        print(f\"âŒ ìµœì í™” ì¤‘ ì§€ì‚¬ {branch} Fold {fold+1} ì‹¤íŒ¨:\")\n",
        "                        print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                        raise e\n",
        "                \n",
        "                # Foldë³„ Huber Loss ê³„ì‚°\n",
        "                # ìˆ˜ì •ëœ ì½”ë“œ\n",
        "                if len(fold_predictions) > 10:\n",
        "                    # âœ… listë¥¼ numpy arrayë¡œ ë³€í™˜\n",
        "                    fold_targets_array = np.array(fold_targets)\n",
        "                    fold_predictions_array = np.array(fold_predictions)\n",
        "                    \n",
        "                    huber = huber_score(fold_targets_array, fold_predictions_array, delta=1.0)\n",
        "                    cv_scores.append(huber)\n",
        "                    print(f\"   Fold {fold+1}: Huber Loss = {huber:.4f} ({len(fold_predictions)}ê°œ ì˜ˆì¸¡)\")\n",
        "                else:\n",
        "                    print(f\"   Fold {fold+1}: ìœ íš¨í•œ ì˜ˆì¸¡ ë¶€ì¡± ({len(fold_predictions)}ê°œ)\")\n",
        "            \n",
        "            if len(cv_scores) == 0:\n",
        "                raise RuntimeError(\"ëª¨ë“  Foldì—ì„œ Prophet ìµœì í™” ì‹¤íŒ¨\")\n",
        "                \n",
        "            return np.mean(cv_scores)\n",
        "        \n",
        "        # Optuna ìµœì í™” ì‹¤í–‰\n",
        "        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=SEED))\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "        \n",
        "        self.best_params = study.best_params\n",
        "        print(f\"   Prophet ìµœì  Huber Loss: {study.best_value:.4f}\")\n",
        "        print(f\"   ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params}\")\n",
        "        return study.best_value\n",
        "\n",
        "    def fit(self, df, target_col='heat_demand'):\n",
        "        \"\"\"ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° í›ˆë ¨\"\"\"\n",
        "        print(f\"Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
        "        \n",
        "        branches = df['branch_id'].unique()\n",
        "        success_count = 0\n",
        "        \n",
        "        # ì‹œì¦Œë³„ í”¼ì³ ì„¤ì • ì‚¬ìš©\n",
        "        feature_config = model_features[f'prophet_{self.season_type}']\n",
        "        regressors = feature_config['basic'] + feature_config['seasonal']\n",
        "        \n",
        "        print(f\"   ì‚¬ìš©í•  regressors: {regressors}\")\n",
        "\n",
        "        for branch in tqdm(branches, desc=\"Prophet ì§€ì‚¬ë³„ í›ˆë ¨\"):\n",
        "            branch_data = df[df['branch_id'] == branch].copy()\n",
        "\n",
        "            if len(branch_data) < 50:\n",
        "                print(f\"âš ï¸ ì§€ì‚¬ {branch}: ë°ì´í„° ë¶€ì¡± ({len(branch_data)}ê°œ) - ìŠ¤í‚µ\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Prophet ë°ì´í„° ì¤€ë¹„\n",
        "                prophet_df = pd.DataFrame({\n",
        "                    'ds': branch_data['tm'],\n",
        "                    'y': branch_data[target_col]\n",
        "                })\n",
        "\n",
        "                # íšŒê·€ë³€ìˆ˜ ì¶”ê°€\n",
        "                missing_regressors = []\n",
        "                for reg in regressors:\n",
        "                    if reg in branch_data.columns:\n",
        "                        prophet_df[reg] = branch_data[reg].values\n",
        "                    else:\n",
        "                        missing_regressors.append(reg)\n",
        "                \n",
        "                if missing_regressors:\n",
        "                    print(f\"âš ï¸ ì§€ì‚¬ {branch}: ëˆ„ë½ëœ regressors: {missing_regressors}\")\n",
        "\n",
        "                # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±\n",
        "                model = Prophet(\n",
        "                    changepoint_prior_scale=self.best_params.get('changepoint_prior_scale', 0.05),\n",
        "                    seasonality_prior_scale=self.best_params.get('seasonality_prior_scale', 10.0),\n",
        "                    holidays_prior_scale=self.best_params.get('holidays_prior_scale', 10.0),\n",
        "                    seasonality_mode=self.best_params.get('seasonality_mode', 'multiplicative'),\n",
        "                    daily_seasonality=True,\n",
        "                    weekly_seasonality=True,\n",
        "                    yearly_seasonality=True  # âœ… ì—°ê°„ ê³„ì ˆì„± í™œì„±í™”\n",
        "                )\n",
        "\n",
        "                # íšŒê·€ë³€ìˆ˜ ì¶”ê°€\n",
        "                added_regressors = []\n",
        "                for reg in regressors:\n",
        "                    if reg in prophet_df.columns and reg not in ['ds', 'y']:\n",
        "                        model.add_regressor(reg)\n",
        "                        added_regressors.append(reg)\n",
        "\n",
        "                # ë°ì´í„° í’ˆì§ˆ í™•ì¸\n",
        "                if prophet_df['y'].isna().sum() > 0:\n",
        "                    print(f\"âš ï¸ ì§€ì‚¬ {branch}: íƒ€ê²Ÿ ë³€ìˆ˜ì— ê²°ì¸¡ì¹˜ {prophet_df['y'].isna().sum()}ê°œ\")\n",
        "                \n",
        "                model.fit(prophet_df)\n",
        "                self.models[branch] = model\n",
        "                success_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ì§€ì‚¬ {branch} Prophet í›ˆë ¨ ì‹¤íŒ¨:\")\n",
        "                print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                print(f\"   ë°ì´í„° í¬ê¸°: {len(branch_data)}\")\n",
        "                print(f\"   Prophet ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {prophet_df.shape}\")\n",
        "                print(f\"   ì¶”ê°€ëœ regressors: {added_regressors}\")\n",
        "                print(f\"   íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:\")\n",
        "                print(f\"     í‰ê· : {prophet_df['y'].mean():.2f}\")\n",
        "                print(f\"     ê²°ì¸¡ì¹˜: {prophet_df['y'].isna().sum()}ê°œ\")\n",
        "                print(f\"     ìµœì†Œê°’: {prophet_df['y'].min():.2f}\")\n",
        "                print(f\"     ìµœëŒ€ê°’: {prophet_df['y'].max():.2f}\")\n",
        "                \n",
        "                # íšŒê·€ë³€ìˆ˜ë³„ ê²°ì¸¡ì¹˜ í™•ì¸\n",
        "                print(f\"   íšŒê·€ë³€ìˆ˜ ê²°ì¸¡ì¹˜ í˜„í™©:\")\n",
        "                for reg in regressors:\n",
        "                    if reg in prophet_df.columns:\n",
        "                        missing = prophet_df[reg].isna().sum()\n",
        "                        print(f\"     {reg}: {missing}ê°œ\")\n",
        "                \n",
        "                raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "\n",
        "        print(f\"   {success_count}/{len(branches)}ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\")\n",
        "        \n",
        "        if success_count == 0:\n",
        "            raise RuntimeError(\"ëª¨ë“  ì§€ì‚¬ì—ì„œ Prophet í›ˆë ¨ ì‹¤íŒ¨!\")\n",
        "\n",
        "    def predict(self, df):\n",
        "        \"\"\"ì˜ˆì¸¡ ì‹¤í–‰\"\"\"\n",
        "        if len(self.models) == 0:\n",
        "            raise RuntimeError(\"í›ˆë ¨ëœ Prophet ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "        \n",
        "        predictions = []\n",
        "        \n",
        "        # ì‹œì¦Œë³„ í”¼ì³ ì„¤ì • ì‚¬ìš©\n",
        "        feature_config = model_features[f'prophet_{self.season_type}']\n",
        "        regressors = feature_config['basic'] + feature_config['seasonal']\n",
        "        \n",
        "        for branch in df['branch_id'].unique():\n",
        "            if branch not in self.models:\n",
        "                print(f\"âš ï¸ ì§€ì‚¬ {branch}: í›ˆë ¨ëœ ëª¨ë¸ ì—†ìŒ, 0ìœ¼ë¡œ ì±„ì›€\")\n",
        "                predictions.extend([0] * len(df[df['branch_id'] == branch]))\n",
        "                continue\n",
        "\n",
        "            branch_data = df[df['branch_id'] == branch].copy()\n",
        "            \n",
        "            try:\n",
        "                future_df = pd.DataFrame({'ds': branch_data['tm']})\n",
        "\n",
        "                # íšŒê·€ë³€ìˆ˜ ì¶”ê°€\n",
        "                for reg in regressors:\n",
        "                    if reg in branch_data.columns:\n",
        "                        future_df[reg] = branch_data[reg].values\n",
        "\n",
        "                forecast = self.models[branch].predict(future_df)\n",
        "                branch_predictions = np.maximum(forecast['yhat'].values, 0)\n",
        "                predictions.extend(branch_predictions)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ì§€ì‚¬ {branch} Prophet ì˜ˆì¸¡ ì‹¤íŒ¨:\")\n",
        "                print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                raise e\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "print(\"Prophet ìµœì í™” ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CatBoost ìµœì í™” ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "class CatBoostOptimizedModel:\n",
        "    def __init__(self, season_type=\"heating\"):\n",
        "        self.model = None\n",
        "        self.feature_cols = None\n",
        "        self.categorical_features = None\n",
        "        self.best_params = {}\n",
        "        self.season_type = season_type\n",
        "        \n",
        "        # ì‹œì¦Œë³„ í”¼ì³ ì„¤ì •\n",
        "        feature_config = model_features[f'catboost_{season_type}']\n",
        "        self.features = feature_config\n",
        "        \n",
        "    def prepare_features(self, df):\n",
        "        \"\"\"í”¼ì³ ì¤€ë¹„ ë° ì „ì²˜ë¦¬\"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # ëª¨ë“  í”¼ì³ ìˆ˜ì§‘ (seasonal ì¶”ê°€)\n",
        "        all_features = []\n",
        "        for ftype in ['numerical', 'categorical', 'flags', 'seasonal']:\n",
        "            if ftype in self.features:\n",
        "                all_features.extend(self.features[ftype])\n",
        "        \n",
        "        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì³ë§Œ ì„ íƒ\n",
        "        available_features = [col for col in all_features if col in df.columns]\n",
        "        missing_features = [col for col in all_features if col not in df.columns]\n",
        "        \n",
        "        if missing_features:\n",
        "            print(f\"âš ï¸ ëˆ„ë½ëœ í”¼ì³ ({len(missing_features)}ê°œ): {missing_features}\")\n",
        "        \n",
        "        self.feature_cols = available_features\n",
        "        \n",
        "        # ë²”ì£¼í˜• í”¼ì³ ì²˜ë¦¬\n",
        "        categorical_features = []\n",
        "        if 'categorical' in self.features:\n",
        "            categorical_features = [col for col in self.features['categorical'] if col in df.columns]\n",
        "            \n",
        "        self.categorical_features = categorical_features\n",
        "        \n",
        "        # ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "        for col in categorical_features:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str)\n",
        "        \n",
        "        print(f\"   ìµœì¢… ì‚¬ìš© í”¼ì³: {len(self.feature_cols)}ê°œ\")\n",
        "        print(f\"   ë²”ì£¼í˜• í”¼ì³: {len(categorical_features)}ê°œ - {categorical_features}\")\n",
        "        \n",
        "        return df[self.feature_cols], categorical_features\n",
        "    \n",
        "    # ë‹¨ì¡°ì„± ì œì•½ ì„¤ì • í•¨ìˆ˜ ì¶”ê°€\n",
        "    def _get_monotone_constraints(self, feature_names):\n",
        "        \"\"\"ë‚œë°© ìˆ˜ìš” ì˜ˆì¸¡ì— ë§ëŠ” ë‹¨ì¡°ì„± ì œì•½ ì„¤ì •\"\"\"\n",
        "        constraints = []\n",
        "        \n",
        "        for feature in feature_names:\n",
        "            # âœ… 2. ë‹¨ì¡°ì„± ì œì•½ ì„¤ì • (ë¬¼ë¦¬ì  ìƒì‹ ë°˜ì˜)\n",
        "            if ('ta' in feature or 'apparent_temp' in feature) and 'lag' not in feature and 'diff' not in feature:  \n",
        "                # ì˜¨ë„, ì²´ê°ì˜¨ë„: ì˜¨ë„ â†“ = ë‚œë°© ìˆ˜ìš” â†‘ (ìŒì˜ ìƒê´€)\n",
        "                constraints.append(-1)\n",
        "            elif feature in ['HDD18']:  \n",
        "                # ë‚œë°©ë„ì¼: HDD â†‘ = ë‚œë°© ìˆ˜ìš” â†‘ (ì–‘ì˜ ìƒê´€)\n",
        "                constraints.append(1)\n",
        "            else:  \n",
        "                # ë‚˜ë¨¸ì§€ëŠ” ì œì•½ ì—†ìŒ (ë²”ì£¼í˜• ë³€ìˆ˜ cold_extreme í¬í•¨)\n",
        "                constraints.append(0)\n",
        "        \n",
        "        constrained_count = sum(1 for c in constraints if c != 0)\n",
        "        print(f\"   ë‹¨ì¡°ì„± ì œì•½: {constrained_count}ê°œ í”¼ì³ì— ì ìš©\")\n",
        "        return constraints\n",
        "\n",
        "    def optimize_hyperparameters(self, df, cv_splits, target_col='heat_demand', n_trials=50):\n",
        "        \"\"\"ì—°ë„ ê¸°ë°˜ CVë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
        "        print(f\"CatBoost Huber Loss í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... (trials: {n_trials})\")\n",
        "        \n",
        "        # í”¼ì³ ì¤€ë¹„\n",
        "        X_full, categorical_features = self.prepare_features(df)\n",
        "        y_full = df[target_col].values\n",
        "        \n",
        "        print(f\"   ìµœì í™” ë°ì´í„°: {X_full.shape}\")\n",
        "        print(f\"   íƒ€ê²Ÿ í†µê³„: í‰ê· ={y_full.mean():.2f}, í‘œì¤€í¸ì°¨={y_full.std():.2f}\")\n",
        "        \n",
        "        def objective(trial):\n",
        "            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
        "            params = {\n",
        "                'iterations': trial.suggest_int('iterations', 500, 2000),\n",
        "                'depth': trial.suggest_int('depth', 4, 10),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 20),\n",
        "                'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "                'random_seed': SEED,\n",
        "                'task_type': 'CPU',\n",
        "                'verbose': 0,\n",
        "                'loss_function': 'Huber:delta=1.0'  # CatBoost ë‚´ì¥ Huber Loss\n",
        "            }\n",
        "            \n",
        "            cv_scores = []\n",
        "            \n",
        "            # ì—°ë„ ê¸°ë°˜ 3-Fold CV\n",
        "            for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
        "                try:\n",
        "                    X_train, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n",
        "                    y_train, y_val = y_full[train_idx], y_full[val_idx]\n",
        "                    \n",
        "                    # ë°ì´í„° í¬ê¸° í™•ì¸\n",
        "                    if len(X_train) < 10 or len(X_val) < 5:\n",
        "                        print(f\"     Fold {fold+1}: ë°ì´í„° ë¶€ì¡± (train={len(X_train)}, val={len(X_val)})\")\n",
        "                        continue\n",
        "                    \n",
        "                    # CatBoost ëª¨ë¸ ìƒì„± ë° í›ˆë ¨\n",
        "                    model = CatBoostRegressor(**params, cat_features=categorical_features)\n",
        "                    model.fit(X_train, y_train, verbose=0)\n",
        "                    \n",
        "                    # ì˜ˆì¸¡ ë° í‰ê°€\n",
        "                    predictions = model.predict(X_val)\n",
        "                    predictions = np.maximum(predictions, 0)  # ìŒìˆ˜ ì œê±°\n",
        "                    \n",
        "                    # í‰ê°€ ì§€í‘œ ê³„ì‚°\n",
        "                    metrics = evaluate_predictions(y_val, predictions, delta=1.0)\n",
        "                    print(f\"     Fold {fold+1}: RMSE={metrics['rmse']:.4f}, Huber={metrics['huber']:.4f}\")\n",
        "                    \n",
        "                    cv_scores.append(metrics['huber'])  # ìµœì í™”ëŠ” Huber Loss ê¸°ì¤€\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Fold {fold+1} CatBoost ìµœì í™” ì‹¤íŒ¨:\")\n",
        "                    print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                    print(f\"   í›ˆë ¨ ë°ì´í„°: {len(X_train) if 'X_train' in locals() else 'N/A'}\")\n",
        "                    print(f\"   ê²€ì¦ ë°ì´í„°: {len(X_val) if 'X_val' in locals() else 'N/A'}\")\n",
        "                    raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "            \n",
        "            if len(cv_scores) == 0:\n",
        "                print(f\"   âš ï¸ ëª¨ë“  Foldì—ì„œ CatBoost ìµœì í™” ì‹¤íŒ¨\")\n",
        "                return 999.0\n",
        "                \n",
        "            return np.mean(cv_scores)\n",
        "        \n",
        "        # Optuna ìµœì í™” ì‹¤í–‰\n",
        "        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=SEED))\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "        \n",
        "        self.best_params = study.best_params\n",
        "        print(f\"   CatBoost ìµœì  Huber Loss: {study.best_value:.4f}\")\n",
        "        print(f\"   ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params}\")\n",
        "        return study.best_value\n",
        "\n",
        "    def fit(self, df, target_col='heat_demand'):\n",
        "        \"\"\"ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ ë°ì´í„° í›ˆë ¨\"\"\"\n",
        "        print(f\"CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\")\n",
        "        \n",
        "        # í”¼ì³ ì¤€ë¹„\n",
        "        X, categorical_features = self.prepare_features(df)\n",
        "        y = df[target_col].values\n",
        "        \n",
        "        # ë°ì´í„° í’ˆì§ˆ í™•ì¸\n",
        "        print(f\"   í›ˆë ¨ ë°ì´í„°: {X.shape}\")\n",
        "        print(f\"   íƒ€ê²Ÿ í†µê³„: í‰ê· ={y.mean():.2f}, í‘œì¤€í¸ì°¨={y.std():.2f}, ë²”ìœ„=[{y.min():.2f}, {y.max():.2f}]\")\n",
        "        \n",
        "        # ê²°ì¸¡ì¹˜ í™•ì¸\n",
        "        missing_info = {}\n",
        "        for col in X.columns:\n",
        "            missing_count = X[col].isna().sum()\n",
        "            if missing_count > 0:\n",
        "                missing_info[col] = missing_count\n",
        "        \n",
        "        if missing_info:\n",
        "            print(f\"   âš ï¸ í”¼ì³ë³„ ê²°ì¸¡ì¹˜: {missing_info}\")\n",
        "            # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
        "            for col, missing_count in missing_info.items():\n",
        "                if col in categorical_features:\n",
        "                    X[col] = X[col].fillna('missing')\n",
        "                else:\n",
        "                    X[col] = X[col].fillna(X[col].median())\n",
        "            print(f\"   ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "        try:\n",
        "            # âœ… ë‹¨ì¡°ì„± ì œì•½ ê³„ì‚° ###############################################################\n",
        "            monotone_constraints = self._get_monotone_constraints(X.columns)\n",
        "            # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±\n",
        "            self.model = CatBoostRegressor(\n",
        "                iterations=self.best_params.get('iterations', 1000),\n",
        "                learning_rate=self.best_params.get('learning_rate', 0.1),\n",
        "                depth=self.best_params.get('depth', 6),\n",
        "                l2_leaf_reg=self.best_params.get('l2_leaf_reg', 3),\n",
        "                border_count=self.best_params.get('border_count', 128),\n",
        "                cat_features=categorical_features,\n",
        "                random_seed=SEED,\n",
        "                verbose=False,\n",
        "                allow_writing_files=False,\n",
        "                loss_function='Huber:delta=1.0',  # Huber Loss ì‚¬ìš©\n",
        "                # âœ… ë‹¨ì¡°ì„± ì œì•½ ì¶”ê°€ ì ìš© ###############################################################\n",
        "                monotone_constraints=monotone_constraints\n",
        "            )\n",
        "\n",
        "            # ëª¨ë¸ í›ˆë ¨\n",
        "            self.model.fit(X, y)\n",
        "            print(f\"   CatBoost í›ˆë ¨ ì™„ë£Œ\")\n",
        "            \n",
        "            # í”¼ì³ ì¤‘ìš”ë„ ì¶œë ¥ (ìƒìœ„ 10ê°œ)\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                feature_importance = dict(zip(X.columns, self.model.feature_importances_))\n",
        "                top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "                print(f\"   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\")\n",
        "                for i, (feature, importance) in enumerate(top_features, 1):\n",
        "                    print(f\"     {i:2d}. {feature}: {importance:.3f}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ CatBoost í›ˆë ¨ ì‹¤íŒ¨:\")\n",
        "            print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "            print(f\"   ë°ì´í„° í¬ê¸°: {X.shape}\")\n",
        "            print(f\"   ë²”ì£¼í˜• í”¼ì³: {categorical_features}\")\n",
        "            print(f\"   íŒŒë¼ë¯¸í„°: {self.best_params}\")\n",
        "            raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "\n",
        "    def predict(self, df):\n",
        "        \"\"\"ì˜ˆì¸¡ ì‹¤í–‰\"\"\"\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"í›ˆë ¨ëœ CatBoost ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "            \n",
        "        try:\n",
        "            # í”¼ì³ ì¤€ë¹„\n",
        "            X, categorical_features = self.prepare_features(df)\n",
        "            \n",
        "            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (í›ˆë ¨ ì‹œì™€ ë™ì¼í•˜ê²Œ)\n",
        "            for col in X.columns:\n",
        "                if X[col].isna().sum() > 0:\n",
        "                    if col in categorical_features:\n",
        "                        X[col] = X[col].fillna('missing')\n",
        "                    else:\n",
        "                        X[col] = X[col].fillna(X[col].median())\n",
        "            \n",
        "            predictions = self.model.predict(X)\n",
        "            predictions = np.maximum(predictions, 0)  # ìŒìˆ˜ ì œê±°\n",
        "            \n",
        "            print(f\"   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ\")\n",
        "            print(f\"   ì˜ˆì¸¡ í†µê³„: í‰ê· ={predictions.mean():.2f}, ë²”ìœ„=[{predictions.min():.2f}, {predictions.max():.2f}]\")\n",
        "            \n",
        "            return predictions\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ CatBoost ì˜ˆì¸¡ ì‹¤íŒ¨:\")\n",
        "            print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "            print(f\"   ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
        "            raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "\n",
        "print(\"CatBoost ìµœì í™” ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stacking"
      },
      "source": [
        "## 7. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í´ë˜ìŠ¤ (Ridge ë©”íƒ€ëª¨ë¸ ìµœì í™”) v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ê³ ë„í™”ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n",
            "\n",
            "ğŸ¯ 2ê°œ ê·¸ë£¹ë³„ ê°œë³„ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.linear_model import Ridge\n",
        "\n",
        "# class AdvancedStackingEnsemble:\n",
        "#     def __init__(self, season_type=\"heating\", group_name=\"\"):\n",
        "#         self.season_type = season_type\n",
        "#         self.group_name = group_name\n",
        "#         self.models = {\n",
        "#             'prophet': ProphetOptimizedModel(season_type),\n",
        "#             'catboost': CatBoostOptimizedModel(season_type)\n",
        "#             # 'lstm': LSTMOptimizedModel(season_type)\n",
        "#         }\n",
        "#         self.meta_model = None\n",
        "#         self.best_meta_params = {}\n",
        "#         self.individual_scores = {}\n",
        "\n",
        "#     def optimize_meta_model(self, level1_features, targets, cv_splits, n_trials=20):\n",
        "#         \"\"\"Ridge ë©”íƒ€ëª¨ë¸ ìµœì í™” (ì—°ë„ ê¸°ë°˜ CV)\"\"\"\n",
        "#         print(f\"Ridge ë©”íƒ€ëª¨ë¸ ìµœì í™” ì¤‘... (trials: {n_trials})\")\n",
        "        \n",
        "#         def objective(trial):\n",
        "#             # Ridge íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§\n",
        "#             alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
        "            \n",
        "#             cv_scores = []\n",
        "            \n",
        "#             # ì—°ë„ ê¸°ë°˜ 3-Fold CV\n",
        "#             for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
        "#                 try:\n",
        "#                     # ë ˆë²¨1 í”¼ì³ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ ì„ íƒ\n",
        "#                     train_meta_mask = np.isin(range(len(level1_features)), train_idx)\n",
        "#                     val_meta_mask = np.isin(range(len(level1_features)), val_idx)\n",
        "                    \n",
        "#                     X_train_meta = level1_features[train_meta_mask]\n",
        "#                     X_val_meta = level1_features[val_meta_mask]\n",
        "#                     y_train_meta = targets[train_meta_mask]\n",
        "#                     y_val_meta = targets[val_meta_mask]\n",
        "                    \n",
        "#                     if len(X_train_meta) < 5 or len(X_val_meta) < 2:\n",
        "#                         print(f\"     ë©”íƒ€ Fold {fold+1}: ë°ì´í„° ë¶€ì¡±\")\n",
        "#                         continue\n",
        "                    \n",
        "#                     # Ridge í›ˆë ¨\n",
        "#                     model = Ridge(alpha=alpha, random_state=SEED)\n",
        "#                     model.fit(X_train_meta, y_train_meta)\n",
        "                    \n",
        "#                     # ì˜ˆì¸¡ ë° í‰ê°€\n",
        "#                     pred = model.predict(X_val_meta)\n",
        "#                     pred = np.maximum(pred, 0)  # ìŒìˆ˜ ì œê±°\n",
        "                    \n",
        "#                     rmse = np.sqrt(mean_squared_error(y_val_meta, pred))\n",
        "#                     cv_scores.append(rmse)\n",
        "#                     print(f\"     ë©”íƒ€ Fold {fold+1}: RMSE = {rmse:.4f}\")\n",
        "                    \n",
        "#                 except Exception as e:\n",
        "#                     print(f\"âŒ ë©”íƒ€ Fold {fold+1} ìµœì í™” ì‹¤íŒ¨:\")\n",
        "#                     print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "#                     raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "                    \n",
        "#             if len(cv_scores) == 0:\n",
        "#                 print(f\"   âš ï¸ ëª¨ë“  ë©”íƒ€ Foldì—ì„œ ìµœì í™” ì‹¤íŒ¨\")\n",
        "#                 return 999.0\n",
        "                \n",
        "#             return np.mean(cv_scores)\n",
        "        \n",
        "#         # Optuna ìµœì í™” ì‹¤í–‰\n",
        "#         study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=SEED))\n",
        "#         study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "        \n",
        "#         self.best_meta_params = study.best_params\n",
        "#         print(f\"   Ridge ìµœì  RMSE: {study.best_value:.4f}\")\n",
        "#         print(f\"   ìµœì  íŒŒë¼ë¯¸í„°: {self.best_meta_params}\")\n",
        "        \n",
        "#         return study.best_value\n",
        "\n",
        "#     def fit(self, train_df, cv_splits, target_col='heat_demand', optimize_trials=None):\n",
        "#         \"\"\"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ (ì—°ë„ ê¸°ë°˜ CV ì‚¬ìš©)\"\"\"\n",
        "#         print(f\"\\n{self.group_name} ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘!\")\n",
        "#         print(\"=\" * 60)\n",
        "        \n",
        "#         if len(train_df) < 100:\n",
        "#             raise RuntimeError(f\"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(train_df)}ê°œ). ìµœì†Œ 100ê°œ í•„ìš”.\")\n",
        "            \n",
        "#         # ê¸°ë³¸ trials ì„¤ì •\n",
        "#         if optimize_trials is None:\n",
        "#             optimize_trials = {'prophet': 30, 'catboost': 50, 'meta': 20}\n",
        "\n",
        "#         print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
        "#         print(f\"ì—°ë„ ë¶„í¬: {dict(train_df['year'].value_counts().sort_index())}\")\n",
        "\n",
        "#         # 1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° í›ˆë ¨\n",
        "#         level1_predictions_dict = {}\n",
        "\n",
        "#         for name, model in self.models.items():\n",
        "#             print(f\"\\n{name.upper()} ìµœì í™” ë° í›ˆë ¨...\")\n",
        "#             try:\n",
        "#                 start_time = datetime.now()\n",
        "                \n",
        "#                 # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (CV ê¸°ë°˜)\n",
        "#                 best_score = model.optimize_hyperparameters(\n",
        "#                     train_df, cv_splits, target_col, n_trials=optimize_trials[name]\n",
        "#                 )\n",
        "                \n",
        "#                 # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í›ˆë ¨ ë°ì´í„°ì— ì¬í›ˆë ¨\n",
        "#                 model.fit(train_df, target_col)\n",
        "                \n",
        "#                 # CVë¥¼ í†µí•œ ë ˆë²¨1 ì˜ˆì¸¡ê°’ ìƒì„± (Out-of-Fold ì˜ˆì¸¡)\n",
        "#                 oof_predictions = np.zeros(len(train_df))\n",
        "                \n",
        "#                 for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
        "#                     print(f\"   OOF Fold {fold+1} ì²˜ë¦¬ ì¤‘...\")\n",
        "                    \n",
        "#                     fold_train = train_df.iloc[train_idx]\n",
        "#                     fold_val = train_df.iloc[val_idx]\n",
        "                    \n",
        "#                     # í´ë“œë³„ ëª¨ë¸ í›ˆë ¨ (ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n",
        "#                     if name == 'prophet':\n",
        "#                         fold_model = ProphetOptimizedModel(self.season_type)\n",
        "#                         fold_model.best_params = model.best_params\n",
        "#                         fold_model.fit(fold_train, target_col)\n",
        "#                         fold_pred = fold_model.predict(fold_val)\n",
        "#                     elif name == 'catboost':\n",
        "#                         fold_model = CatBoostOptimizedModel(self.season_type)\n",
        "#                         fold_model.best_params = model.best_params\n",
        "#                         fold_model.fit(fold_train, target_col)\n",
        "#                         fold_pred = fold_model.predict(fold_val)\n",
        "#                     # else:  # lstm\n",
        "#                     #     fold_model = LSTMOptimizedModel(self.season_type)\n",
        "#                     #     fold_model.best_params = model.best_params\n",
        "#                     #     # LSTMì€ CV splitsë¥¼ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•Šê³  fitë§Œ ìˆ˜í–‰\n",
        "#                     #     fold_model.fit(fold_train, target_col)\n",
        "#                     #     fold_pred = fold_model.predict(fold_val)\n",
        "                    \n",
        "#                     oof_predictions[val_idx] = fold_pred\n",
        "                    \n",
        "#                     # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (LSTMì˜ ê²½ìš°)\n",
        "#                     if name == 'lstm' and torch.cuda.is_available():\n",
        "#                         torch.cuda.empty_cache()\n",
        "\n",
        "#                 level1_predictions_dict[name] = oof_predictions\n",
        "\n",
        "#                 # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ê³„ì‚°\n",
        "#                 metrics = evaluate_predictions(train_df[target_col].values, oof_predictions, delta=1.0)\n",
        "#                 mae = mean_absolute_error(train_df[target_col].values, oof_predictions)\n",
        "#                 train_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "#                 self.individual_scores[name] = {\n",
        "#                     'rmse': metrics['rmse'],\n",
        "#                     'huber': metrics['huber'], \n",
        "#                     'mae': mae, \n",
        "#                     'optuna_score': best_score,\n",
        "#                     'train_time': train_time\n",
        "#                 }\n",
        "\n",
        "#                 print(f\"   {name} ì„±ëŠ¥: RMSE={metrics['rmse']:.4f}, Huber={metrics['huber']:.4f}, MAE={mae:.4f}\")\n",
        "#                 print(f\"   Optuna ìµœì  ì ìˆ˜: {best_score:.4f}\")\n",
        "#                 print(f\"   ì´ ì‹œê°„: {train_time:.1f}ì´ˆ\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"âŒ {name} í›ˆë ¨ ì‹¤íŒ¨:\")\n",
        "#                 print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "#                 raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "\n",
        "#         # 2ë‹¨ê³„: ë©”íƒ€ ëª¨ë¸ ìµœì í™” ë° í›ˆë ¨\n",
        "#         print(f\"\\nRidge ë©”íƒ€ ëª¨ë¸ ìµœì í™” ë° í›ˆë ¨...\")\n",
        "        \n",
        "#         # ë ˆë²¨1 í”¼ì³ êµ¬ì„±\n",
        "#         level1_features = np.column_stack(list(level1_predictions_dict.values()))\n",
        "#         targets = train_df[target_col].values\n",
        "        \n",
        "#         print(f\"   ë©”íƒ€ ëª¨ë¸ ì…ë ¥: {level1_features.shape}\")\n",
        "#         print(f\"   ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ í†µê³„:\")\n",
        "#         for i, (name, pred) in enumerate(level1_predictions_dict.items()):\n",
        "#             print(f\"     {name}: í‰ê· ={pred.mean():.2f}, í‘œì¤€í¸ì°¨={pred.std():.2f}\")\n",
        "        \n",
        "#         # ë©”íƒ€ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
        "#         meta_score = self.optimize_meta_model(level1_features, targets, cv_splits, optimize_trials['meta'])\n",
        "        \n",
        "#         # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ë©”íƒ€ëª¨ë¸ í›ˆë ¨\n",
        "#         try:\n",
        "#             self.meta_model = Ridge(\n",
        "#                 alpha=self.best_meta_params.get('alpha', 1.0),\n",
        "#                 random_state=SEED\n",
        "#             )\n",
        "#             self.meta_model.fit(level1_features, targets)\n",
        "\n",
        "#             # ìŠ¤íƒœí‚¹ ì„±ëŠ¥ ê³„ì‚°\n",
        "#             stacking_pred = self.meta_model.predict(level1_features)\n",
        "#             stacking_pred = np.maximum(stacking_pred, 0)  # ìŒìˆ˜ ì œê±°\n",
        "            \n",
        "#             stacking_metrics = evaluate_predictions(targets, stacking_pred, delta=1.0)\n",
        "#             stacking_mae = mean_absolute_error(targets, stacking_pred)\n",
        "\n",
        "#             self.individual_scores['stacking'] = {\n",
        "#                 'rmse': stacking_metrics['rmse'],\n",
        "#                 'huber': stacking_metrics['huber'], \n",
        "#                 'mae': stacking_mae,\n",
        "#                 'optuna_score': meta_score\n",
        "#             }\n",
        "            \n",
        "#             print(f\"   ìŠ¤íƒœí‚¹ ì„±ëŠ¥: RMSE={stacking_metrics['rmse']:.4f}, Huber={stacking_metrics['huber']:.4f}, MAE={stacking_mae:.4f}\")\n",
        "            \n",
        "#             # ë©”íƒ€ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¶œë ¥\n",
        "#             if hasattr(self.meta_model, 'coef_'):\n",
        "#                 model_names = list(level1_predictions_dict.keys())\n",
        "#                 print(f\"   ë©”íƒ€ëª¨ë¸ ê°€ì¤‘ì¹˜:\")\n",
        "#                 for i, (name, coef) in enumerate(zip(model_names, self.meta_model.coef_)):\n",
        "#                     print(f\"     {name}: {coef:.4f}\")\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             print(f\"âŒ ë©”íƒ€ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨:\")\n",
        "#             print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "#             raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "            \n",
        "#         print(f\"âœ… {self.group_name} ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!\")\n",
        "\n",
        "#     def predict(self, test_df):\n",
        "#         \"\"\"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
        "#         if self.meta_model is None:\n",
        "#             raise RuntimeError(f\"{self.group_name} ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "#         level1_predictions = {}\n",
        "\n",
        "#         # 1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡\n",
        "#         print(f\"   {self.group_name} ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
        "#         for name, model in self.models.items():\n",
        "#             try:\n",
        "#                 level1_predictions[name] = model.predict(test_df)\n",
        "#                 pred_stats = level1_predictions[name]\n",
        "#                 print(f\"     {name}: í‰ê· ={pred_stats.mean():.2f}, ë²”ìœ„=[{pred_stats.min():.2f}, {pred_stats.max():.2f}]\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"âŒ {name} ì˜ˆì¸¡ ì‹¤íŒ¨:\")\n",
        "#                 print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "#                 raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "\n",
        "#         # 2ë‹¨ê³„: ë©”íƒ€ ëª¨ë¸ ì˜ˆì¸¡\n",
        "#         try:\n",
        "#             meta_features = np.column_stack(list(level1_predictions.values()))\n",
        "#             final_pred = self.meta_model.predict(meta_features)\n",
        "#             final_pred = np.maximum(final_pred, 0)  # ìŒìˆ˜ ì œê±°\n",
        "\n",
        "#             print(f\"   {self.group_name} ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ ì™„ë£Œ: í‰ê· ={final_pred.mean():.2f}, ë²”ìœ„=[{final_pred.min():.2f}, {final_pred.max():.2f}]\")\n",
        "            \n",
        "#             return final_pred, level1_predictions\n",
        "            \n",
        "#         except Exception as e:\n",
        "#             print(f\"âŒ {self.group_name} ë©”íƒ€ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨:\")\n",
        "#             print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "#             raise e  # âœ… ì—ëŸ¬ ë°œìƒì‹œí‚¤ê³  ì¤‘ë‹¨\n",
        "\n",
        "# print(\"âœ… ê³ ë„í™”ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
        "\n",
        "# # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬\n",
        "# ensemble_models = {}\n",
        "# group_results = {}\n",
        "\n",
        "# print(\"\\nğŸ¯ 2ê°œ ê·¸ë£¹ë³„ ê°œë³„ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í´ë˜ìŠ¤ (Ridge ë©”íƒ€ëª¨ë¸ ìµœì í™”) v2 - íŒŒë¼ë¯¸í„° ê³ ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ìµœì  íŒŒë¼ë¯¸í„° ì ìš©ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n",
            "\n",
            "ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ 2ê°œ ê·¸ë£¹ë³„ ê°œë³„ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# ë‚œë°© ì‹œì¦Œ ëª¨ë¸\\nheating_ensemble = AdvancedStackingEnsemble(season_type=\"heating\", group_name=\"ë‚œë°©ì‹œì¦Œ\")\\nheating_ensemble.fit(heating_train_df, heating_cv_splits, use_predefined_params=True)\\n\\n# ë¹„ë‚œë°© ì‹œì¦Œ ëª¨ë¸  \\nnon_heating_ensemble = AdvancedStackingEnsemble(season_type=\"non_heating\", group_name=\"ë¹„ë‚œë°©ì‹œì¦Œ\")\\nnon_heating_ensemble.fit(non_heating_train_df, non_heating_cv_splits, use_predefined_params=True)\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "class AdvancedStackingEnsemble:\n",
        "    def __init__(self, season_type=\"heating\", group_name=\"\"):\n",
        "        self.season_type = season_type\n",
        "        self.group_name = group_name\n",
        "        self.models = {\n",
        "            'prophet': ProphetOptimizedModel(season_type),\n",
        "            'catboost': CatBoostOptimizedModel(season_type)\n",
        "            # 'lstm': LSTMOptimizedModel(season_type)\n",
        "        }\n",
        "        self.meta_model = None\n",
        "        self.best_meta_params = {}\n",
        "        self.individual_scores = {}\n",
        "        \n",
        "        # ğŸ¯ ë¯¸ë¦¬ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°ë“¤\n",
        "        self.predefined_params = self._get_predefined_params()\n",
        "\n",
        "    \n",
        "    def _get_predefined_params(self):\n",
        "        \"\"\"ì‹œì¦Œë³„ ìµœì  íŒŒë¼ë¯¸í„° ë°˜í™˜\"\"\"\n",
        "        if self.season_type == \"heating\":\n",
        "            # ë‚œë°© ì‹œì¦Œ ìµœì  íŒŒë¼ë¯¸í„°\n",
        "            return {\n",
        "                'prophet': {\n",
        "                    'changepoint_prior_scale': 0.0026364803038431655,\n",
        "                    'seasonality_prior_scale': 0.13066739238053282,\n",
        "                    'holidays_prior_scale': 5.3994844097874335,\n",
        "                    'seasonality_mode': 'multiplicative'\n",
        "                },\n",
        "                'catboost': {\n",
        "                    'iterations': 1678,\n",
        "                    'depth': 5,\n",
        "                    'learning_rate': 0.05748924681991978,\n",
        "                    'l2_leaf_reg': 12.255876808378806,\n",
        "                    'border_count': 42\n",
        "                },\n",
        "                'ridge': {\n",
        "                    'alpha': 63.512210106407046\n",
        "                },\n",
        "                'expected_rmse': {\n",
        "                    'ridge': 24.8784\n",
        "                }\n",
        "            }\n",
        "        else:\n",
        "            # ë¹„ë‚œë°© ì‹œì¦Œ ìµœì  íŒŒë¼ë¯¸í„°\n",
        "            return {\n",
        "                'prophet': {\n",
        "                    'changepoint_prior_scale': 0.0010695090612476649,\n",
        "                    'seasonality_prior_scale': 3.652041851774491,\n",
        "                    'holidays_prior_scale': 0.45441617690609376,\n",
        "                    'seasonality_mode': 'additive'\n",
        "                },\n",
        "                'catboost': {\n",
        "                    'iterations': 1818,\n",
        "                    'depth': 8,\n",
        "                    'learning_rate': 0.08527855281875678,\n",
        "                    'l2_leaf_reg': 14.955151393164728,\n",
        "                    'border_count': 35\n",
        "                },\n",
        "                'ridge': {\n",
        "                    'alpha': 63.512210106407046\n",
        "                },\n",
        "                'expected_rmse': {\n",
        "                    'ridge': 9.8138\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def fit(self, train_df, cv_splits, target_col='heat_demand', use_predefined_params=True):\n",
        "        \"\"\"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ (ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©)\"\"\"\n",
        "        print(f\"\\n{self.group_name} ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘! ({self.season_type} ì‹œì¦Œ)\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        if len(train_df) < 100:\n",
        "            raise RuntimeError(f\"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(train_df)}ê°œ). ìµœì†Œ 100ê°œ í•„ìš”.\")\n",
        "\n",
        "        print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
        "        print(f\"ì—°ë„ ë¶„í¬: {dict(train_df['year'].value_counts().sort_index())}\")\n",
        "        \n",
        "        if use_predefined_params:\n",
        "            print(f\"ğŸ¯ {self.season_type} ì‹œì¦Œ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\")\n",
        "            print(f\"   Prophet: {self.predefined_params['prophet']}\")\n",
        "            print(f\"   CatBoost: {self.predefined_params['catboost']}\")\n",
        "            print(f\"   Ridge: {self.predefined_params['ridge']}\")\n",
        "\n",
        "        # 1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ ìµœì  íŒŒë¼ë¯¸í„° ì„¤ì • ë° í›ˆë ¨\n",
        "        level1_predictions_dict = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\n{name.upper()} í›ˆë ¨...\")\n",
        "            try:\n",
        "                start_time = datetime.now()\n",
        "                \n",
        "                if use_predefined_params:\n",
        "                    # ğŸ¯ ë¯¸ë¦¬ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
        "                    model.best_params = self.predefined_params[name].copy()\n",
        "                    print(f\"   ìµœì  íŒŒë¼ë¯¸í„° ì ìš©: {model.best_params}\")\n",
        "                    best_score = self.predefined_params['expected_rmse'].get(name, 0.0)\n",
        "                else:\n",
        "                    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ê¸°ì¡´ ë°©ì‹)\n",
        "                    best_score = model.optimize_hyperparameters(\n",
        "                        train_df, cv_splits, target_col, n_trials=30\n",
        "                    )\n",
        "                \n",
        "                # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í›ˆë ¨ ë°ì´í„°ì— ì¬í›ˆë ¨\n",
        "                model.fit(train_df, target_col)\n",
        "                \n",
        "                # CVë¥¼ í†µí•œ ë ˆë²¨1 ì˜ˆì¸¡ê°’ ìƒì„± (Out-of-Fold ì˜ˆì¸¡)\n",
        "                oof_predictions = np.zeros(len(train_df))\n",
        "                \n",
        "                for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
        "                    print(f\"   OOF Fold {fold+1} ì²˜ë¦¬ ì¤‘...\")\n",
        "                    \n",
        "                    fold_train = train_df.iloc[train_idx]\n",
        "                    fold_val = train_df.iloc[val_idx]\n",
        "                    \n",
        "                    # í´ë“œë³„ ëª¨ë¸ í›ˆë ¨ (ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©)\n",
        "                    if name == 'prophet':\n",
        "                        fold_model = ProphetOptimizedModel(self.season_type)\n",
        "                        fold_model.best_params = model.best_params\n",
        "                        fold_model.fit(fold_train, target_col)\n",
        "                        fold_pred = fold_model.predict(fold_val)\n",
        "                    elif name == 'catboost':\n",
        "                        fold_model = CatBoostOptimizedModel(self.season_type)\n",
        "                        fold_model.best_params = model.best_params\n",
        "                        fold_model.fit(fold_train, target_col)\n",
        "                        fold_pred = fold_model.predict(fold_val)\n",
        "                    \n",
        "                    oof_predictions[val_idx] = fold_pred\n",
        "                    \n",
        "                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (í•„ìš”ì‹œ)\n",
        "                    if name == 'lstm' and torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                level1_predictions_dict[name] = oof_predictions\n",
        "\n",
        "                # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ê³„ì‚°\n",
        "                metrics = evaluate_predictions(train_df[target_col].values, oof_predictions, delta=1.0)\n",
        "                mae = mean_absolute_error(train_df[target_col].values, oof_predictions)\n",
        "                train_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "                self.individual_scores[name] = {\n",
        "                    'rmse': metrics['rmse'],\n",
        "                    'huber': metrics['huber'], \n",
        "                    'mae': mae, \n",
        "                    'optuna_score': best_score,\n",
        "                    'train_time': train_time\n",
        "                }\n",
        "\n",
        "                print(f\"   {name} ì„±ëŠ¥: RMSE={metrics['rmse']:.4f}, Huber={metrics['huber']:.4f}, MAE={mae:.4f}\")\n",
        "                if use_predefined_params:\n",
        "                    print(f\"   ì˜ˆìƒ ì ìˆ˜ ëŒ€ë¹„: {best_score:.4f}\")\n",
        "                else:\n",
        "                    print(f\"   Optuna ìµœì  ì ìˆ˜: {best_score:.4f}\")\n",
        "                print(f\"   ì´ ì‹œê°„: {train_time:.1f}ì´ˆ\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ {name} í›ˆë ¨ ì‹¤íŒ¨:\")\n",
        "                print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                raise e\n",
        "\n",
        "        # 2ë‹¨ê³„: ë©”íƒ€ ëª¨ë¸ í›ˆë ¨\n",
        "        print(f\"\\nRidge ë©”íƒ€ ëª¨ë¸ í›ˆë ¨...\")\n",
        "        \n",
        "        # ë ˆë²¨1 í”¼ì³ êµ¬ì„±\n",
        "        level1_features = np.column_stack(list(level1_predictions_dict.values()))\n",
        "        targets = train_df[target_col].values\n",
        "        \n",
        "        print(f\"   ë©”íƒ€ ëª¨ë¸ ì…ë ¥: {level1_features.shape}\")\n",
        "        print(f\"   ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ í†µê³„:\")\n",
        "        for i, (name, pred) in enumerate(level1_predictions_dict.items()):\n",
        "            print(f\"     {name}: í‰ê· ={pred.mean():.2f}, í‘œì¤€í¸ì°¨={pred.std():.2f}\")\n",
        "        \n",
        "        try:\n",
        "            if use_predefined_params:\n",
        "                # ğŸ¯ ë¯¸ë¦¬ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
        "                self.best_meta_params = self.predefined_params['ridge'].copy()\n",
        "                meta_score = self.predefined_params['expected_rmse']['ridge']\n",
        "                print(f\"   ìµœì  Ridge íŒŒë¼ë¯¸í„° ì ìš©: {self.best_meta_params}\")\n",
        "                print(f\"   ì˜ˆìƒ RMSE: {meta_score:.4f}\")\n",
        "            else:\n",
        "                # ë©”íƒ€ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ê¸°ì¡´ ë°©ì‹)\n",
        "                meta_score = self.optimize_meta_model(level1_features, targets, cv_splits, 20)\n",
        "            \n",
        "            # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ë©”íƒ€ëª¨ë¸ í›ˆë ¨\n",
        "            self.meta_model = Ridge(\n",
        "                alpha=self.best_meta_params.get('alpha', 1.0),\n",
        "                random_state=SEED\n",
        "            )\n",
        "            self.meta_model.fit(level1_features, targets)\n",
        "\n",
        "            # ìŠ¤íƒœí‚¹ ì„±ëŠ¥ ê³„ì‚°\n",
        "            stacking_pred = self.meta_model.predict(level1_features)\n",
        "            stacking_pred = np.maximum(stacking_pred, 0)  # ìŒìˆ˜ ì œê±°\n",
        "            \n",
        "            stacking_metrics = evaluate_predictions(targets, stacking_pred, delta=1.0)\n",
        "            stacking_mae = mean_absolute_error(targets, stacking_pred)\n",
        "\n",
        "            self.individual_scores['stacking'] = {\n",
        "                'rmse': stacking_metrics['rmse'],\n",
        "                'huber': stacking_metrics['huber'], \n",
        "                'mae': stacking_mae,\n",
        "                'optuna_score': meta_score\n",
        "            }\n",
        "            \n",
        "            print(f\"   ìŠ¤íƒœí‚¹ ì„±ëŠ¥: RMSE={stacking_metrics['rmse']:.4f}, Huber={stacking_metrics['huber']:.4f}, MAE={stacking_mae:.4f}\")\n",
        "            \n",
        "            # ë©”íƒ€ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¶œë ¥\n",
        "            if hasattr(self.meta_model, 'coef_'):\n",
        "                model_names = list(level1_predictions_dict.keys())\n",
        "                print(f\"   ë©”íƒ€ëª¨ë¸ ê°€ì¤‘ì¹˜:\")\n",
        "                for i, (name, coef) in enumerate(zip(model_names, self.meta_model.coef_)):\n",
        "                    print(f\"     {name}: {coef:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ë©”íƒ€ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨:\")\n",
        "            print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "            raise e\n",
        "            \n",
        "        print(f\"âœ… {self.group_name} ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!\")\n",
        "\n",
        "    def optimize_meta_model(self, level1_features, targets, cv_splits, n_trials=20):\n",
        "        \"\"\"Ridge ë©”íƒ€ëª¨ë¸ ìµœì í™” (ê¸°ì¡´ ì½”ë“œ ìœ ì§€ - í•„ìš”ì‹œ ì‚¬ìš©)\"\"\"\n",
        "        print(f\"Ridge ë©”íƒ€ëª¨ë¸ ìµœì í™” ì¤‘... (trials: {n_trials})\")\n",
        "        \n",
        "        def objective(trial):\n",
        "            alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
        "            \n",
        "            cv_scores = []\n",
        "            \n",
        "            for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
        "                try:\n",
        "                    train_meta_mask = np.isin(range(len(level1_features)), train_idx)\n",
        "                    val_meta_mask = np.isin(range(len(level1_features)), val_idx)\n",
        "                    \n",
        "                    X_train_meta = level1_features[train_meta_mask]\n",
        "                    X_val_meta = level1_features[val_meta_mask]\n",
        "                    y_train_meta = targets[train_meta_mask]\n",
        "                    y_val_meta = targets[val_meta_mask]\n",
        "                    \n",
        "                    if len(X_train_meta) < 5 or len(X_val_meta) < 2:\n",
        "                        print(f\"     ë©”íƒ€ Fold {fold+1}: ë°ì´í„° ë¶€ì¡±\")\n",
        "                        continue\n",
        "                    \n",
        "                    model = Ridge(alpha=alpha, random_state=SEED)\n",
        "                    model.fit(X_train_meta, y_train_meta)\n",
        "                    \n",
        "                    pred = model.predict(X_val_meta)\n",
        "                    pred = np.maximum(pred, 0)\n",
        "                    \n",
        "                    rmse = np.sqrt(mean_squared_error(y_val_meta, pred))\n",
        "                    cv_scores.append(rmse)\n",
        "                    print(f\"     ë©”íƒ€ Fold {fold+1}: RMSE = {rmse:.4f}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ ë©”íƒ€ Fold {fold+1} ìµœì í™” ì‹¤íŒ¨:\")\n",
        "                    print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                    raise e\n",
        "                    \n",
        "            if len(cv_scores) == 0:\n",
        "                print(f\"   âš ï¸ ëª¨ë“  ë©”íƒ€ Foldì—ì„œ ìµœì í™” ì‹¤íŒ¨\")\n",
        "                return 999.0\n",
        "                \n",
        "            return np.mean(cv_scores)\n",
        "        \n",
        "        study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=SEED))\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "        \n",
        "        self.best_meta_params = study.best_params\n",
        "        print(f\"   Ridge ìµœì  RMSE: {study.best_value:.4f}\")\n",
        "        print(f\"   ìµœì  íŒŒë¼ë¯¸í„°: {self.best_meta_params}\")\n",
        "        \n",
        "        return study.best_value\n",
        "\n",
        "    def predict(self, test_df):\n",
        "        \"\"\"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì˜ˆì¸¡\"\"\"\n",
        "        if self.meta_model is None:\n",
        "            raise RuntimeError(f\"{self.group_name} ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "        level1_predictions = {}\n",
        "\n",
        "        # 1ë‹¨ê³„: ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡\n",
        "        print(f\"   {self.group_name} ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\")\n",
        "        for name, model in self.models.items():\n",
        "            try:\n",
        "                level1_predictions[name] = model.predict(test_df)\n",
        "                pred_stats = level1_predictions[name]\n",
        "                print(f\"     {name}: í‰ê· ={pred_stats.mean():.2f}, ë²”ìœ„=[{pred_stats.min():.2f}, {pred_stats.max():.2f}]\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ {name} ì˜ˆì¸¡ ì‹¤íŒ¨:\")\n",
        "                print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "                raise e\n",
        "\n",
        "        # 2ë‹¨ê³„: ë©”íƒ€ ëª¨ë¸ ì˜ˆì¸¡\n",
        "        try:\n",
        "            meta_features = np.column_stack(list(level1_predictions.values()))\n",
        "            final_pred = self.meta_model.predict(meta_features)\n",
        "            final_pred = np.maximum(final_pred, 0)  # ìŒìˆ˜ ì œê±°\n",
        "\n",
        "            print(f\"   {self.group_name} ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ ì™„ë£Œ: í‰ê· ={final_pred.mean():.2f}, ë²”ìœ„=[{final_pred.min():.2f}, {final_pred.max():.2f}]\")\n",
        "            \n",
        "            return final_pred, level1_predictions\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {self.group_name} ë©”íƒ€ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨:\")\n",
        "            print(f\"   ì—ëŸ¬: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "print(\"âœ… ìµœì  íŒŒë¼ë¯¸í„° ì ìš©ëœ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬\n",
        "ensemble_models = {}\n",
        "group_results = {}\n",
        "\n",
        "print(\"\\nğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ 2ê°œ ê·¸ë£¹ë³„ ê°œë³„ í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "\"\"\"\n",
        "# ë‚œë°© ì‹œì¦Œ ëª¨ë¸\n",
        "heating_ensemble = AdvancedStackingEnsemble(season_type=\"heating\", group_name=\"ë‚œë°©ì‹œì¦Œ\")\n",
        "heating_ensemble.fit(heating_train_df, heating_cv_splits, use_predefined_params=True)\n",
        "\n",
        "# ë¹„ë‚œë°© ì‹œì¦Œ ëª¨ë¸  \n",
        "non_heating_ensemble = AdvancedStackingEnsemble(season_type=\"non_heating\", group_name=\"ë¹„ë‚œë°©ì‹œì¦Œ\")\n",
        "non_heating_ensemble.fit(non_heating_train_df, non_heating_cv_splits, use_predefined_params=True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 8. ê·¸ë£¹ë³„ ê°œë³„ í›ˆë ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (í˜„ì¬) Prophet\n",
        "\n",
        "\"ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì€ í›„, ì§€ì‚¬ë³„ë¡œ í•™ìŠµ\" âœ…\n",
        "\n",
        "í•˜ì´í¼íŒŒë¼ë¯¸í„°: ëª¨ë“  ì§€ì‚¬ í†µí•© ì„±ëŠ¥ìœ¼ë¡œ ìµœì í™”\n",
        "ëª¨ë¸ í›ˆë ¨: ì°¾ì€ íŒŒë¼ë¯¸í„°ë¡œ ì§€ì‚¬ë³„ ê°œë³„ ëª¨ë¸ ìƒì„±\n",
        "\n",
        "2. ì‹¤ì œë¡œëŠ”\n",
        "\n",
        "í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ëª¨ë“  ì§€ì‚¬ì— ì ìš©í•´ì„œ í…ŒìŠ¤íŠ¸\n",
        "ì§€ì‚¬ë³„ ì„±ëŠ¥ì„ ì¢…í•©í•´ì„œ ê·¸ íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ì ìˆ˜ ê³„ì‚°\n",
        "30ë²ˆ ë°˜ë³µí•´ì„œ ê°€ì¥ ì¢‹ì€ íŒŒë¼ë¯¸í„° ì¡°í•© ì°¾ê¸°\n",
        "\n",
        "3. ìµœì¢… ê²°ê³¼\n",
        "\n",
        "ì „ì²´ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ ì„ íƒ\n",
        "ì´ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë“  ì§€ì‚¬ì— ë™ì¼í•˜ê²Œ ì ìš©í•´ì„œ ê°œë³„ ëª¨ë¸ í›ˆë ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "train_all_groups"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ ì‹œì¦Œë³„ 2ê°œ ê·¸ë£¹ í›ˆë ¨ ì‹œì‘ (ì—°ë„ ê¸°ë°˜ 3-Fold CV)\n",
            "\n",
            "============================================================\n",
            "ğŸ”¥ HEATING ê·¸ë£¹ í›ˆë ¨\n",
            "ğŸ“Š ë°ì´í„° í¬ê¸°: 289,997ê°œ\n",
            "ğŸ¢ ì§€ì‚¬ ìˆ˜: 19ê°œ\n",
            "ğŸ“… ì—°ë„ ë¶„í¬: {2021: 96653, 2022: 96672, 2023: 96672}\n",
            "ğŸ¯ íƒ€ê²Ÿ í†µê³„: í‰ê· =135.94, í‘œì¤€í¸ì°¨=135.29\n",
            "ğŸ”„ heating CV ë¶„í•  ìƒì„± ì¤‘...\n",
            "heating ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\n",
            "   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\n",
            "     2021ë…„: 96,653ê°œ\n",
            "     2022ë…„: 96,672ê°œ\n",
            "     2023ë…„: 96,672ê°œ\n",
            "   Fold 2021: í›ˆë ¨ 193,344ê°œ, ê²€ì¦ 96,653ê°œ\n",
            "   Fold 2022: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "   Fold 2023: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "ğŸ—ï¸ heating ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...\n",
            "ğŸš€ heating í›ˆë ¨ ì‹œì‘...\n",
            "\n",
            "heating ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘!\n",
            "============================================================\n",
            "í›ˆë ¨ ë°ì´í„°: 289,997ê°œ\n",
            "ì—°ë„ ë¶„í¬: {2021: 96653, 2022: 96672, 2023: 96672}\n",
            "\n",
            "PROPHET ìµœì í™” ë° í›ˆë ¨...\n",
            "Prophet Huber Loss í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... (trials: 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2686c049abb48de9b8c2937057477bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[W 2025-06-24 11:03:40,374] Trial 0 failed with parameters: {'changepoint_prior_scale': 0.010253509690168494, 'seasonality_prior_scale': 7.969454818643936, 'holidays_prior_scale': 2.9106359131330697, 'seasonality_mode': 'additive'} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/jisupark_1/workspace/star_track_python/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/var/folders/pt/8357krnj4tv48kdjx9mrhd3r0000gp/T/ipykernel_36917/2016375091.py\", line 68, in objective\n",
            "    model.fit(prophet_df)\n",
            "  File \"/Users/jisupark_1/workspace/star_track_python/.venv/lib/python3.10/site-packages/prophet/forecaster.py\", line 1235, in fit\n",
            "    self.params = self.stan_backend.fit(stan_init, dat, **kwargs)\n",
            "  File \"/Users/jisupark_1/workspace/star_track_python/.venv/lib/python3.10/site-packages/prophet/models.py\", line 126, in fit\n",
            "    self.stan_fit = self.model.optimize(**args)\n",
            "  File \"/Users/jisupark_1/workspace/star_track_python/.venv/lib/python3.10/site-packages/cmdstanpy/model.py\", line 644, in optimize\n",
            "    self._run_cmdstan(\n",
            "  File \"/Users/jisupark_1/workspace/star_track_python/.venv/lib/python3.10/site-packages/cmdstanpy/model.py\", line 2087, in _run_cmdstan\n",
            "    line = proc.stdout.readline()\n",
            "KeyboardInterrupt\n",
            "[W 2025-06-24 11:03:40,380] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# ëª¨ë“  ê·¸ë£¹ í›ˆë ¨ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mtrain_all_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[19], line 60\u001b[0m, in \u001b[0;36mtrain_all_groups\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m í›ˆë ¨ ì‹œì‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 60\u001b[0m \u001b[43mensemble_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_cv_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheat_demand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrials\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m total_time \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì €ì¥\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[18], line 99\u001b[0m, in \u001b[0;36mAdvancedStackingEnsemble.fit\u001b[0;34m(self, train_df, cv_splits, target_col, optimize_trials)\u001b[0m\n\u001b[1;32m     96\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (CV ê¸°ë°˜)\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_trials\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í›ˆë ¨ ë°ì´í„°ì— ì¬í›ˆë ¨\u001b[39;00m\n\u001b[1;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df, target_col)\n",
            "Cell \u001b[0;32mIn[15], line 115\u001b[0m, in \u001b[0;36mProphetOptimizedModel.optimize_hyperparameters\u001b[0;34m(self, df, cv_splits, target_col, n_trials)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Optuna ìµœì í™” ì‹¤í–‰\u001b[39;00m\n\u001b[1;32m    114\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mSEED))\n\u001b[0;32m--> 115\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Prophet ìµœì  Huber Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[15], line 68\u001b[0m, in \u001b[0;36mProphetOptimizedModel.optimize_hyperparameters.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reg \u001b[38;5;129;01min\u001b[39;00m prophet_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m reg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     66\u001b[0m         model\u001b[38;5;241m.\u001b[39madd_regressor(reg)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprophet_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„\u001b[39;00m\n\u001b[1;32m     71\u001b[0m future_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mto_datetime(branch_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtm\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     73\u001b[0m })\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/prophet/forecaster.py:1235\u001b[0m, in \u001b[0;36mProphet.fit\u001b[0;34m(self, df, **kwargs)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_backend\u001b[38;5;241m.\u001b[39msampling(stan_init, dat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcmc_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstan_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstan_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_backend\u001b[38;5;241m.\u001b[39mstan_fit\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;66;03m# If no changepoints were requested, replace delta with 0s\u001b[39;00m\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/prophet/models.py:126\u001b[0m, in \u001b[0;36mCmdStanPyBackend.fit\u001b[0;34m(self, stan_init, stan_data, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m args\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstan_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Fall back on Newton\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewton_fallback \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNewton\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/cmdstanpy/model.py:644\u001b[0m, in \u001b[0;36mCmdStanModel.optimize\u001b[0;34m(self, data, seed, inits, output_dir, sig_figs, save_profile, algorithm, init_alpha, tol_obj, tol_rel_obj, tol_grad, tol_rel_grad, tol_param, history_size, iter, save_iterations, require_converged, show_console, refresh, time_fmt, timeout, jacobian)\u001b[0m\n\u001b[1;32m    642\u001b[0m     dummy_chain_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    643\u001b[0m     runset \u001b[38;5;241m=\u001b[39m RunSet(args\u001b[38;5;241m=\u001b[39margs, chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, time_fmt\u001b[38;5;241m=\u001b[39mtime_fmt)\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_cmdstan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrunset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_chain_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_console\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_console\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m runset\u001b[38;5;241m.\u001b[39mraise_for_timeouts()\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runset\u001b[38;5;241m.\u001b[39m_check_retcodes():\n",
            "File \u001b[0;32m~/workspace/star_track_python/.venv/lib/python3.10/site-packages/cmdstanpy/model.py:2087\u001b[0m, in \u001b[0;36mCmdStanModel._run_cmdstan\u001b[0;34m(self, runset, idx, show_progress, show_console, progress_hook, timeout)\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2087\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2088\u001b[0m         fd_out\u001b[38;5;241m.\u001b[39mwrite(line)\n\u001b[1;32m   2089\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# # ëª¨ë“  ê·¸ë£¹ í›ˆë ¨ í•¨ìˆ˜\n",
        "# def train_all_groups():\n",
        "#     \"\"\"2ê°œ ê·¸ë£¹ í›ˆë ¨ (ë‚œë°©/ë¹„ë‚œë°©) - ì—°ë„ ê¸°ë°˜ CV ì ìš©\"\"\"\n",
        "#     group_configs = {\n",
        "#         \"heating\": {\n",
        "#             \"season\": \"heating\", \n",
        "#             # \"trials\": {\"prophet\": 40, \"catboost\": 60, \"meta\": 25}\n",
        "#             \"trials\": {\"prophet\": 1, \"catboost\": 1, \"meta\": 1}\n",
        "#         },\n",
        "#         \"non_heating\": {\n",
        "#             \"season\": \"non_heating\", \n",
        "#             # \"trials\": {\"prophet\": 30, \"catboost\": 50, \"meta\": 20}\n",
        "#             \"trials\": {\"prophet\": 1, \"catboost\": 1,  \"meta\": 1}\n",
        "#         }\n",
        "#     }\n",
        "    \n",
        "#     print(\"ğŸš€ ì‹œì¦Œë³„ 2ê°œ ê·¸ë£¹ í›ˆë ¨ ì‹œì‘ (ì—°ë„ ê¸°ë°˜ 3-Fold CV)\")\n",
        "#     total_start_time = datetime.now()\n",
        "    \n",
        "#     for group_name, config in group_configs.items():\n",
        "#         print(f\"\\n{'='*60}\")\n",
        "#         print(f\"ğŸ”¥ {group_name.upper()} ê·¸ë£¹ í›ˆë ¨\")\n",
        "        \n",
        "#         # ê·¸ë£¹ ë°ì´í„° ê²€ì¦\n",
        "#         if group_name not in train_groups:\n",
        "#             raise KeyError(f\"'{group_name}' ê·¸ë£¹ì´ train_groupsì— ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            \n",
        "#         group_data = train_groups[group_name]\n",
        "        \n",
        "#         if len(group_data) == 0:\n",
        "#             raise ValueError(f\"{group_name} ê·¸ë£¹ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        \n",
        "#         if 'heat_demand' not in group_data.columns:\n",
        "#             raise ValueError(f\"{group_name} ê·¸ë£¹ì— 'heat_demand' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        \n",
        "#         print(f\"ğŸ“Š ë°ì´í„° í¬ê¸°: {len(group_data):,}ê°œ\")\n",
        "#         print(f\"ğŸ¢ ì§€ì‚¬ ìˆ˜: {group_data['branch_id'].nunique()}ê°œ\")\n",
        "#         print(f\"ğŸ“… ì—°ë„ ë¶„í¬: {dict(group_data['year'].value_counts().sort_index())}\")\n",
        "#         print(f\"ğŸ¯ íƒ€ê²Ÿ í†µê³„: í‰ê· ={group_data['heat_demand'].mean():.2f}, í‘œì¤€í¸ì°¨={group_data['heat_demand'].std():.2f}\")\n",
        "        \n",
        "#         # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰ í™•ì¸\n",
        "#         if len(group_data) < 1000:\n",
        "#             raise ValueError(f\"{group_name} ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(group_data):,}ê°œ). ìµœì†Œ 1,000ê°œ í•„ìš”.\")\n",
        "        \n",
        "#         # ê·¸ë£¹ë³„ CV ë¶„í•  ìƒì„±\n",
        "#         print(f\"ğŸ”„ {group_name} CV ë¶„í•  ìƒì„± ì¤‘...\")\n",
        "#         group_cv_splits = create_year_based_cv_splits(group_data, group_name)\n",
        "        \n",
        "#         # ì•™ìƒë¸” ëª¨ë¸ ìƒì„±\n",
        "#         print(f\"ğŸ—ï¸ {group_name} ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
        "#         ensemble_models[group_name] = AdvancedStackingEnsemble(\n",
        "#             season_type=config[\"season\"], \n",
        "#             group_name=group_name\n",
        "#         )\n",
        "        \n",
        "#         # í›ˆë ¨ ì‹¤í–‰\n",
        "#         print(f\"ğŸš€ {group_name} í›ˆë ¨ ì‹œì‘...\")\n",
        "#         start_time = datetime.now()\n",
        "        \n",
        "#         ensemble_models[group_name].fit(\n",
        "#             group_data, \n",
        "#             group_cv_splits,\n",
        "#             target_col='heat_demand',\n",
        "#             optimize_trials=config[\"trials\"]\n",
        "#         )\n",
        "        \n",
        "#         total_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "#         # ê²°ê³¼ ì €ì¥\n",
        "#         group_results[group_name] = {\n",
        "#             'scores': ensemble_models[group_name].individual_scores.copy(),\n",
        "#             'total_time': total_time,\n",
        "#             'data_size': len(group_data),\n",
        "#             'branch_count': group_data['branch_id'].nunique(),\n",
        "#             'year_distribution': dict(group_data['year'].value_counts().sort_index())\n",
        "#         }\n",
        "        \n",
        "#         # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥ (Huber Loss í¬í•¨)\n",
        "#         print(f\"\\nğŸ“ˆ {group_name} ìµœì¢… ê²°ê³¼:\")\n",
        "#         print(f\"{'ëª¨ë¸':12s} {'RMSE':>8s} {'Huber':>8s} {'MAE':>8s} {'ì‹œê°„(ì´ˆ)':>8s}\")\n",
        "#         print(\"-\" * 50)\n",
        "        \n",
        "#         for model, scores in ensemble_models[group_name].individual_scores.items():\n",
        "#             rmse = scores.get('rmse', 999)\n",
        "#             huber = scores.get('huber', 999)\n",
        "#             mae = scores.get('mae', 999)\n",
        "#             model_time = scores.get('train_time', 0)\n",
        "            \n",
        "#             print(f\"{model:12s} {rmse:8.4f} {huber:8.4f} {mae:8.4f} {model_time:8.1f}\")\n",
        "        \n",
        "#         print(f\"   â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)\")\n",
        "        \n",
        "#         # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸\n",
        "#         best_model = min(\n",
        "#             [(name, score.get('huber', 999)) for name, score in ensemble_models[group_name].individual_scores.items()],\n",
        "#             key=lambda x: x[1]\n",
        "#         )\n",
        "#         print(f\"   ğŸ† ìµœê³  ì„±ëŠ¥: {best_model[0]} (Huber Loss: {best_model[1]:.4f})\")\n",
        "#         print(f\"âœ… {group_name} ê·¸ë£¹ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    \n",
        "#     # ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
        "#     total_training_time = (datetime.now() - total_start_time).total_seconds()\n",
        "    \n",
        "#     print(f\"\\nğŸ‰ ì „ì²´ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "#     print(f\"=\" * 60)\n",
        "#     print(f\"â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {total_training_time/60:.1f}ë¶„ ({total_training_time/3600:.1f}ì‹œê°„)\")\n",
        "    \n",
        "#     # ì „ì²´ í‰ê·  ì„±ëŠ¥\n",
        "#     print(f\"\\nğŸ“Š ì „ì²´ í‰ê·  ì„±ëŠ¥:\")\n",
        "#     avg_scores = {'rmse': [], 'huber': [], 'mae': []}\n",
        "    \n",
        "#     for group_name, result in group_results.items():\n",
        "#         if result is not None and 'scores' in result:\n",
        "#             stacking_score = result['scores'].get('stacking', {})\n",
        "#             for metric in avg_scores.keys():\n",
        "#                 if metric in stacking_score:\n",
        "#                     avg_scores[metric].append(stacking_score[metric])\n",
        "    \n",
        "#     for metric, scores in avg_scores.items():\n",
        "#         if scores:\n",
        "#             print(f\"   í‰ê·  {metric.upper()}: {np.mean(scores):.4f} (Â±{np.std(scores):.4f})\")\n",
        "    \n",
        "#     # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.empty_cache()\n",
        "#         print(f\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "# # ëª¨ë“  ê·¸ë£¹ í›ˆë ¨ ì‹¤í–‰\n",
        "# train_all_groups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8-2. Train "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ë¹ ë¥¸ í›ˆë ¨ ì‹œì‘!\n",
            "ğŸš€ ì‹œì¦Œë³„ 2ê°œ ê·¸ë£¹ í›ˆë ¨ ì‹œì‘ (ì—°ë„ ê¸°ë°˜ 3-Fold CV)\n",
            "ğŸ¯ ë¯¸ë¦¬ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ í›ˆë ¨!\n",
            "\n",
            "============================================================\n",
            "ğŸ”¥ HEATING ê·¸ë£¹ í›ˆë ¨\n",
            "ğŸ“Š ë°ì´í„° í¬ê¸°: 289,997ê°œ\n",
            "ğŸ¢ ì§€ì‚¬ ìˆ˜: 19ê°œ\n",
            "ğŸ“… ì—°ë„ ë¶„í¬: {2021: 96653, 2022: 96672, 2023: 96672}\n",
            "ğŸ¯ íƒ€ê²Ÿ í†µê³„: í‰ê· =135.94, í‘œì¤€í¸ì°¨=135.29\n",
            "ğŸ”„ heating CV ë¶„í•  ìƒì„± ì¤‘...\n",
            "heating ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\n",
            "   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\n",
            "     2021ë…„: 96,653ê°œ\n",
            "     2022ë…„: 96,672ê°œ\n",
            "     2023ë…„: 96,672ê°œ\n",
            "   Fold 2021: í›ˆë ¨ 193,344ê°œ, ê²€ì¦ 96,653ê°œ\n",
            "   Fold 2022: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "   Fold 2023: í›ˆë ¨ 193,325ê°œ, ê²€ì¦ 96,672ê°œ\n",
            "ğŸ—ï¸ heating ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...\n",
            "ğŸš€ heating í›ˆë ¨ ì‹œì‘...\n",
            "\n",
            "heating ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘! (heating ì‹œì¦Œ)\n",
            "============================================================\n",
            "í›ˆë ¨ ë°ì´í„°: 289,997ê°œ\n",
            "ì—°ë„ ë¶„í¬: {2021: 96653, 2022: 96672, 2023: 96672}\n",
            "ğŸ¯ heating ì‹œì¦Œ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
            "   Prophet: {'changepoint_prior_scale': 0.0026364803038431655, 'seasonality_prior_scale': 0.13066739238053282, 'holidays_prior_scale': 5.3994844097874335, 'seasonality_mode': 'multiplicative'}\n",
            "   CatBoost: {'iterations': 1678, 'depth': 5, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 12.255876808378806, 'border_count': 42}\n",
            "   Ridge: {'alpha': 63.512210106407046}\n",
            "\n",
            "PROPHET í›ˆë ¨...\n",
            "   ìµœì  íŒŒë¼ë¯¸í„° ì ìš©: {'changepoint_prior_scale': 0.0026364803038431655, 'seasonality_prior_scale': 0.13066739238053282, 'holidays_prior_scale': 5.3994844097874335, 'seasonality_mode': 'multiplicative'}\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd3f8deca94c4d64a42a1916aed67833",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   OOF Fold 1 ì²˜ë¦¬ ì¤‘...\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9175451fdf994a24b77fb466d174cbf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   OOF Fold 2 ì²˜ë¦¬ ì¤‘...\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c46270cde8954638aade011cd887f763",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   OOF Fold 3 ì²˜ë¦¬ ì¤‘...\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06f9ce1a8c434f4386d2ec67bb30bcfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   prophet ì„±ëŠ¥: RMSE=29.5520, Huber=18.6505, MAE=19.1402\n",
            "   ì˜ˆìƒ ì ìˆ˜ ëŒ€ë¹„: 0.0000\n",
            "   ì´ ì‹œê°„: 172.2ì´ˆ\n",
            "\n",
            "CATBOOST í›ˆë ¨...\n",
            "   ìµœì  íŒŒë¼ë¯¸í„° ì ìš©: {'iterations': 1678, 'depth': 5, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 12.255876808378806, 'border_count': 42}\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   í›ˆë ¨ ë°ì´í„°: (289997, 49)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =135.94, í‘œì¤€í¸ì°¨=135.29, ë²”ìœ„=[0.00, 966.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 12ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. branch_id: 51.460\n",
            "      2. daily_ta_max: 14.662\n",
            "      3. heating_month_cos: 9.952\n",
            "      4. hour_cos: 6.971\n",
            "      5. ta: 3.549\n",
            "      6. ta_ma_24h: 2.600\n",
            "      7. si: 2.528\n",
            "      8. ta_lag_24h: 1.386\n",
            "      9. rn_day: 1.091\n",
            "     10. daily_ta_min: 0.944\n",
            "   OOF Fold 1 ì²˜ë¦¬ ì¤‘...\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   í›ˆë ¨ ë°ì´í„°: (193344, 49)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =136.92, í‘œì¤€í¸ì°¨=135.53, ë²”ìœ„=[0.00, 903.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 12ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. ta_lag_3h: 32.152\n",
            "      2. ws: 31.649\n",
            "      3. branch_id: 10.934\n",
            "      4. daily_temp_range: 10.707\n",
            "      5. ta_lag_24h: 7.380\n",
            "      6. daily_ta_mean: 3.562\n",
            "      7. temp_category: 0.577\n",
            "      8. daily_ta_min: 0.515\n",
            "      9. ta: 0.420\n",
            "     10. ta_ma_24h: 0.391\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 96653ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =131.00, ë²”ìœ„=[0.00, 778.18]\n",
            "   OOF Fold 2 ì²˜ë¦¬ ì¤‘...\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   í›ˆë ¨ ë°ì´í„°: (193325, 49)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =132.00, í‘œì¤€í¸ì°¨=131.82, ë²”ìœ„=[0.00, 966.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 12ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. branch_id: 38.867\n",
            "      2. daily_ta_mean: 26.497\n",
            "      3. day: 14.698\n",
            "      4. holiday_type: 4.647\n",
            "      5. hour_sin: 2.955\n",
            "      6. hm: 2.147\n",
            "      7. heating_month_cos: 1.775\n",
            "      8. month_cat: 1.238\n",
            "      9. ta_ma_24h: 1.140\n",
            "     10. ta: 1.126\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 96672ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =134.49, ë²”ìœ„=[1.63, 805.13]\n",
            "   OOF Fold 3 ì²˜ë¦¬ ì¤‘...\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   í›ˆë ¨ ë°ì´í„°: (193325, 49)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =138.89, í‘œì¤€í¸ì°¨=138.37, ë²”ìœ„=[0.00, 966.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 12ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. branch_id: 38.345\n",
            "      2. daily_ta_mean: 30.046\n",
            "      3. day: 12.920\n",
            "      4. holiday_type: 3.697\n",
            "      5. dayofyear: 3.269\n",
            "      6. hour_sin: 3.004\n",
            "      7. month_cat: 2.412\n",
            "      8. ta: 1.551\n",
            "      9. ta_lag_24h: 0.948\n",
            "     10. peak_time: 0.550\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 96672ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =133.86, ë²”ìœ„=[0.80, 812.86]\n",
            "   catboost ì„±ëŠ¥: RMSE=23.3722, Huber=13.9441, MAE=14.4312\n",
            "   ì˜ˆìƒ ì ìˆ˜ ëŒ€ë¹„: 0.0000\n",
            "   ì´ ì‹œê°„: 338.8ì´ˆ\n",
            "\n",
            "Ridge ë©”íƒ€ ëª¨ë¸ í›ˆë ¨...\n",
            "   ë©”íƒ€ ëª¨ë¸ ì…ë ¥: (289997, 2)\n",
            "   ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ í†µê³„:\n",
            "     prophet: í‰ê· =141.98, í‘œì¤€í¸ì°¨=138.65\n",
            "     catboost: í‰ê· =133.12, í‘œì¤€í¸ì°¨=129.30\n",
            "   ìµœì  Ridge íŒŒë¼ë¯¸í„° ì ìš©: {'alpha': 63.512210106407046}\n",
            "   ì˜ˆìƒ RMSE: 24.8784\n",
            "   ìŠ¤íƒœí‚¹ ì„±ëŠ¥: RMSE=22.6328, Huber=13.7256, MAE=14.2128\n",
            "   ë©”íƒ€ëª¨ë¸ ê°€ì¤‘ì¹˜:\n",
            "     prophet: 0.1500\n",
            "     catboost: 0.8723\n",
            "âœ… heating ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!\n",
            "\n",
            "ğŸ“ˆ heating ìµœì¢… ê²°ê³¼:\n",
            "ëª¨ë¸               RMSE    Huber      MAE    ì‹œê°„(ì´ˆ)\n",
            "--------------------------------------------------\n",
            "prophet       29.5520  18.6505  19.1402    172.2\n",
            "catboost      23.3722  13.9441  14.4312    338.8\n",
            "stacking      22.6328  13.7256  14.2128      0.0\n",
            "   â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: 511.0ì´ˆ (8.5ë¶„)\n",
            "   ğŸ† ìµœê³  ì„±ëŠ¥: stacking (Huber Loss: 13.7256)\n",
            "âœ… heating ê·¸ë£¹ í›ˆë ¨ ì™„ë£Œ!\n",
            "\n",
            "============================================================\n",
            "ğŸ”¥ NON_HEATING ê·¸ë£¹ í›ˆë ¨\n",
            "ğŸ“Š ë°ì´í„° í¬ê¸°: 209,304ê°œ\n",
            "ğŸ¢ ì§€ì‚¬ ìˆ˜: 19ê°œ\n",
            "ğŸ“… ì—°ë„ ë¶„í¬: {2021: 69768, 2022: 69768, 2023: 69768}\n",
            "ğŸ¯ íƒ€ê²Ÿ í†µê³„: í‰ê· =40.35, í‘œì¤€í¸ì°¨=32.00\n",
            "ğŸ”„ non_heating CV ë¶„í•  ìƒì„± ì¤‘...\n",
            "non_heating ê·¸ë£¹ - ì—°ë„ ê¸°ë°˜ 3-Fold CV ë¶„í•  ìƒì„±...\n",
            "   ì—°ë„ë³„ ë°ì´í„° ë¶„í¬:\n",
            "     2021ë…„: 69,768ê°œ\n",
            "     2022ë…„: 69,768ê°œ\n",
            "     2023ë…„: 69,768ê°œ\n",
            "   Fold 2021: í›ˆë ¨ 139,536ê°œ, ê²€ì¦ 69,768ê°œ\n",
            "   Fold 2022: í›ˆë ¨ 139,536ê°œ, ê²€ì¦ 69,768ê°œ\n",
            "   Fold 2023: í›ˆë ¨ 139,536ê°œ, ê²€ì¦ 69,768ê°œ\n",
            "ğŸ—ï¸ non_heating ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...\n",
            "ğŸš€ non_heating í›ˆë ¨ ì‹œì‘...\n",
            "\n",
            "non_heating ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì‹œì‘! (non_heating ì‹œì¦Œ)\n",
            "============================================================\n",
            "í›ˆë ¨ ë°ì´í„°: 209,304ê°œ\n",
            "ì—°ë„ ë¶„í¬: {2021: 69768, 2022: 69768, 2023: 69768}\n",
            "ğŸ¯ non_heating ì‹œì¦Œ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
            "   Prophet: {'changepoint_prior_scale': 0.0010695090612476649, 'seasonality_prior_scale': 3.652041851774491, 'holidays_prior_scale': 0.45441617690609376, 'seasonality_mode': 'additive'}\n",
            "   CatBoost: {'iterations': 1818, 'depth': 8, 'learning_rate': 0.08527855281875678, 'l2_leaf_reg': 14.955151393164728, 'border_count': 35}\n",
            "   Ridge: {'alpha': 63.512210106407046}\n",
            "\n",
            "PROPHET í›ˆë ¨...\n",
            "   ìµœì  íŒŒë¼ë¯¸í„° ì ìš©: {'changepoint_prior_scale': 0.0010695090612476649, 'seasonality_prior_scale': 3.652041851774491, 'holidays_prior_scale': 0.45441617690609376, 'seasonality_mode': 'additive'}\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'non_heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d5013834cb34a859ddf744405c3cfbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ ì§€ì‚¬ A: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ B: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ C: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ D: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ E: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ F: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ G: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ H: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ I: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ J: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ K: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ L: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ M: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ N: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ O: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ P: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ Q: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ R: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ S: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   OOF Fold 1 ì²˜ë¦¬ ì¤‘...\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'non_heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9e5aec32b084c9faa4c61ec42f42d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ ì§€ì‚¬ A: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ B: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ C: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ D: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ E: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ F: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ G: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ H: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ I: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ J: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ K: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ L: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ M: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ N: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ O: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ P: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ Q: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ R: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ S: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   OOF Fold 2 ì²˜ë¦¬ ì¤‘...\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'non_heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "431c316ce41345658c71f52234acb5e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ ì§€ì‚¬ A: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ B: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ C: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ D: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ E: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ F: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ G: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ H: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ I: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ J: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ K: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ L: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ M: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ N: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ O: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ P: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ Q: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ R: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ S: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   OOF Fold 3 ì²˜ë¦¬ ì¤‘...\n",
            "Prophet ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ì‚¬ìš©í•  regressors: ['ta', 'hm', 'ws', 'HDD18', 'apparent_temp', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'ta_lag_3h', 'ta_lag_6h', 'non_heating_month_order']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62ff9b7991394364a0d939e8d4543286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Prophet ì§€ì‚¬ë³„ í›ˆë ¨:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ ì§€ì‚¬ A: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ B: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ C: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ D: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ E: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ F: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ G: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ H: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ I: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ J: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ K: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ L: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ M: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ N: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ O: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ P: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ Q: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ R: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "âš ï¸ ì§€ì‚¬ S: ëˆ„ë½ëœ regressors: ['apparent_temp']\n",
            "   19/19ê°œ ì§€ì‚¬ í›ˆë ¨ ì™„ë£Œ\n",
            "   prophet ì„±ëŠ¥: RMSE=11.9944, Huber=7.7240, MAE=8.2030\n",
            "   ì˜ˆìƒ ì ìˆ˜ ëŒ€ë¹„: 0.0000\n",
            "   ì´ ì‹œê°„: 100.2ì´ˆ\n",
            "\n",
            "CATBOOST í›ˆë ¨...\n",
            "   ìµœì  íŒŒë¼ë¯¸í„° ì ìš©: {'iterations': 1818, 'depth': 8, 'learning_rate': 0.08527855281875678, 'l2_leaf_reg': 14.955151393164728, 'border_count': 35}\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   í›ˆë ¨ ë°ì´í„°: (209304, 47)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =40.35, í‘œì¤€í¸ì°¨=32.00, ë²”ìœ„=[0.00, 292.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 11ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. branch_id: 21.790\n",
            "      2. day: 21.281\n",
            "      3. ta_diff_6h: 19.691\n",
            "      4. ta_lag_3h: 10.360\n",
            "      5. hm: 9.716\n",
            "      6. hour_sin: 5.632\n",
            "      7. non_heating_month_sin: 3.217\n",
            "      8. peak_time: 2.196\n",
            "      9. hour_cat: 0.853\n",
            "     10. hour_cos: 0.787\n",
            "   OOF Fold 1 ì²˜ë¦¬ ì¤‘...\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   í›ˆë ¨ ë°ì´í„°: (139536, 47)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =40.89, í‘œì¤€í¸ì°¨=31.63, ë²”ìœ„=[0.00, 251.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 11ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. branch_id: 27.406\n",
            "      2. rn_day: 13.764\n",
            "      3. non_heating_month_order: 8.731\n",
            "      4. hour_sin: 7.062\n",
            "      5. daily_ta_min: 5.264\n",
            "      6. temp_category: 4.354\n",
            "      7. si: 4.352\n",
            "      8. peak_time: 4.238\n",
            "      9. ws: 4.174\n",
            "     10. dayofyear: 3.564\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 69768ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =39.76, ë²”ìœ„=[0.00, 195.08]\n",
            "   OOF Fold 2 ì²˜ë¦¬ ì¤‘...\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   í›ˆë ¨ ë°ì´í„°: (139536, 47)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =40.09, í‘œì¤€í¸ì°¨=32.28, ë²”ìœ„=[0.00, 292.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 11ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. non_heating_month_sin: 17.179\n",
            "      2. rn_hr1: 15.544\n",
            "      3. daily_ta_max: 14.273\n",
            "      4. branch_id: 10.833\n",
            "      5. hour_sin: 10.543\n",
            "      6. ws: 9.521\n",
            "      7. hour_cos: 9.321\n",
            "      8. temp_category: 2.228\n",
            "      9. dayofyear: 1.853\n",
            "     10. peak_time: 1.489\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 69768ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =38.57, ë²”ìœ„=[0.00, 204.16]\n",
            "   OOF Fold 3 ì²˜ë¦¬ ì¤‘...\n",
            "CatBoost ëª¨ë¸ í›ˆë ¨ ì¤‘...\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   í›ˆë ¨ ë°ì´í„°: (139536, 47)\n",
            "   íƒ€ê²Ÿ í†µê³„: í‰ê· =40.06, í‘œì¤€í¸ì°¨=32.07, ë²”ìœ„=[0.00, 292.00]\n",
            "   ë‹¨ì¡°ì„± ì œì•½: 11ê°œ í”¼ì³ì— ì ìš©\n",
            "   CatBoost í›ˆë ¨ ì™„ë£Œ\n",
            "   ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì³:\n",
            "      1. branch_id: 23.211\n",
            "      2. rn_day: 12.385\n",
            "      3. non_heating_month_order: 7.984\n",
            "      4. temp_category: 7.548\n",
            "      5. hour_sin: 7.336\n",
            "      6. ws: 6.858\n",
            "      7. peak_time: 5.467\n",
            "      8. daily_ta_min: 4.930\n",
            "      9. non_heating_month_sin: 4.671\n",
            "     10. dayofyear: 2.226\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 69768ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =38.62, ë²”ìœ„=[0.00, 204.35]\n",
            "   catboost ì„±ëŠ¥: RMSE=10.0258, Huber=6.1526, MAE=6.6258\n",
            "   ì˜ˆìƒ ì ìˆ˜ ëŒ€ë¹„: 0.0000\n",
            "   ì´ ì‹œê°„: 377.3ì´ˆ\n",
            "\n",
            "Ridge ë©”íƒ€ ëª¨ë¸ í›ˆë ¨...\n",
            "   ë©”íƒ€ ëª¨ë¸ ì…ë ¥: (209304, 2)\n",
            "   ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ í†µê³„:\n",
            "     prophet: í‰ê· =41.48, í‘œì¤€í¸ì°¨=30.42\n",
            "     catboost: í‰ê· =38.99, í‘œì¤€í¸ì°¨=29.11\n",
            "   ìµœì  Ridge íŒŒë¼ë¯¸í„° ì ìš©: {'alpha': 63.512210106407046}\n",
            "   ì˜ˆìƒ RMSE: 9.8138\n",
            "   ìŠ¤íƒœí‚¹ ì„±ëŠ¥: RMSE=9.7575, Huber=6.0626, MAE=6.5357\n",
            "   ë©”íƒ€ëª¨ë¸ ê°€ì¤‘ì¹˜:\n",
            "     prophet: 0.1600\n",
            "     catboost: 0.8847\n",
            "âœ… non_heating ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ!\n",
            "\n",
            "ğŸ“ˆ non_heating ìµœì¢… ê²°ê³¼:\n",
            "ëª¨ë¸               RMSE    Huber      MAE    ì‹œê°„(ì´ˆ)\n",
            "--------------------------------------------------\n",
            "prophet       11.9944   7.7240   8.2030    100.2\n",
            "catboost      10.0258   6.1526   6.6258    377.3\n",
            "stacking       9.7575   6.0626   6.5357      0.0\n",
            "   â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: 477.5ì´ˆ (8.0ë¶„)\n",
            "   ğŸ† ìµœê³  ì„±ëŠ¥: stacking (Huber Loss: 6.0626)\n",
            "âœ… non_heating ê·¸ë£¹ í›ˆë ¨ ì™„ë£Œ!\n",
            "\n",
            "ğŸ‰ ì „ì²´ í›ˆë ¨ ì™„ë£Œ!\n",
            "============================================================\n",
            "â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: 16.5ë¶„ (0.3ì‹œê°„)\n",
            "\n",
            "ğŸ“Š ì „ì²´ í‰ê·  ì„±ëŠ¥:\n",
            "   í‰ê·  RMSE: 16.1952 (Â±6.4376)\n",
            "   í‰ê·  HUBER: 9.8941 (Â±3.8315)\n",
            "   í‰ê·  MAE: 10.3742 (Â±3.8385)\n",
            "\n",
            "ğŸ¯ ì‚¬ìš©ëœ ìµœì  íŒŒë¼ë¯¸í„°:\n",
            "\n",
            "HEATING ì‹œì¦Œ:\n",
            "   Prophet: {'changepoint_prior_scale': 0.0026364803038431655, 'seasonality_prior_scale': 0.13066739238053282, 'holidays_prior_scale': 5.3994844097874335, 'seasonality_mode': 'multiplicative'}\n",
            "   CatBoost: {'iterations': 1678, 'depth': 5, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 12.255876808378806, 'border_count': 42}\n",
            "   Ridge: {'alpha': 63.512210106407046}\n",
            "\n",
            "NON_HEATING ì‹œì¦Œ:\n",
            "   Prophet: {'changepoint_prior_scale': 0.0010695090612476649, 'seasonality_prior_scale': 3.652041851774491, 'holidays_prior_scale': 0.45441617690609376, 'seasonality_mode': 'additive'}\n",
            "   CatBoost: {'iterations': 1818, 'depth': 8, 'learning_rate': 0.08527855281875678, 'l2_leaf_reg': 14.955151393164728, 'border_count': 35}\n",
            "   Ridge: {'alpha': 63.512210106407046}\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë“  ê·¸ë£¹ í›ˆë ¨ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „)\n",
        "def train_all_groups():\n",
        "    \"\"\"2ê°œ ê·¸ë£¹ í›ˆë ¨ (ë‚œë°©/ë¹„ë‚œë°©) - ì—°ë„ ê¸°ë°˜ CV ì ìš©\"\"\"\n",
        "    group_configs = {\n",
        "        \"heating\": {\n",
        "            \"season\": \"heating\", \n",
        "            \"use_predefined\": True  # ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
        "        },\n",
        "        \"non_heating\": {\n",
        "            \"season\": \"non_heating\", \n",
        "            \"use_predefined\": True  # ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"ğŸš€ ì‹œì¦Œë³„ 2ê°œ ê·¸ë£¹ í›ˆë ¨ ì‹œì‘ (ì—°ë„ ê¸°ë°˜ 3-Fold CV)\")\n",
        "    print(\"ğŸ¯ ë¯¸ë¦¬ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ í›ˆë ¨!\")\n",
        "    total_start_time = datetime.now()\n",
        "    \n",
        "    for group_name, config in group_configs.items():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ğŸ”¥ {group_name.upper()} ê·¸ë£¹ í›ˆë ¨\")\n",
        "        \n",
        "        # ê·¸ë£¹ ë°ì´í„° ê²€ì¦\n",
        "        if group_name not in train_groups:\n",
        "            raise KeyError(f\"'{group_name}' ê·¸ë£¹ì´ train_groupsì— ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            \n",
        "        group_data = train_groups[group_name]\n",
        "        \n",
        "        if len(group_data) == 0:\n",
        "            raise ValueError(f\"{group_name} ê·¸ë£¹ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        \n",
        "        if 'heat_demand' not in group_data.columns:\n",
        "            raise ValueError(f\"{group_name} ê·¸ë£¹ì— 'heat_demand' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        \n",
        "        print(f\"ğŸ“Š ë°ì´í„° í¬ê¸°: {len(group_data):,}ê°œ\")\n",
        "        print(f\"ğŸ¢ ì§€ì‚¬ ìˆ˜: {group_data['branch_id'].nunique()}ê°œ\")\n",
        "        print(f\"ğŸ“… ì—°ë„ ë¶„í¬: {dict(group_data['year'].value_counts().sort_index())}\")\n",
        "        print(f\"ğŸ¯ íƒ€ê²Ÿ í†µê³„: í‰ê· ={group_data['heat_demand'].mean():.2f}, í‘œì¤€í¸ì°¨={group_data['heat_demand'].std():.2f}\")\n",
        "        \n",
        "        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰ í™•ì¸\n",
        "        if len(group_data) < 1000:\n",
        "            raise ValueError(f\"{group_name} ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(group_data):,}ê°œ). ìµœì†Œ 1,000ê°œ í•„ìš”.\")\n",
        "        \n",
        "        # ê·¸ë£¹ë³„ CV ë¶„í•  ìƒì„±\n",
        "        print(f\"ğŸ”„ {group_name} CV ë¶„í•  ìƒì„± ì¤‘...\")\n",
        "        group_cv_splits = create_year_based_cv_splits(group_data, group_name)\n",
        "        \n",
        "        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„±\n",
        "        print(f\"ğŸ—ï¸ {group_name} ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
        "        ensemble_models[group_name] = AdvancedStackingEnsemble(\n",
        "            season_type=config[\"season\"], \n",
        "            group_name=group_name\n",
        "        )\n",
        "        \n",
        "        # í›ˆë ¨ ì‹¤í–‰ (ğŸ¯ ìˆ˜ì •ëœ ë¶€ë¶„)\n",
        "        print(f\"ğŸš€ {group_name} í›ˆë ¨ ì‹œì‘...\")\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        ensemble_models[group_name].fit(\n",
        "            group_data, \n",
        "            group_cv_splits,\n",
        "            target_col='heat_demand',\n",
        "            use_predefined_params=config[\"use_predefined\"]  # ğŸ¯ ë³€ê²½ë¨\n",
        "        )\n",
        "        \n",
        "        total_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        group_results[group_name] = {\n",
        "            'scores': ensemble_models[group_name].individual_scores.copy(),\n",
        "            'total_time': total_time,\n",
        "            'data_size': len(group_data),\n",
        "            'branch_count': group_data['branch_id'].nunique(),\n",
        "            'year_distribution': dict(group_data['year'].value_counts().sort_index())\n",
        "        }\n",
        "        \n",
        "        # ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥ (Huber Loss í¬í•¨)\n",
        "        print(f\"\\nğŸ“ˆ {group_name} ìµœì¢… ê²°ê³¼:\")\n",
        "        print(f\"{'ëª¨ë¸':12s} {'RMSE':>8s} {'Huber':>8s} {'MAE':>8s} {'ì‹œê°„(ì´ˆ)':>8s}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        for model, scores in ensemble_models[group_name].individual_scores.items():\n",
        "            rmse = scores.get('rmse', 999)\n",
        "            huber = scores.get('huber', 999)\n",
        "            mae = scores.get('mae', 999)\n",
        "            model_time = scores.get('train_time', 0)\n",
        "            \n",
        "            print(f\"{model:12s} {rmse:8.4f} {huber:8.4f} {mae:8.4f} {model_time:8.1f}\")\n",
        "        \n",
        "        print(f\"   â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)\")\n",
        "        \n",
        "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸\n",
        "        best_model = min(\n",
        "            [(name, score.get('huber', 999)) for name, score in ensemble_models[group_name].individual_scores.items()],\n",
        "            key=lambda x: x[1]\n",
        "        )\n",
        "        print(f\"   ğŸ† ìµœê³  ì„±ëŠ¥: {best_model[0]} (Huber Loss: {best_model[1]:.4f})\")\n",
        "        print(f\"âœ… {group_name} ê·¸ë£¹ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    \n",
        "    # ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
        "    total_training_time = (datetime.now() - total_start_time).total_seconds()\n",
        "    \n",
        "    print(f\"\\nğŸ‰ ì „ì²´ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    print(f\"=\" * 60)\n",
        "    print(f\"â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {total_training_time/60:.1f}ë¶„ ({total_training_time/3600:.1f}ì‹œê°„)\")\n",
        "    \n",
        "    # ì „ì²´ í‰ê·  ì„±ëŠ¥\n",
        "    print(f\"\\nğŸ“Š ì „ì²´ í‰ê·  ì„±ëŠ¥:\")\n",
        "    avg_scores = {'rmse': [], 'huber': [], 'mae': []}\n",
        "    \n",
        "    for group_name, result in group_results.items():\n",
        "        if result is not None and 'scores' in result:\n",
        "            stacking_score = result['scores'].get('stacking', {})\n",
        "            for metric in avg_scores.keys():\n",
        "                if metric in stacking_score:\n",
        "                    avg_scores[metric].append(stacking_score[metric])\n",
        "    \n",
        "    for metric, scores in avg_scores.items():\n",
        "        if scores:\n",
        "            print(f\"   í‰ê·  {metric.upper()}: {np.mean(scores):.4f} (Â±{np.std(scores):.4f})\")\n",
        "    \n",
        "    # ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš© ê²°ê³¼ ìš”ì•½\n",
        "    print(f\"\\nğŸ¯ ì‚¬ìš©ëœ ìµœì  íŒŒë¼ë¯¸í„°:\")\n",
        "    for group_name in group_configs.keys():\n",
        "        if group_name in ensemble_models:\n",
        "            model = ensemble_models[group_name]\n",
        "            print(f\"\\n{group_name.upper()} ì‹œì¦Œ:\")\n",
        "            print(f\"   Prophet: {model.predefined_params['prophet']}\")\n",
        "            print(f\"   CatBoost: {model.predefined_params['catboost']}\")\n",
        "            print(f\"   Ridge: {model.predefined_params['ridge']}\")\n",
        "    \n",
        "    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "    if 'torch' in globals() and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "# ğŸš€ ëª¨ë“  ê·¸ë£¹ í›ˆë ¨ ì‹¤í–‰\n",
        "print(\"ğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ ë¹ ë¥¸ í›ˆë ¨ ì‹œì‘!\")\n",
        "train_all_groups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ í˜„ì¬ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤...\n",
            "ğŸ’¾ í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ì‹œì‘...\n",
            "ğŸ“ ì €ì¥ ê²½ë¡œ: ./saved_models/\n",
            "ğŸ” ì €ì¥í•  ëª¨ë¸ í™•ì¸:\n",
            "   ensemble_models: ['heating', 'non_heating']\n",
            "   group_results: ['heating', 'non_heating']\n",
            "\n",
            "ğŸ’¾ HEATING ëª¨ë¸ ì €ì¥ ì¤‘...\n",
            "   âœ… ì•™ìƒë¸” ëª¨ë¸: ensemble_heating.pkl\n",
            "   âœ… Prophet: prophet_heating.joblib\n",
            "   âœ… CatBoost ëª¨ë¸: catboost_heating.cbm\n",
            "   âœ… Ridge ë©”íƒ€ëª¨ë¸: ridge_heating.joblib\n",
            "   âœ… ìµœì  íŒŒë¼ë¯¸í„°: params_heating.pkl\n",
            "\n",
            "ğŸ’¾ NON_HEATING ëª¨ë¸ ì €ì¥ ì¤‘...\n",
            "   âœ… ì•™ìƒë¸” ëª¨ë¸: ensemble_non_heating.pkl\n",
            "   âœ… Prophet: prophet_non_heating.joblib\n",
            "   âœ… CatBoost ëª¨ë¸: catboost_non_heating.cbm\n",
            "   âœ… Ridge ë©”íƒ€ëª¨ë¸: ridge_non_heating.joblib\n",
            "   âœ… ìµœì  íŒŒë¼ë¯¸í„°: params_non_heating.pkl\n",
            "\n",
            "ğŸ’¾ ì„±ëŠ¥ ê²°ê³¼ ì €ì¥ ì¤‘...\n",
            "   âœ… heating ê²°ê³¼: results_heating.pkl\n",
            "   âœ… non_heating ê²°ê³¼: results_non_heating.pkl\n",
            "\n",
            "ğŸ’¾ ì „ì²´ ëª¨ë¸ íŒ¨í‚¤ì§€ ì €ì¥ ì¤‘...\n",
            "   âœ… ì „ì²´ íŒ¨í‚¤ì§€: full_model_package.pkl\n",
            "\n",
            "ğŸ‰ ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n",
            "==================================================\n",
            "ğŸ“ ì €ì¥ ê²½ë¡œ: ./saved_models/\n",
            "ğŸ“Š ì €ì¥ëœ íŒŒì¼ ìˆ˜: 13ê°œ\n",
            "\n",
            "ğŸ“‹ ì €ì¥ëœ íŒŒì¼ ëª©ë¡:\n",
            "   ensemble_heating.pkl             57.7MB\n",
            "   prophet_heating.joblib           58.3MB\n",
            "   catboost_heating.cbm              1.6MB\n",
            "   ridge_heating.joblib              0.0MB\n",
            "   params_heating.pkl                0.0MB\n",
            "   ensemble_non_heating.pkl         47.8MB\n",
            "   prophet_non_heating.joblib       40.5MB\n",
            "   catboost_non_heating.cbm          9.0MB\n",
            "   ridge_non_heating.joblib          0.0MB\n",
            "   params_non_heating.pkl            0.0MB\n",
            "   results_heating.pkl               0.0MB\n",
            "   results_non_heating.pkl           0.0MB\n",
            "   full_model_package.pkl          105.5MB\n",
            "\n",
            "ğŸ’¾ ì´ ì €ì¥ ìš©ëŸ‰: 320.4MB\n",
            "\n",
            "âœ… ì €ì¥ ì™„ë£Œ! 13ê°œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ğŸ’¡ ë‚˜ì¤‘ì— ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´:\n",
            "   ensemble_models, group_results = load_saved_models('./saved_models/')\n"
          ]
        }
      ],
      "source": [
        "def save_trained_models(ensemble_models, group_results, model_save_path=\"./saved_models/\"):\n",
        "    \"\"\"ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "    import joblib\n",
        "    from datetime import datetime\n",
        "    \n",
        "    print(f\"ğŸ’¾ í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ì‹œì‘...\")\n",
        "    print(f\"ğŸ“ ì €ì¥ ê²½ë¡œ: {model_save_path}\")\n",
        "    \n",
        "    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "    \n",
        "    # ì €ì¥í•  ëª¨ë¸ í™•ì¸\n",
        "    print(f\"ğŸ” ì €ì¥í•  ëª¨ë¸ í™•ì¸:\")\n",
        "    print(f\"   ensemble_models: {list(ensemble_models.keys()) if ensemble_models else 'ì—†ìŒ'}\")\n",
        "    print(f\"   group_results: {list(group_results.keys()) if group_results else 'ì—†ìŒ'}\")\n",
        "    \n",
        "    saved_files = []\n",
        "    \n",
        "    # ê° ê·¸ë£¹ë³„ ëª¨ë¸ ì €ì¥\n",
        "    for group_name in ensemble_models.keys():\n",
        "        print(f\"\\nğŸ’¾ {group_name.upper()} ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
        "        \n",
        "        try:\n",
        "            # 1. ì „ì²´ ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ (pickle)\n",
        "            ensemble_file = os.path.join(model_save_path, f\"ensemble_{group_name}.pkl\")\n",
        "            with open(ensemble_file, 'wb') as f:\n",
        "                pickle.dump(ensemble_models[group_name], f)\n",
        "            saved_files.append(ensemble_file)\n",
        "            print(f\"   âœ… ì•™ìƒë¸” ëª¨ë¸: ensemble_{group_name}.pkl\")\n",
        "            \n",
        "            # 2. ê°œë³„ ëª¨ë¸ë³„ ì €ì¥\n",
        "            ensemble_model = ensemble_models[group_name]\n",
        "            \n",
        "            for model_name, model in ensemble_model.models.items():\n",
        "                try:\n",
        "                    if model_name == 'prophet':\n",
        "                        # Prophet ëª¨ë¸ì€ joblibë¡œ ì €ì¥\n",
        "                        prophet_file = os.path.join(model_save_path, f\"prophet_{group_name}.joblib\")\n",
        "                        joblib.dump(model, prophet_file)\n",
        "                        saved_files.append(prophet_file)\n",
        "                        print(f\"   âœ… Prophet: prophet_{group_name}.joblib\")\n",
        "                        \n",
        "                    elif model_name == 'catboost':\n",
        "                        # CatBoost ëª¨ë¸ ì €ì¥ ì‹œë„\n",
        "                        try:\n",
        "                            if hasattr(model, 'model') and model.model is not None:\n",
        "                                # CatBoost ì „ìš© í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
        "                                catboost_file = os.path.join(model_save_path, f\"catboost_{group_name}.cbm\")\n",
        "                                model.model.save_model(catboost_file)\n",
        "                                saved_files.append(catboost_file)\n",
        "                                print(f\"   âœ… CatBoost ëª¨ë¸: catboost_{group_name}.cbm\")\n",
        "                            else:\n",
        "                                # ì „ì²´ ê°ì²´ë¥¼ joblibë¡œ ì €ì¥\n",
        "                                catboost_file = os.path.join(model_save_path, f\"catboost_{group_name}.joblib\")\n",
        "                                joblib.dump(model, catboost_file)\n",
        "                                saved_files.append(catboost_file)\n",
        "                                print(f\"   âœ… CatBoost ê°ì²´: catboost_{group_name}.joblib\")\n",
        "                        except Exception as e:\n",
        "                            # ì‹¤íŒ¨ì‹œ joblibë¡œ ë°±ì—… ì €ì¥\n",
        "                            catboost_file = os.path.join(model_save_path, f\"catboost_{group_name}.joblib\")\n",
        "                            joblib.dump(model, catboost_file)\n",
        "                            saved_files.append(catboost_file)\n",
        "                            print(f\"   âœ… CatBoost ë°±ì—…: catboost_{group_name}.joblib\")\n",
        "                            \n",
        "                except Exception as e:\n",
        "                    print(f\"   âš ï¸ {model_name} ê°œë³„ ì €ì¥ ì‹¤íŒ¨: {str(e)[:50]}...\")\n",
        "            \n",
        "            # 3. ë©”íƒ€ ëª¨ë¸ (Ridge) ì €ì¥\n",
        "            if hasattr(ensemble_model, 'meta_model') and ensemble_model.meta_model is not None:\n",
        "                ridge_file = os.path.join(model_save_path, f\"ridge_{group_name}.joblib\")\n",
        "                joblib.dump(ensemble_model.meta_model, ridge_file)\n",
        "                saved_files.append(ridge_file)\n",
        "                print(f\"   âœ… Ridge ë©”íƒ€ëª¨ë¸: ridge_{group_name}.joblib\")\n",
        "            \n",
        "            # 4. ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥\n",
        "            if hasattr(ensemble_model, 'predefined_params'):\n",
        "                params_file = os.path.join(model_save_path, f\"params_{group_name}.pkl\")\n",
        "                with open(params_file, 'wb') as f:\n",
        "                    pickle.dump(ensemble_model.predefined_params, f)\n",
        "                saved_files.append(params_file)\n",
        "                print(f\"   âœ… ìµœì  íŒŒë¼ë¯¸í„°: params_{group_name}.pkl\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ {group_name} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}\")\n",
        "    \n",
        "    # 5. ì„±ëŠ¥ ê²°ê³¼ ì €ì¥\n",
        "    if group_results:\n",
        "        print(f\"\\nğŸ’¾ ì„±ëŠ¥ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
        "        for group_name, result in group_results.items():\n",
        "            try:\n",
        "                results_file = os.path.join(model_save_path, f\"results_{group_name}.pkl\")\n",
        "                with open(results_file, 'wb') as f:\n",
        "                    pickle.dump(result, f)\n",
        "                saved_files.append(results_file)\n",
        "                print(f\"   âœ… {group_name} ê²°ê³¼: results_{group_name}.pkl\")\n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸ {group_name} ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}\")\n",
        "    \n",
        "    # 6. ì „ì²´ ëª¨ë¸ íŒ¨í‚¤ì§€ ì €ì¥\n",
        "    try:\n",
        "        print(f\"\\nğŸ’¾ ì „ì²´ ëª¨ë¸ íŒ¨í‚¤ì§€ ì €ì¥ ì¤‘...\")\n",
        "        \n",
        "        full_package = {\n",
        "            'ensemble_models': ensemble_models,\n",
        "            'group_results': group_results,\n",
        "            'training_info': {\n",
        "                'save_date': datetime.now().isoformat(),\n",
        "                'model_groups': list(ensemble_models.keys()),\n",
        "                'total_models': len(ensemble_models)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        package_file = os.path.join(model_save_path, \"full_model_package.pkl\")\n",
        "        with open(package_file, 'wb') as f:\n",
        "            pickle.dump(full_package, f)\n",
        "        saved_files.append(package_file)\n",
        "        \n",
        "        print(f\"   âœ… ì „ì²´ íŒ¨í‚¤ì§€: full_model_package.pkl\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ ì „ì²´ íŒ¨í‚¤ì§€ ì €ì¥ ì‹¤íŒ¨: {str(e)}\")\n",
        "    \n",
        "    # ì €ì¥ ì™„ë£Œ ìš”ì•½\n",
        "    print(f\"\\nğŸ‰ ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"ğŸ“ ì €ì¥ ê²½ë¡œ: {model_save_path}\")\n",
        "    print(f\"ğŸ“Š ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(saved_files)}ê°œ\")\n",
        "    \n",
        "    # íŒŒì¼ ëª©ë¡ê³¼ í¬ê¸° ì¶œë ¥\n",
        "    print(f\"\\nğŸ“‹ ì €ì¥ëœ íŒŒì¼ ëª©ë¡:\")\n",
        "    total_size = 0\n",
        "    for file_path in saved_files:\n",
        "        if os.path.exists(file_path):\n",
        "            file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
        "            total_size += file_size\n",
        "            file_name = os.path.basename(file_path)\n",
        "            print(f\"   {file_name:<30} {file_size:>6.1f}MB\")\n",
        "    \n",
        "    print(f\"\\nğŸ’¾ ì´ ì €ì¥ ìš©ëŸ‰: {total_size:.1f}MB\")\n",
        "    \n",
        "    return saved_files\n",
        "\n",
        "# ğŸš€ ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ì‹¤í–‰\n",
        "print(\"ğŸ’¾ í˜„ì¬ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "# ëª¨ë¸ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
        "if 'ensemble_models' in globals() and ensemble_models:\n",
        "    if 'group_results' in globals() and group_results:\n",
        "        saved_files = save_trained_models(ensemble_models, group_results, \"./saved_models/\")\n",
        "        print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ! {len(saved_files)}ê°œ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(\"âš ï¸ group_resultsê°€ ì—†ì–´ì„œ ëª¨ë¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\")\n",
        "        saved_files = save_trained_models(ensemble_models, {}, \"./saved_models/\")\n",
        "else:\n",
        "    print(\"âŒ ì €ì¥í•  í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "    print(\"   ë¨¼ì € train_all_groups()ë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”.\")\n",
        "\n",
        "# ğŸ“¥ ë¡œë“œ í•¨ìˆ˜ (ì°¸ê³ ìš©)\n",
        "def load_saved_models(model_save_path=\"./saved_models/\"):\n",
        "    \"\"\"ì €ì¥ëœ ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    import pickle\n",
        "    import os\n",
        "    \n",
        "    print(f\"ğŸ“¥ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì¤‘... ({model_save_path})\")\n",
        "    \n",
        "    # ì „ì²´ íŒ¨í‚¤ì§€ ë¡œë“œ ì‹œë„\n",
        "    package_file = os.path.join(model_save_path, \"full_model_package.pkl\")\n",
        "    if os.path.exists(package_file):\n",
        "        try:\n",
        "            with open(package_file, 'rb') as f:\n",
        "                package = pickle.load(f)\n",
        "            \n",
        "            print(\"âœ… ì „ì²´ ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë“œ ì„±ê³µ!\")\n",
        "            print(f\"   ì €ì¥ ì¼ì‹œ: {package['training_info']['save_date']}\")\n",
        "            print(f\"   ëª¨ë¸ ê·¸ë£¹: {package['training_info']['model_groups']}\")\n",
        "            \n",
        "            return package['ensemble_models'], package['group_results']\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ì „ì²´ íŒ¨í‚¤ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    \n",
        "    print(\"âŒ ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    return {}, {}\n",
        "\n",
        "print(f\"\\nğŸ’¡ ë‚˜ì¤‘ì— ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´:\")\n",
        "print(f\"   ensemble_models, group_results = load_saved_models('./saved_models/')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results"
      },
      "source": [
        "## 9. ì „ì²´ ê·¸ë£¹ ê²°ê³¼ ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ† ì „ì²´ ê·¸ë£¹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½\n",
            "========================================================================================================================\n",
            "ê·¸ë£¹ëª…                  ë°ì´í„°         Prophet        CatBoost        Stacking    ì‹œê°„(ë¶„)\n",
            "                              RMSE/Huber      RMSE/Huber      RMSE/Huber      RMSE/Huber         \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "heating          289,997     29.55/18.65     23.37/13.94     22.63/13.73      8.5\n",
            "non_heating      209,304      11.99/7.72      10.03/6.15       9.76/6.06      8.0\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "TOTAL            499,301                                                                     16.5\n",
            "\n",
            "âœ… ì„±ê³µí•œ ê·¸ë£¹: 2/2\n",
            "â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: 16.5ë¶„ (0.3ì‹œê°„)\n",
            "\n",
            "ğŸ¥‡ ê·¸ë£¹ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (Huber Loss ê¸°ì¤€):\n",
            "   heating        : STACKING   (Huber: 13.7256)\n",
            "   non_heating    : STACKING   (Huber: 6.0626)\n",
            "\n",
            "ğŸ“Š ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:\n",
            "ëª¨ë¸                í‰ê·  RMSE     í‰ê·  Huber\n",
            "----------------------------------------\n",
            "PROPHET           20.7732      13.1873\n",
            "CATBOOST          16.6990      10.0483\n",
            "STACKING          16.1952       9.8941\n",
            "\n",
            "ğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ê°œì„  íš¨ê³¼ (Huber Loss ê¸°ì¤€):\n",
            "   heating        :  +1.57% ê°œì„ \n",
            "                     (ìµœê³  ê°œë³„: 13.9441 â†’ ìŠ¤íƒœí‚¹: 13.7256)\n",
            "   non_heating    :  +1.46% ê°œì„ \n",
            "                     (ìµœê³  ê°œë³„: 6.1526 â†’ ìŠ¤íƒœí‚¹: 6.0626)\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²´ ê·¸ë£¹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½\n",
        "print(\"\\nğŸ† ì „ì²´ ê·¸ë£¹ í›ˆë ¨ ê²°ê³¼ ìš”ì•½\")\n",
        "print(\"=\" * 120)\n",
        "\n",
        "total_time = 0\n",
        "total_data_size = 0\n",
        "successful_groups = 0\n",
        "\n",
        "# í—¤ë” ì¶œë ¥ (RMSE/Huber í˜•íƒœë¡œ)\n",
        "print(f\"{'ê·¸ë£¹ëª…':15s} {'ë°ì´í„°':>8s} {'Prophet':>15s} {'CatBoost':>15s} {'Stacking':>15s} {'ì‹œê°„(ë¶„)':>8s}\")\n",
        "print(f\"{'':15s} {'':>8s} {'RMSE/Huber':>15s} {'RMSE/Huber':>15s} {'RMSE/Huber':>15s} {'RMSE/Huber':>15s} {'':>8s}\")\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for group_name, result in group_results.items():\n",
        "    if result is not None:\n",
        "        scores = result['scores']\n",
        "        data_size = result['data_size']\n",
        "        group_time = result['total_time']\n",
        "        \n",
        "        total_time += group_time\n",
        "        total_data_size += data_size\n",
        "        successful_groups += 1\n",
        "        \n",
        "        # RMSEì™€ Huber Loss ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°\n",
        "        prophet_rmse = scores.get('prophet', {}).get('rmse', 999)\n",
        "        prophet_huber = scores.get('prophet', {}).get('huber', 999)\n",
        "        catboost_rmse = scores.get('catboost', {}).get('rmse', 999)\n",
        "        catboost_huber = scores.get('catboost', {}).get('huber', 999)\n",
        "        stacking_rmse = scores.get('stacking', {}).get('rmse', 999)\n",
        "        stacking_huber = scores.get('stacking', {}).get('huber', 999)\n",
        "        \n",
        "        # RMSE/Huber í˜•íƒœë¡œ ì¶œë ¥\n",
        "        prophet_display = f\"{prophet_rmse:.2f}/{prophet_huber:.2f}\"\n",
        "        catboost_display = f\"{catboost_rmse:.2f}/{catboost_huber:.2f}\"\n",
        "        stacking_display = f\"{stacking_rmse:.2f}/{stacking_huber:.2f}\"\n",
        "        \n",
        "        print(f\"{group_name:15s} {data_size:8,d} {prophet_display:>15s} {catboost_display:>15s} {stacking_display:>15s} {group_time/60:8.1f}\")\n",
        "    else:\n",
        "        print(f\"{group_name:15s} {'N/A':>8s} {'N/A':>15s} {'N/A':>15s} {'N/A':>15s} {'N/A':>15s} {'N/A':>8s}\")\n",
        "\n",
        "print(\"-\" * 120)\n",
        "print(f\"{'TOTAL':15s} {total_data_size:8,d} {'':>15s} {'':>15s} {'':>15s} {'':>15s} {total_time/60:8.1f}\")\n",
        "print(f\"\\nâœ… ì„±ê³µí•œ ê·¸ë£¹: {successful_groups}/2\")\n",
        "print(f\"â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {total_time/60:.1f}ë¶„ ({total_time/3600:.1f}ì‹œê°„)\")\n",
        "\n",
        "# ê·¸ë£¹ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸° (Huber Loss ê¸°ì¤€)\n",
        "print(f\"\\nğŸ¥‡ ê·¸ë£¹ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (Huber Loss ê¸°ì¤€):\")\n",
        "for group_name, result in group_results.items():\n",
        "    if result is not None:\n",
        "        scores = result['scores']\n",
        "        best_model = min(\n",
        "            [(name, score['huber']) for name, score in scores.items() \n",
        "             if isinstance(score, dict) and 'huber' in score],\n",
        "            key=lambda x: x[1],\n",
        "            default=(\"None\", 999)\n",
        "        )\n",
        "        print(f\"   {group_name:15s}: {best_model[0].upper():10s} (Huber: {best_model[1]:.4f})\")\n",
        "\n",
        "# ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ (RMSEì™€ Huber ëª¨ë‘)\n",
        "print(f\"\\nğŸ“Š ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:\")\n",
        "model_avg_scores = {\n",
        "    'prophet': {'rmse': [], 'huber': []}, \n",
        "    'catboost': {'rmse': [], 'huber': []}, \n",
        "    'stacking': {'rmse': [], 'huber': []}\n",
        "}\n",
        "\n",
        "for result in group_results.values():\n",
        "    if result is not None:\n",
        "        for model_name in model_avg_scores.keys():\n",
        "            if (model_name in result['scores'] and \n",
        "                isinstance(result['scores'][model_name], dict)):\n",
        "                score_dict = result['scores'][model_name]\n",
        "                if 'rmse' in score_dict:\n",
        "                    model_avg_scores[model_name]['rmse'].append(score_dict['rmse'])\n",
        "                if 'huber' in score_dict:\n",
        "                    model_avg_scores[model_name]['huber'].append(score_dict['huber'])\n",
        "\n",
        "print(f\"{'ëª¨ë¸':12s} {'í‰ê·  RMSE':>12s} {'í‰ê·  Huber':>12s}\")\n",
        "print(\"-\" * 40)\n",
        "for model_name, scores in model_avg_scores.items():\n",
        "    rmse_scores = scores['rmse']\n",
        "    huber_scores = scores['huber']\n",
        "    \n",
        "    if rmse_scores and huber_scores:\n",
        "        avg_rmse = np.mean(rmse_scores)\n",
        "        avg_huber = np.mean(huber_scores)\n",
        "        print(f\"{model_name.upper():12s} {avg_rmse:12.4f} {avg_huber:12.4f}\")\n",
        "\n",
        "# ìŠ¤íƒœí‚¹ì˜ ê°œì„  íš¨ê³¼ ë¶„ì„\n",
        "print(f\"\\nğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ê°œì„  íš¨ê³¼ (Huber Loss ê¸°ì¤€):\")\n",
        "for group_name, result in group_results.items():\n",
        "    if result is not None:\n",
        "        scores = result['scores']\n",
        "        individual_huber_scores = []\n",
        "        \n",
        "        for model in ['prophet', 'catboost']:\n",
        "            if model in scores and 'huber' in scores[model]:\n",
        "                individual_huber_scores.append(scores[model]['huber'])\n",
        "        \n",
        "        if individual_huber_scores and 'stacking' in scores and 'huber' in scores['stacking']:\n",
        "            best_individual = min(individual_huber_scores)\n",
        "            stacking_score = scores['stacking']['huber']\n",
        "            improvement = ((best_individual - stacking_score) / best_individual) * 100\n",
        "            \n",
        "            print(f\"   {group_name:15s}: {improvement:+6.2f}% ê°œì„ \")\n",
        "            print(f\"                     (ìµœê³  ê°œë³„: {best_individual:.4f} â†’ ìŠ¤íƒœí‚¹: {stacking_score:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prediction"
      },
      "source": [
        "## ğŸ”Ÿ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "prediction_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\n",
            "\n",
            "ğŸ“Š heating ì˜ˆì¸¡ ì¤‘...\n",
            "   heating ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\n",
            "     prophet: í‰ê· =131.16, ë²”ìœ„=[0.00, 823.31]\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 49ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 9ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time', 'cold_warning_level']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 97147ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =128.38, ë²”ìœ„=[0.00, 818.08]\n",
            "     catboost: í‰ê· =128.38, ë²”ìœ„=[0.00, 818.08]\n",
            "   heating ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ ì™„ë£Œ: í‰ê· =130.19, ë²”ìœ„=[0.00, 835.22]\n",
            "   âœ… heating: 97,147ê°œ ì˜ˆì¸¡ ì™„ë£Œ\n",
            "   ğŸ“ˆ ì˜ˆì¸¡ê°’ ë²”ìœ„: 0.00 ~ 835.22\n",
            "   ğŸ“Š ì˜ˆì¸¡ê°’ í‰ê· : 130.19\n",
            "\n",
            "ğŸ“Š non_heating ì˜ˆì¸¡ ì¤‘...\n",
            "   non_heating ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘...\n",
            "     prophet: í‰ê· =42.28, ë²”ìœ„=[0.00, 208.47]\n",
            "   ìµœì¢… ì‚¬ìš© í”¼ì³: 47ê°œ\n",
            "   ë²”ì£¼í˜• í”¼ì³: 8ê°œ - ['branch_id', 'hour_cat', 'month_cat', 'weekday_name', 'temp_category', 'wind_category', 'holiday_type', 'peak_time']\n",
            "   CatBoost ì˜ˆì¸¡ ì™„ë£Œ: 69768ê°œ\n",
            "   ì˜ˆì¸¡ í†µê³„: í‰ê· =39.64, ë²”ìœ„=[1.45, 198.30]\n",
            "     catboost: í‰ê· =39.64, ë²”ìœ„=[1.45, 198.30]\n",
            "   non_heating ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ ì™„ë£Œ: í‰ê· =41.06, ë²”ìœ„=[1.32, 201.03]\n",
            "   âœ… non_heating: 69,768ê°œ ì˜ˆì¸¡ ì™„ë£Œ\n",
            "   ğŸ“ˆ ì˜ˆì¸¡ê°’ ë²”ìœ„: 1.32 ~ 201.03\n",
            "   ğŸ“Š ì˜ˆì¸¡ê°’ í‰ê· : 41.06\n",
            "\n",
            "âœ… ëª¨ë“  ê·¸ë£¹ ì˜ˆì¸¡ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
        "print(\"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘...\")\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©\n",
        "test_predictions = {}\n",
        "individual_predictions = {}\n",
        "\n",
        "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•©ì—ì„œë„ 2ê°œ ê·¸ë£¹ë§Œ ì²˜ë¦¬\n",
        "for group_name in ['heating', 'non_heating']:\n",
        "    if group_name in ensemble_models and len(test_groups[group_name]) > 0:\n",
        "        print(f\"\\nğŸ“Š {group_name} ì˜ˆì¸¡ ì¤‘...\")\n",
        "        \n",
        "        try:\n",
        "            pred, individual_pred = ensemble_models[group_name].predict(test_groups[group_name])\n",
        "            test_predictions[group_name] = pred\n",
        "            individual_predictions[group_name] = individual_pred\n",
        "            \n",
        "            print(f\"   âœ… {group_name}: {len(pred):,}ê°œ ì˜ˆì¸¡ ì™„ë£Œ\")\n",
        "            print(f\"   ğŸ“ˆ ì˜ˆì¸¡ê°’ ë²”ìœ„: {pred.min():.2f} ~ {pred.max():.2f}\")\n",
        "            print(f\"   ğŸ“Š ì˜ˆì¸¡ê°’ í‰ê· : {pred.mean():.2f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ {group_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)[:100]}...\")\n",
        "            test_predictions[group_name] = np.zeros(len(test_groups[group_name]))\n",
        "    else:\n",
        "        if len(test_groups[group_name]) > 0:\n",
        "            print(f\"âš ï¸ {group_name}: í›ˆë ¨ëœ ëª¨ë¸ ì—†ìŒ, 0ìœ¼ë¡œ ì±„ì›€\")\n",
        "            test_predictions[group_name] = np.zeros(len(test_groups[group_name]))\n",
        "\n",
        "print(\"\\nâœ… ëª¨ë“  ê·¸ë£¹ ì˜ˆì¸¡ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_results"
      },
      "source": [
        "## 1ï¸âƒ£1ï¸âƒ£ ìµœì¢… ê²°ê³¼ í†µí•© ë° ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ test_groupsì—ì„œ test_df ì¬êµ¬ì„± ì¤‘...\n",
            "test_groups ì •ë³´:\n",
            "   heating: 97,147ê°œ, ì»¬ëŸ¼: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand', 'year', 'month', 'day', 'hour', 'dayofweek', 'dayofyear', 'ta_missing', 'ws_missing', 'rn_day_missing', 'rn_hr1_missing', 'hm_missing', 'si_missing', 'ta_chi_missing', 'heat_demand_missing', 'heating_season', 'day_of_year', 'cold_extreme', 'strong_wind', 'heavy_rain', 'hour_cat', 'month_cat', 'weekday_name', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'heating_month_order', 'heating_month_sin', 'heating_month_cos', 'temp_category', 'cold_warning_level', 'wind_category', 'is_holiday', 'holiday_type', 'peak_time', 'HDD18', 'apparent_temp', 'ta_lag_3h', 'ta_lag_6h', 'ta_lag_24h', 'ta_ma_6h', 'ta_ma_12h', 'ta_ma_24h', 'ta_diff_3h', 'ta_diff_6h', 'tm_daily', 'daily_ta_min', 'daily_ta_max', 'daily_ta_mean', 'daily_temp_range']\n",
            "   non_heating: 69,768ê°œ, ì»¬ëŸ¼: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand', 'year', 'month', 'day', 'hour', 'dayofweek', 'dayofyear', 'ta_missing', 'ws_missing', 'rn_day_missing', 'rn_hr1_missing', 'hm_missing', 'si_missing', 'ta_chi_missing', 'heat_demand_missing', 'heating_season', 'day_of_year', 'cold_extreme', 'strong_wind', 'heavy_rain', 'hour_cat', 'month_cat', 'weekday_name', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'non_heating_month_order', 'non_heating_month_sin', 'non_heating_month_cos', 'temp_category', 'wind_category', 'is_holiday', 'holiday_type', 'peak_time', 'HDD18', 'ta_lag_3h', 'ta_lag_6h', 'ta_lag_24h', 'ta_ma_6h', 'ta_ma_12h', 'ta_ma_24h', 'ta_diff_3h', 'ta_diff_6h', 'tm_daily', 'daily_ta_min', 'daily_ta_max', 'daily_ta_mean', 'daily_temp_range']\n",
            "   âœ… heating: 97147ê°œ ë°ì´í„° ì¶”ê°€\n",
            "   âœ… non_heating: 69768ê°œ ë°ì´í„° ì¶”ê°€\n",
            "âœ… test_df ì¬êµ¬ì„± ì™„ë£Œ!\n",
            "   í¬ê¸°: (166915, 66)\n",
            "   ì»¬ëŸ¼: ['tm', 'branch_id', 'ta', 'wd', 'ws', 'rn_day', 'rn_hr1', 'hm', 'si', 'ta_chi', 'heat_demand', 'year', 'month', 'day', 'hour', 'dayofweek', 'dayofyear', 'ta_missing', 'ws_missing', 'rn_day_missing', 'rn_hr1_missing', 'hm_missing', 'si_missing', 'ta_chi_missing', 'heat_demand_missing', 'heating_season', 'day_of_year', 'cold_extreme', 'strong_wind', 'heavy_rain', 'hour_cat', 'month_cat', 'weekday_name', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 'dayofweek_cos', 'heating_month_order', 'heating_month_sin', 'heating_month_cos', 'temp_category', 'cold_warning_level', 'wind_category', 'is_holiday', 'holiday_type', 'peak_time', 'HDD18', 'apparent_temp', 'ta_lag_3h', 'ta_lag_6h', 'ta_lag_24h', 'ta_ma_6h', 'ta_ma_12h', 'ta_ma_24h', 'ta_diff_3h', 'ta_diff_6h', 'tm_daily', 'daily_ta_min', 'daily_ta_max', 'daily_ta_mean', 'daily_temp_range', 'non_heating_month_order', 'non_heating_month_sin', 'non_heating_month_cos']\n",
            "   ì¸ë±ìŠ¤ ë²”ìœ„: 0 ~ 97146\n",
            "âœ… í•„ìˆ˜ ì»¬ëŸ¼ ëª¨ë‘ ìˆìŒ: ['tm', 'branch_id']\n",
            "\n",
            "ğŸ¯ test_df ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•©ì„ ì§„í–‰í•©ë‹ˆë‹¤...\n",
            "ğŸ’¾ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ë° ì €ì¥...\n",
            "ğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì¤‘...\n",
            "   heating: 97147ê°œ ì¸ë±ìŠ¤, 97147ê°œ ì˜ˆì¸¡ê°’\n",
            "     âœ… 97147ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹ ì™„ë£Œ\n",
            "   non_heating: 69768ê°œ ì¸ë±ìŠ¤, 69768ê°œ ì˜ˆì¸¡ê°’\n",
            "     âœ… 69768ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹ ì™„ë£Œ\n",
            "\n",
            "ğŸ“Š ì´ 166,915ê°œ / 166,915ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹ ì™„ë£Œ (100.0%)\n",
            "\n",
            "ğŸ“ˆ ìµœì¢… ì˜ˆì¸¡ê°’ í†µê³„:\n",
            "ëª¨ë¸                         í‰ê·      í‘œì¤€í¸ì°¨      ìµœì†Œê°’      ìµœëŒ€ê°’      0ê°œìˆ˜\n",
            "----------------------------------------------------------------------\n",
            "stacking_prediction      28.7     41.2      0.0    302.0    70012\n",
            "prophet_prediction       29.5     41.1      0.0    266.7    69997\n",
            "catboost_prediction      28.0     40.3      0.0    302.1    70103\n",
            "\n",
            "ğŸ“ ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: advanced_stacking_ensemble_predictions.csv\n",
            "ğŸ“ ì œì¶œìš© íŒŒì¼ ì €ì¥: submission_advanced_stacking.csv\n",
            "\n",
            "ğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ í†µê³„ (ìŠ¤íƒœí‚¹ ëª¨ë¸):\n",
            "       count   mean    std  min    max\n",
            "ë¹„ë‚œë°©ì‹œì¦Œ  69768  34.27  42.89  0.0  302.0\n",
            "ë‚œë°©ì‹œì¦Œ   97147  24.63  39.50  0.0  298.1\n",
            "\n",
            "ğŸŠ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n",
            "ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼: submission_advanced_stacking.csv\n",
            "ğŸ“ˆ stacking ì˜ˆì¸¡ê°’ ìš”ì•½:\n",
            "   í‰ê· : 28.7\n",
            "   0ì´ ì•„ë‹Œ ê°’: 96,903ê°œ (58.1%)\n",
            "   ë²”ìœ„: [0.0, 302.0]\n"
          ]
        }
      ],
      "source": [
        "# test_groupsì—ì„œ test_df ë¹ ë¥¸ ì¬êµ¬ì„±\n",
        "print(\"ğŸ”„ test_groupsì—ì„œ test_df ì¬êµ¬ì„± ì¤‘...\")\n",
        "\n",
        "# test_groups ì •ë³´ í™•ì¸\n",
        "print(f\"test_groups ì •ë³´:\")\n",
        "for group_name, group_data in test_groups.items():\n",
        "    print(f\"   {group_name}: {len(group_data):,}ê°œ, ì»¬ëŸ¼: {list(group_data.columns)}\")\n",
        "\n",
        "# test_df ì¬êµ¬ì„±\n",
        "try:\n",
        "    # ëª¨ë“  ê·¸ë£¹ì„ í•©ì³ì„œ test_df ìƒì„±\n",
        "    test_df_list = []\n",
        "    \n",
        "    for group_name, group_data in test_groups.items():\n",
        "        if len(group_data) > 0:\n",
        "            # ê·¸ë£¹ ë°ì´í„° ë³µì‚¬\n",
        "            group_copy = group_data.copy()\n",
        "            test_df_list.append(group_copy)\n",
        "            print(f\"   âœ… {group_name}: {len(group_copy)}ê°œ ë°ì´í„° ì¶”ê°€\")\n",
        "    \n",
        "    # ëª¨ë“  ê·¸ë£¹ í•©ì¹˜ê¸° (ì›ë³¸ ì¸ë±ìŠ¤ ìœ ì§€)\n",
        "    test_df = pd.concat(test_df_list, ignore_index=False)\n",
        "    test_df = test_df.sort_index()  # ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
        "    \n",
        "    print(f\"âœ… test_df ì¬êµ¬ì„± ì™„ë£Œ!\")\n",
        "    print(f\"   í¬ê¸°: {test_df.shape}\")\n",
        "    print(f\"   ì»¬ëŸ¼: {list(test_df.columns)}\")\n",
        "    print(f\"   ì¸ë±ìŠ¤ ë²”ìœ„: {test_df.index.min()} ~ {test_df.index.max()}\")\n",
        "    \n",
        "    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
        "    required_cols = ['tm', 'branch_id']\n",
        "    missing_cols = [col for col in required_cols if col not in test_df.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}\")\n",
        "    else:\n",
        "        print(f\"âœ… í•„ìˆ˜ ì»¬ëŸ¼ ëª¨ë‘ ìˆìŒ: {required_cols}\")\n",
        "    \n",
        "    # heating_season ì»¬ëŸ¼ í™•ì¸/ìƒì„±\n",
        "    if 'heating_season' not in test_df.columns:\n",
        "        print(\"ğŸ”§ heating_season ì»¬ëŸ¼ì„ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
        "        test_df['heating_season'] = test_df['tm'].dt.month.isin([10,11,12,1,2,3,4]).astype(int)\n",
        "        print(\"âœ… heating_season ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ test_df ì¬êµ¬ì„± ì‹¤íŒ¨: {e}\")\n",
        "    raise e\n",
        "\n",
        "print(f\"\\nğŸ¯ test_df ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•©ì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "# Colab í™˜ê²½ ì²´í¬\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•©\n",
        "print(\"ğŸ’¾ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ë° ì €ì¥...\")\n",
        "\n",
        "# ê¸°ë³¸ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "result_df = test_df[['tm', 'branch_id', 'heating_season']].copy()\n",
        "\n",
        "# ê·¸ë£¹ë³„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©\n",
        "final_stacking_pred = np.zeros(len(test_df))\n",
        "final_prophet_pred = np.zeros(len(test_df))\n",
        "final_catboost_pred = np.zeros(len(test_df))\n",
        "\n",
        "print(\"ğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ì¤‘...\")\n",
        "\n",
        "# ê° ê·¸ë£¹ë³„ë¡œ í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì— ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "total_assigned = 0\n",
        "for group_name, group_data in test_groups.items():\n",
        "    if len(group_data) > 0 and group_name in test_predictions:\n",
        "        group_indices = group_data.index\n",
        "        group_pred = test_predictions[group_name]\n",
        "        \n",
        "        print(f\"   {group_name}: {len(group_indices)}ê°œ ì¸ë±ìŠ¤, {len(group_pred)}ê°œ ì˜ˆì¸¡ê°’\")\n",
        "        \n",
        "        # ì¸ë±ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸°\n",
        "        min_length = min(len(group_indices), len(group_pred))\n",
        "        if min_length > 0:\n",
        "            # ì¸ë±ìŠ¤ê°€ test_df ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸\n",
        "            valid_indices = [idx for idx in group_indices[:min_length] if idx < len(test_df)]\n",
        "            valid_length = len(valid_indices)\n",
        "            \n",
        "            if valid_length > 0:\n",
        "                final_stacking_pred[valid_indices] = group_pred[:valid_length]\n",
        "                total_assigned += valid_length\n",
        "                \n",
        "                # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ë„ ì €ì¥\n",
        "                if group_name in individual_predictions:\n",
        "                    individual_pred = individual_predictions[group_name]\n",
        "                    \n",
        "                    if 'prophet' in individual_pred and len(individual_pred['prophet']) >= valid_length:\n",
        "                        final_prophet_pred[valid_indices] = individual_pred['prophet'][:valid_length]\n",
        "                    if 'catboost' in individual_pred and len(individual_pred['catboost']) >= valid_length:\n",
        "                        final_catboost_pred[valid_indices] = individual_pred['catboost'][:valid_length]\n",
        "                \n",
        "                print(f\"     âœ… {valid_length}ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹ ì™„ë£Œ\")\n",
        "            else:\n",
        "                print(f\"     âš ï¸ ìœ íš¨í•œ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸ {group_name}: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "\n",
        "print(f\"\\nğŸ“Š ì´ {total_assigned:,}ê°œ / {len(test_df):,}ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹ ì™„ë£Œ ({total_assigned/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "# í• ë‹¹ë˜ì§€ ì•Šì€ ì˜ˆì¸¡ê°’ì´ ë§ë‹¤ë©´ ê²½ê³ \n",
        "if total_assigned < len(test_df) * 0.5:\n",
        "    print(f\"âš ï¸ í• ë‹¹ëœ ì˜ˆì¸¡ê°’ì´ ì ìŠµë‹ˆë‹¤! ì˜ˆì¸¡ ê³¼ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "# ìŒìˆ˜ê°’ ì œê±°\n",
        "final_stacking_pred = np.maximum(final_stacking_pred, 0)\n",
        "final_prophet_pred = np.maximum(final_prophet_pred, 0)\n",
        "final_catboost_pred = np.maximum(final_catboost_pred, 0)\n",
        "\n",
        "# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "result_df['stacking_prediction'] = final_stacking_pred.round(1)\n",
        "result_df['prophet_prediction'] = final_prophet_pred.round(1)\n",
        "result_df['catboost_prediction'] = final_catboost_pred.round(1)\n",
        "\n",
        "# í†µê³„ ì¶œë ¥\n",
        "print(f\"\\nğŸ“ˆ ìµœì¢… ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
        "prediction_cols = ['stacking_prediction', 'prophet_prediction', 'catboost_prediction']\n",
        "\n",
        "print(f\"{'ëª¨ë¸':20s} {'í‰ê· ':>8s} {'í‘œì¤€í¸ì°¨':>8s} {'ìµœì†Œê°’':>8s} {'ìµœëŒ€ê°’':>8s} {'0ê°œìˆ˜':>8s}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for col in prediction_cols:\n",
        "    mean_val = result_df[col].mean()\n",
        "    std_val = result_df[col].std()\n",
        "    max_val = result_df[col].max()\n",
        "    min_val = result_df[col].min()\n",
        "    zero_count = (result_df[col] == 0).sum()\n",
        "    \n",
        "    print(f\"{col:20s} {mean_val:8.1f} {std_val:8.1f} {min_val:8.1f} {max_val:8.1f} {zero_count:8d}\")\n",
        "\n",
        "# CSV íŒŒì¼ ì €ì¥\n",
        "result_filename = 'advanced_stacking_ensemble_predictions.csv'\n",
        "result_df.to_csv(result_filename, index=False)\n",
        "print(f\"\\nğŸ“ ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {result_filename}\")\n",
        "\n",
        "# ì œì¶œìš© íŒŒì¼ ìƒì„± (ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ê²°ê³¼ë§Œ)\n",
        "submission_df = test_df[['tm', 'branch_id']].copy()\n",
        "submission_df['heat_demand'] = result_df['stacking_prediction']\n",
        "\n",
        "submission_filename = 'submission_advanced_stacking.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "print(f\"ğŸ“ ì œì¶œìš© íŒŒì¼ ì €ì¥: {submission_filename}\")\n",
        "\n",
        "# ê·¸ë£¹ë³„ ì˜ˆì¸¡ í†µê³„\n",
        "print(f\"\\nğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ í†µê³„ (ìŠ¤íƒœí‚¹ ëª¨ë¸):\")\n",
        "try:\n",
        "    group_stats = result_df.groupby('heating_season')['stacking_prediction'].agg([\n",
        "        'count', 'mean', 'std', 'min', 'max'\n",
        "    ]).round(2)\n",
        "    group_stats.index = ['ë¹„ë‚œë°©ì‹œì¦Œ', 'ë‚œë°©ì‹œì¦Œ']\n",
        "    print(group_stats)\n",
        "except Exception as e:\n",
        "    print(f\"   ê·¸ë£¹ë³„ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# Google Drive ì €ì¥ (Colab í™˜ê²½)\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        save_drive = input(\"\\nGoogle Driveì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").lower().strip()\n",
        "        if save_drive == 'y':\n",
        "            import os\n",
        "            os.system(f\"cp {result_filename} /content/drive/MyDrive/\")\n",
        "            os.system(f\"cp {submission_filename} /content/drive/MyDrive/\")\n",
        "            print(\"âœ… Google Drive ì €ì¥ ì™„ë£Œ!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Google Drive ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "print(\"\\nğŸŠ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼: {submission_filename}\")\n",
        "\n",
        "# ìµœì¢… í™•ì¸\n",
        "stacking_nonzero = (result_df['stacking_prediction'] > 0).sum()\n",
        "print(f\"ğŸ“ˆ stacking ì˜ˆì¸¡ê°’ ìš”ì•½:\")\n",
        "print(f\"   í‰ê· : {result_df['stacking_prediction'].mean():.1f}\")\n",
        "print(f\"   0ì´ ì•„ë‹Œ ê°’: {stacking_nonzero:,}ê°œ ({stacking_nonzero/len(result_df)*100:.1f}%)\")\n",
        "print(f\"   ë²”ìœ„: [{result_df['stacking_prediction'].min():.1f}, {result_df['stacking_prediction'].max():.1f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_summary"
      },
      "source": [
        "## 1ï¸âƒ£2ï¸âƒ£ ìµœì¢… ë¶„ì„ ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "final_analysis"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ë° ì €ì¥...\n",
            "ğŸ¯ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì‹¤ì œ ì˜ˆì¸¡ê°’ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
            "ğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì •í™• í†µí•© ì¤‘...\n",
            "\n",
            "ğŸ“Š heating ê·¸ë£¹ ì²˜ë¦¬:\n",
            "   ê·¸ë£¹ ë°ì´í„° í¬ê¸°: 97147\n",
            "   ê·¸ë£¹ ì¸ë±ìŠ¤ ë²”ìœ„: 0 ~ 97146\n",
            "   ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ê°’ í¬ê¸°: 97147\n",
            "   âœ… ìŠ¤íƒœí‚¹: 97147ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
            "   âœ… Prophet: 97147ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
            "   âœ… CatBoost: 97147ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
            "\n",
            "ğŸ“Š non_heating ê·¸ë£¹ ì²˜ë¦¬:\n",
            "   ê·¸ë£¹ ë°ì´í„° í¬ê¸°: 69768\n",
            "   ê·¸ë£¹ ì¸ë±ìŠ¤ ë²”ìœ„: 0 ~ 69767\n",
            "   ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ê°’ í¬ê¸°: 69768\n",
            "   âœ… ìŠ¤íƒœí‚¹: 69768ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
            "   âœ… Prophet: 69768ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
            "   âœ… CatBoost: 69768ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
            "\n",
            "ğŸ“Š ì „ì²´ í• ë‹¹ ì™„ë£Œ: 166,915ê°œ / 166,915ê°œ (100.0%)\n",
            "\n",
            "ğŸ” ê·¸ë£¹ë³„ í• ë‹¹ ê²°ê³¼:\n",
            "   heating: 97,147/97,147 (100.0% ì»¤ë²„ë¦¬ì§€)\n",
            "   non_heating: 69,768/69,768 (100.0% ì»¤ë²„ë¦¬ì§€)\n",
            "\n",
            "ğŸ” ì˜ˆì¸¡ê°’ í’ˆì§ˆ ê²€ì¦:\n",
            "   í• ë‹¹ëœ ì˜ˆì¸¡ê°’: 166,915ê°œ\n",
            "   ì˜ê°’ ê°œìˆ˜: 69,998ê°œ (41.9%)\n",
            "   ğŸ’¡ ì˜ê°’ ë¶„ì„ ì¤‘...\n",
            "   ğŸ“ ì˜ê°’ì´ ë§ì€ ì§€ì‚¬: {'O': 8785, 'P': 8785, 'Q': 8785, 'R': 8785, 'S': 8785}\n",
            "   ğŸ” ì˜ê°’ ì›ì¸ ë¶„ì„:\n",
            "   ğŸ“Š ê°œë³„ ëª¨ë¸ë“¤ì˜ ì‹¤ì œ 0 ì˜ˆì¸¡ ë¹„ìœ¨: 0.3%\n",
            "   âš ï¸ í• ë‹¹ ì‹¤íŒ¨ë¡œ ì¸í•œ ì˜ê°’ì´ ë§ì•„ ë³´ì…ë‹ˆë‹¤\n",
            "   ğŸ”§ ë‚¨ì€ ì˜ê°’ ì¶”ê°€ ë³µêµ¬ ì‹œë„...\n",
            "   âœ… 3009ê°œ ì˜ê°’ì„ ë™ì¼ ì¡°ê±´ í‰ê· ê°’ìœ¼ë¡œ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤\n",
            "\n",
            "ğŸ“ˆ ìµœì¢… ì˜ˆì¸¡ê°’ í†µê³„:\n",
            "ëª¨ë¸                         í‰ê·      í‘œì¤€í¸ì°¨      ìµœì†Œê°’      ìµœëŒ€ê°’     ì˜ê°’ê°œìˆ˜\n",
            "----------------------------------------------------------------------\n",
            "stacking_prediction      29.3     40.9      0.0    302.0    64099\n",
            "prophet_prediction       29.5     41.1      0.0    266.7    69997\n",
            "catboost_prediction      28.0     40.3      0.0    302.1    70103\n",
            "\n",
            "ğŸ” ëª¨ë¸ê°„ ìƒê´€ê´€ê³„:\n",
            "   Prophet vs CatBoost: 0.979\n",
            "   Prophet vs Stacking: 0.984\n",
            "   CatBoost vs Stacking: 0.999\n",
            "   (0ì´ ì•„ë‹Œ 96,585ê°œ ë°ì´í„° ê¸°ì¤€)\n",
            "\n",
            "ğŸ“ ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: model_accurate_predictions.csv\n",
            "ğŸ“ ì œì¶œìš© íŒŒì¼ ì €ì¥: model_accurate_submission.csv\n",
            "\n",
            "ğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ í†µê³„ (ìŠ¤íƒœí‚¹ ëª¨ë¸):\n",
            "       count   mean    std  min    max\n",
            "ë¹„ë‚œë°©ì‹œì¦Œ  69768  34.99  42.47  0.0  302.0\n",
            "ë‚œë°©ì‹œì¦Œ   97147  25.15  39.29  0.0  298.1\n",
            "\n",
            "ğŸ“ O~S ì§€ì‚¬ë³„ ë³µêµ¬ ìƒíƒœ:\n",
            "   âŒ ì§€ì‚¬ O: í‰ê·    0.00 (ë‚œë°© 0.0, ë¹„ë‚œë°© 0.0)\n",
            "      ì˜ê°’: 8,785ê°œ / 8,785ê°œ (100.0%)\n",
            "   âŒ ì§€ì‚¬ P: í‰ê·    0.00 (ë‚œë°© 0.0, ë¹„ë‚œë°© 0.0)\n",
            "      ì˜ê°’: 8,785ê°œ / 8,785ê°œ (100.0%)\n",
            "   âŒ ì§€ì‚¬ Q: í‰ê·    0.00 (ë‚œë°© 0.0, ë¹„ë‚œë°© 0.0)\n",
            "      ì˜ê°’: 8,785ê°œ / 8,785ê°œ (100.0%)\n",
            "   âŒ ì§€ì‚¬ R: í‰ê·    0.00 (ë‚œë°© 0.0, ë¹„ë‚œë°© 0.0)\n",
            "      ì˜ê°’: 8,785ê°œ / 8,785ê°œ (100.0%)\n",
            "   âŒ ì§€ì‚¬ S: í‰ê·    0.00 (ë‚œë°© 0.0, ë¹„ë‚œë°© 0.0)\n",
            "      ì˜ê°’: 8,785ê°œ / 8,785ê°œ (100.0%)\n",
            "\n",
            "ğŸ• ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ íŒ¨í„´:\n",
            "   ìƒìœ„ 5ê°œ ì‹œê°„ëŒ€: {17: 34.8, 5: 34.7, 18: 33.8, 6: 33.7, 4: 32.5}\n",
            "\n",
            "ğŸŠ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\n",
            "==================================================\n",
            "ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: 166,915ê°œ\n",
            "ğŸ“ˆ ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡ê°’ ì‚¬ìš©ë¥ : 166,915/166,915 (100.0%)\n",
            "ğŸ“‰ ìµœì¢… ì˜ê°’: 64,099ê°œ (38.4%)\n",
            "ğŸ“ ì €ì¥ëœ íŒŒì¼:\n",
            "   - ìƒì„¸ ê²°ê³¼: model_accurate_predictions.csv\n",
            "   - ì œì¶œìš©: model_accurate_submission.csv\n",
            "ğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í‰ê·  ì˜ˆì¸¡ê°’: 29.3\n",
            "ğŸ† í›ˆë ¨ëœ ëª¨ë¸ì˜ ì‹¤ì œ ì˜ˆì¸¡ê°’ ê¸°ë°˜ ì œì¶œ ì¤€ë¹„ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© (ëª¨ë¸ ì˜ˆì¸¡ê°’ ì •í™• ì¶”ì¶œ ë²„ì „)\n",
        "print(\"ğŸ’¾ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í†µí•© ë° ì €ì¥...\")\n",
        "print(\"ğŸ¯ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì‹¤ì œ ì˜ˆì¸¡ê°’ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤\")\n",
        "\n",
        "# ê¸°ë³¸ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)\n",
        "required_cols = ['tm', 'branch_id']\n",
        "optional_cols = ['heating_season']\n",
        "\n",
        "# ê¸°ë³¸ ì»¬ëŸ¼ í™•ì¸\n",
        "result_df = test_df[required_cols].copy()\n",
        "\n",
        "# ì„ íƒì  ì»¬ëŸ¼ ì¶”ê°€\n",
        "for col in optional_cols:\n",
        "    if col in test_df.columns:\n",
        "        result_df[col] = test_df[col]\n",
        "    else:\n",
        "        print(f\"âš ï¸ '{col}' ì»¬ëŸ¼ì´ ì—†ì–´ì„œ ìë™ ìƒì„±í•©ë‹ˆë‹¤\")\n",
        "        if col == 'heating_season':\n",
        "            # tm ì»¬ëŸ¼ì—ì„œ heating_season ìƒì„±\n",
        "            result_df[col] = pd.to_datetime(result_df['tm']).dt.month.isin([10,11,12,1,2,3,4]).astype(int)\n",
        "\n",
        "# ê·¸ë£¹ë³„ ì˜ˆì¸¡ ê²°ê³¼ í†µí•© (ê°œì„ ëœ ë°©ì‹)\n",
        "final_stacking_pred = np.zeros(len(test_df))\n",
        "final_prophet_pred = np.zeros(len(test_df))\n",
        "final_catboost_pred = np.zeros(len(test_df))\n",
        "\n",
        "print(\"ğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ ê²°ê³¼ ì •í™• í†µí•© ì¤‘...\")\n",
        "\n",
        "# ğŸ¯ í•µì‹¬ ê°œì„ : ìˆœì„œ ë³´ì¥ëœ ì •í™•í•œ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "total_assigned = 0\n",
        "assignment_log = {}\n",
        "\n",
        "for group_name, group_data in test_groups.items():\n",
        "    if len(group_data) > 0 and group_name in test_predictions:\n",
        "        \n",
        "        print(f\"\\nğŸ“Š {group_name} ê·¸ë£¹ ì²˜ë¦¬:\")\n",
        "        \n",
        "        # ê·¸ë£¹ ë°ì´í„°ì™€ ì˜ˆì¸¡ê°’\n",
        "        group_indices = group_data.index.tolist()\n",
        "        group_stacking_pred = np.array(test_predictions[group_name])\n",
        "        \n",
        "        print(f\"   ê·¸ë£¹ ë°ì´í„° í¬ê¸°: {len(group_data)}\")\n",
        "        print(f\"   ê·¸ë£¹ ì¸ë±ìŠ¤ ë²”ìœ„: {min(group_indices)} ~ {max(group_indices)}\")\n",
        "        print(f\"   ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ê°’ í¬ê¸°: {len(group_stacking_pred)}\")\n",
        "        \n",
        "        # ê¸¸ì´ í™•ì¸ ë° ì•ˆì „í•œ í• ë‹¹\n",
        "        min_length = min(len(group_indices), len(group_stacking_pred))\n",
        "        \n",
        "        if min_length > 0:\n",
        "            # ğŸ¯ ìˆœì„œëŒ€ë¡œ ì •í™•íˆ í• ë‹¹\n",
        "            assigned_count = 0\n",
        "            \n",
        "            for i in range(min_length):\n",
        "                test_idx = group_indices[i]\n",
        "                \n",
        "                # test_df ë²”ìœ„ ë‚´ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸\n",
        "                if 0 <= test_idx < len(test_df):\n",
        "                    # ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "                    final_stacking_pred[test_idx] = group_stacking_pred[i]\n",
        "                    assigned_count += 1\n",
        "            \n",
        "            print(f\"   âœ… ìŠ¤íƒœí‚¹: {assigned_count}ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\")\n",
        "            total_assigned += assigned_count\n",
        "            \n",
        "            # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ë„ ì •í™•íˆ í• ë‹¹\n",
        "            if group_name in individual_predictions:\n",
        "                individual_pred = individual_predictions[group_name]\n",
        "                \n",
        "                # Prophet ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "                if 'prophet' in individual_pred:\n",
        "                    prophet_pred = np.array(individual_pred['prophet'])\n",
        "                    prophet_assigned = 0\n",
        "                    \n",
        "                    for i in range(min(min_length, len(prophet_pred))):\n",
        "                        test_idx = group_indices[i]\n",
        "                        if 0 <= test_idx < len(test_df):\n",
        "                            final_prophet_pred[test_idx] = prophet_pred[i]\n",
        "                            prophet_assigned += 1\n",
        "                    \n",
        "                    print(f\"   âœ… Prophet: {prophet_assigned}ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\")\n",
        "                \n",
        "                # CatBoost ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "                if 'catboost' in individual_pred:\n",
        "                    catboost_pred = np.array(individual_pred['catboost'])\n",
        "                    catboost_assigned = 0\n",
        "                    \n",
        "                    for i in range(min(min_length, len(catboost_pred))):\n",
        "                        test_idx = group_indices[i]\n",
        "                        if 0 <= test_idx < len(test_df):\n",
        "                            final_catboost_pred[test_idx] = catboost_pred[i]\n",
        "                            catboost_assigned += 1\n",
        "                    \n",
        "                    print(f\"   âœ… CatBoost: {catboost_assigned}ê°œ ì˜ˆì¸¡ê°’ í• ë‹¹\")\n",
        "            \n",
        "            # í• ë‹¹ ë¡œê·¸ ì €ì¥\n",
        "            assignment_log[group_name] = {\n",
        "                'total_data': len(group_data),\n",
        "                'predictions': len(group_stacking_pred),\n",
        "                'assigned': assigned_count,\n",
        "                'coverage': assigned_count / len(group_data) * 100\n",
        "            }\n",
        "        \n",
        "        else:\n",
        "            print(f\"   âš ï¸ í• ë‹¹í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "    else:\n",
        "        if group_name not in test_predictions:\n",
        "            print(f\"   âš ï¸ {group_name}: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸ {group_name}: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
        "\n",
        "print(f\"\\nğŸ“Š ì „ì²´ í• ë‹¹ ì™„ë£Œ: {total_assigned:,}ê°œ / {len(test_df):,}ê°œ ({total_assigned/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "# ğŸ” í• ë‹¹ ê²°ê³¼ ìƒì„¸ ë¶„ì„\n",
        "print(f\"\\nğŸ” ê·¸ë£¹ë³„ í• ë‹¹ ê²°ê³¼:\")\n",
        "for group_name, log in assignment_log.items():\n",
        "    print(f\"   {group_name}: {log['assigned']:,}/{log['total_data']:,} ({log['coverage']:.1f}% ì»¤ë²„ë¦¬ì§€)\")\n",
        "\n",
        "# ìŒìˆ˜ê°’ ì œê±° (í•˜ì§€ë§Œ ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡ê°’ ìµœëŒ€í•œ ë³´ì¡´)\n",
        "original_negatives = np.sum(final_stacking_pred < 0)\n",
        "final_stacking_pred = np.maximum(final_stacking_pred, 0)\n",
        "final_prophet_pred = np.maximum(final_prophet_pred, 0) \n",
        "final_catboost_pred = np.maximum(final_catboost_pred, 0)\n",
        "\n",
        "if original_negatives > 0:\n",
        "    print(f\"âš ï¸ {original_negatives}ê°œ ìŒìˆ˜ ì˜ˆì¸¡ê°’ì„ 0ìœ¼ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤\")\n",
        "\n",
        "# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "result_df['stacking_prediction'] = final_stacking_pred.round(1)\n",
        "result_df['prophet_prediction'] = final_prophet_pred.round(1)\n",
        "result_df['catboost_prediction'] = final_catboost_pred.round(1)\n",
        "\n",
        "# ğŸ” ì˜ˆì¸¡ê°’ í’ˆì§ˆ ê²€ì¦\n",
        "unassigned_count = np.sum(final_stacking_pred == 0)\n",
        "print(f\"\\nğŸ” ì˜ˆì¸¡ê°’ í’ˆì§ˆ ê²€ì¦:\")\n",
        "print(f\"   í• ë‹¹ëœ ì˜ˆì¸¡ê°’: {total_assigned:,}ê°œ\")\n",
        "print(f\"   ì˜ê°’ ê°œìˆ˜: {unassigned_count:,}ê°œ ({unassigned_count/len(test_df)*100:.1f}%)\")\n",
        "\n",
        "if unassigned_count > 0:\n",
        "    print(f\"   ğŸ’¡ ì˜ê°’ ë¶„ì„ ì¤‘...\")\n",
        "    \n",
        "    # ì§€ì‚¬ë³„ ì˜ê°’ ë¶„í¬ í™•ì¸\n",
        "    zero_by_branch = result_df[result_df['stacking_prediction'] == 0]['branch_id'].value_counts()\n",
        "    if len(zero_by_branch) > 0:\n",
        "        print(f\"   ğŸ“ ì˜ê°’ì´ ë§ì€ ì§€ì‚¬: {dict(zero_by_branch.head())}\")\n",
        "    \n",
        "    # ğŸ¯ ì‹¤ì œ ëª¨ë¸ì´ 0ì„ ì˜ˆì¸¡í–ˆëŠ”ì§€ vs í• ë‹¹ ì‹¤íŒ¨ì¸ì§€ êµ¬ë¶„\n",
        "    print(f\"   ğŸ” ì˜ê°’ ì›ì¸ ë¶„ì„:\")\n",
        "    \n",
        "    # individual_predictionsì—ì„œ ì‹¤ì œ 0 ì˜ˆì¸¡ ë¹„ìœ¨ í™•ì¸\n",
        "    total_model_zeros = 0\n",
        "    total_model_predictions = 0\n",
        "    \n",
        "    for group_name in ['heating', 'non_heating']:\n",
        "        if group_name in individual_predictions:\n",
        "            for model_name, pred in individual_predictions[group_name].items():\n",
        "                pred_array = np.array(pred)\n",
        "                model_zeros = np.sum(pred_array == 0)\n",
        "                total_model_zeros += model_zeros\n",
        "                total_model_predictions += len(pred_array)\n",
        "    \n",
        "    model_zero_rate = total_model_zeros / total_model_predictions * 100 if total_model_predictions > 0 else 0\n",
        "    print(f\"   ğŸ“Š ê°œë³„ ëª¨ë¸ë“¤ì˜ ì‹¤ì œ 0 ì˜ˆì¸¡ ë¹„ìœ¨: {model_zero_rate:.1f}%\")\n",
        "    \n",
        "    if unassigned_count > total_model_zeros * 1.5:  # í• ë‹¹ ì‹¤íŒ¨ê°€ ë” ë§ë‹¤ë©´\n",
        "        print(f\"   âš ï¸ í• ë‹¹ ì‹¤íŒ¨ë¡œ ì¸í•œ ì˜ê°’ì´ ë§ì•„ ë³´ì…ë‹ˆë‹¤\")\n",
        "        \n",
        "        # ğŸ”§ ì¶”ê°€ ë³µêµ¬ ì‹œë„: ë‚¨ì€ ì˜ê°’ë“¤ì„ ê°™ì€ ì§€ì‚¬ì˜ ì‹¤ì œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
        "        print(f\"   ğŸ”§ ë‚¨ì€ ì˜ê°’ ì¶”ê°€ ë³µêµ¬ ì‹œë„...\")\n",
        "        \n",
        "        # ì˜ê°’ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        zero_mask = result_df['stacking_prediction'] == 0\n",
        "        zero_indices = result_df[zero_mask].index\n",
        "        \n",
        "        fixed_zeros = 0\n",
        "        \n",
        "        # ì§€ì‚¬ë³„, ì‹œì¦Œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬\n",
        "        for branch in result_df['branch_id'].unique():\n",
        "            for season in [0, 1]:\n",
        "                # í•´ë‹¹ ì§€ì‚¬, ì‹œì¦Œì—ì„œ ì˜ê°’ì¸ í–‰ë“¤\n",
        "                branch_season_zeros = result_df[\n",
        "                    (result_df['branch_id'] == branch) & \n",
        "                    (result_df['heating_season'] == season) & \n",
        "                    (result_df['stacking_prediction'] == 0)\n",
        "                ]\n",
        "                \n",
        "                if len(branch_season_zeros) > 0:\n",
        "                    # ê°™ì€ ì§€ì‚¬, ì‹œì¦Œì—ì„œ 0ì´ ì•„ë‹Œ ê°’ë“¤ì˜ í‰ê· \n",
        "                    same_condition_nonzero = result_df[\n",
        "                        (result_df['branch_id'] == branch) & \n",
        "                        (result_df['heating_season'] == season) & \n",
        "                        (result_df['stacking_prediction'] > 0)\n",
        "                    ]['stacking_prediction']\n",
        "                    \n",
        "                    if len(same_condition_nonzero) > 0:\n",
        "                        replacement_val = same_condition_nonzero.mean()\n",
        "                        \n",
        "                        # í•´ë‹¹ ì¡°ê±´ì˜ ì˜ê°’ë“¤ì„ ëª¨ë‘ ëŒ€ì²´\n",
        "                        result_df.loc[branch_season_zeros.index, 'stacking_prediction'] = replacement_val\n",
        "                        fixed_zeros += len(branch_season_zeros)\n",
        "        \n",
        "        if fixed_zeros > 0:\n",
        "            print(f\"   âœ… {fixed_zeros}ê°œ ì˜ê°’ì„ ë™ì¼ ì¡°ê±´ í‰ê· ê°’ìœ¼ë¡œ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤\")\n",
        "    else:\n",
        "        print(f\"   â„¹ï¸ ëŒ€ë¶€ë¶„ ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡ê°’(0)ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤\")\n",
        "\n",
        "# ìµœì¢… ì˜ê°’ ê°œìˆ˜ ì¬ê³„ì‚°\n",
        "final_zeros = np.sum(result_df['stacking_prediction'] == 0)\n",
        "\n",
        "# í†µê³„ ì¶œë ¥\n",
        "print(f\"\\nğŸ“ˆ ìµœì¢… ì˜ˆì¸¡ê°’ í†µê³„:\")\n",
        "prediction_cols = ['stacking_prediction', 'prophet_prediction', 'catboost_prediction']\n",
        "\n",
        "print(f\"{'ëª¨ë¸':20s} {'í‰ê· ':>8s} {'í‘œì¤€í¸ì°¨':>8s} {'ìµœì†Œê°’':>8s} {'ìµœëŒ€ê°’':>8s} {'ì˜ê°’ê°œìˆ˜':>8s}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for col in prediction_cols:\n",
        "    mean_val = result_df[col].mean()\n",
        "    std_val = result_df[col].std()\n",
        "    max_val = result_df[col].max()\n",
        "    min_val = result_df[col].min()\n",
        "    zero_count = (result_df[col] == 0).sum()\n",
        "    \n",
        "    print(f\"{col:20s} {mean_val:8.1f} {std_val:8.1f} {min_val:8.1f} {max_val:8.1f} {zero_count:8d}\")\n",
        "\n",
        "# ğŸ“Š ëª¨ë¸ê°„ ìƒê´€ê´€ê³„ ë¶„ì„\n",
        "print(f\"\\nğŸ” ëª¨ë¸ê°„ ìƒê´€ê´€ê³„:\")\n",
        "try:\n",
        "    # 0ì´ ì•„ë‹Œ ê°’ë“¤ë§Œìœ¼ë¡œ ìƒê´€ê´€ê³„ ê³„ì‚° (ë” ì •í™•í•œ ë¶„ì„)\n",
        "    non_zero_mask = (result_df[prediction_cols] > 0).all(axis=1)\n",
        "    if non_zero_mask.sum() > 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ\n",
        "        corr_data = result_df[non_zero_mask][prediction_cols]\n",
        "        corr_matrix = corr_data.corr()\n",
        "        \n",
        "        print(f\"   Prophet vs CatBoost: {corr_matrix.loc['prophet_prediction', 'catboost_prediction']:.3f}\")\n",
        "        print(f\"   Prophet vs Stacking: {corr_matrix.loc['prophet_prediction', 'stacking_prediction']:.3f}\")\n",
        "        print(f\"   CatBoost vs Stacking: {corr_matrix.loc['catboost_prediction', 'stacking_prediction']:.3f}\")\n",
        "        print(f\"   (0ì´ ì•„ë‹Œ {non_zero_mask.sum():,}ê°œ ë°ì´í„° ê¸°ì¤€)\")\n",
        "    else:\n",
        "        # ì „ì²´ ë°ì´í„°ë¡œ ê³„ì‚°\n",
        "        corr_matrix = result_df[prediction_cols].corr()\n",
        "        print(f\"   Prophet vs CatBoost: {corr_matrix.loc['prophet_prediction', 'catboost_prediction']:.3f}\")\n",
        "        print(f\"   Prophet vs Stacking: {corr_matrix.loc['prophet_prediction', 'stacking_prediction']:.3f}\")\n",
        "        print(f\"   CatBoost vs Stacking: {corr_matrix.loc['catboost_prediction', 'stacking_prediction']:.3f}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"   ìƒê´€ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# CSV íŒŒì¼ ì €ì¥\n",
        "result_filename = 'model_accurate_predictions.csv'\n",
        "result_df.to_csv(result_filename, index=False)\n",
        "print(f\"\\nğŸ“ ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {result_filename}\")\n",
        "\n",
        "# ì œì¶œìš© íŒŒì¼ ìƒì„± (ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ê²°ê³¼ë§Œ)\n",
        "submission_df = test_df[['tm', 'branch_id']].copy()\n",
        "submission_df['heat_demand'] = result_df['stacking_prediction']\n",
        "\n",
        "submission_filename = 'model_accurate_submission.csv'\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "print(f\"ğŸ“ ì œì¶œìš© íŒŒì¼ ì €ì¥: {submission_filename}\")\n",
        "\n",
        "# ê·¸ë£¹ë³„ ì˜ˆì¸¡ í†µê³„ (ì•ˆì „í•œ ë²„ì „)\n",
        "print(f\"\\nğŸ“Š ê·¸ë£¹ë³„ ì˜ˆì¸¡ í†µê³„ (ìŠ¤íƒœí‚¹ ëª¨ë¸):\")\n",
        "try:\n",
        "    # heating_season ê¸°ì¤€ìœ¼ë¡œë§Œ ê·¸ë£¹í™”\n",
        "    if 'heating_season' in result_df.columns:\n",
        "        season_stats = result_df.groupby('heating_season')['stacking_prediction'].agg([\n",
        "            'count', 'mean', 'std', 'min', 'max'\n",
        "        ]).round(2)\n",
        "        season_stats.index = ['ë¹„ë‚œë°©ì‹œì¦Œ', 'ë‚œë°©ì‹œì¦Œ']\n",
        "        print(season_stats)\n",
        "    \n",
        "    # ğŸ¯ ë¬¸ì œê°€ ë˜ì—ˆë˜ O~S ì§€ì‚¬ë“¤ íŠ¹ë³„ í™•ì¸\n",
        "    problem_branches = ['O', 'P', 'Q', 'R', 'S']\n",
        "    print(f\"\\nğŸ“ O~S ì§€ì‚¬ë³„ ë³µêµ¬ ìƒíƒœ:\")\n",
        "    \n",
        "    for branch in problem_branches:\n",
        "        branch_data = result_df[result_df['branch_id'] == branch]\n",
        "        if len(branch_data) > 0:\n",
        "            total_count = len(branch_data)\n",
        "            zero_count = (branch_data['stacking_prediction'] == 0).sum()\n",
        "            avg_pred = branch_data['stacking_prediction'].mean()\n",
        "            \n",
        "            # ì‹œì¦Œë³„ í‰ê· \n",
        "            heating_avg = branch_data[branch_data['heating_season'] == 1]['stacking_prediction'].mean()\n",
        "            non_heating_avg = branch_data[branch_data['heating_season'] == 0]['stacking_prediction'].mean()\n",
        "            \n",
        "            status = \"âœ…\" if zero_count == 0 else \"âš ï¸\" if zero_count < total_count * 0.05 else \"âŒ\"\n",
        "            \n",
        "            print(f\"   {status} ì§€ì‚¬ {branch}: í‰ê·  {avg_pred:6.2f} (ë‚œë°© {heating_avg:.1f}, ë¹„ë‚œë°© {non_heating_avg:.1f})\")\n",
        "            print(f\"      ì˜ê°’: {zero_count:,}ê°œ / {total_count:,}ê°œ ({zero_count/total_count*100:.1f}%)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"   ê·¸ë£¹ë³„ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ íŒ¨í„´ ë¶„ì„\n",
        "print(f\"\\nğŸ• ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡ íŒ¨í„´:\")\n",
        "try:\n",
        "    result_df['hour'] = pd.to_datetime(result_df['tm']).dt.hour\n",
        "    hourly_stats = result_df.groupby('hour')['stacking_prediction'].mean().round(1)\n",
        "    \n",
        "    # í”¼í¬ ì‹œê°„ëŒ€ ì°¾ê¸°\n",
        "    top_hours = hourly_stats.nlargest(5)\n",
        "    print(f\"   ìƒìœ„ 5ê°œ ì‹œê°„ëŒ€: {dict(top_hours)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "# í™˜ê²½ ê°ì§€ ë° Drive ì €ì¥\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        save_drive = input(\"\\nGoogle Driveì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").lower().strip()\n",
        "        if save_drive == 'y':\n",
        "            import os\n",
        "            os.system(f\"cp {result_filename} /content/drive/MyDrive/\")\n",
        "            os.system(f\"cp {submission_filename} /content/drive/MyDrive/\")\n",
        "            print(\"âœ… Google Drive ì €ì¥ ì™„ë£Œ!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Google Drive ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# ğŸ¯ ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
        "print(f\"\\nğŸŠ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: {len(result_df):,}ê°œ\")\n",
        "print(f\"ğŸ“ˆ ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡ê°’ ì‚¬ìš©ë¥ : {total_assigned:,}/{len(result_df):,} ({total_assigned/len(result_df)*100:.1f}%)\")\n",
        "print(f\"ğŸ“‰ ìµœì¢… ì˜ê°’: {final_zeros:,}ê°œ ({final_zeros/len(result_df)*100:.1f}%)\")\n",
        "print(f\"ğŸ“ ì €ì¥ëœ íŒŒì¼:\")\n",
        "print(f\"   - ìƒì„¸ ê²°ê³¼: {result_filename}\")\n",
        "print(f\"   - ì œì¶œìš©: {submission_filename}\")\n",
        "print(f\"ğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í‰ê·  ì˜ˆì¸¡ê°’: {result_df['stacking_prediction'].mean():.1f}\")\n",
        "print(f\"ğŸ† í›ˆë ¨ëœ ëª¨ë¸ì˜ ì‹¤ì œ ì˜ˆì¸¡ê°’ ê¸°ë°˜ ì œì¶œ ì¤€ë¹„ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ ì›ë³¸ test_df êµ¬ì¡° ìœ ì§€í•˜ë©° ì˜ˆì¸¡ê°’ ì •í™• ë§¤í•‘\n",
            "============================================================\n",
            "ğŸ“Š ì›ë³¸ test_df êµ¬ì¡°:\n",
            "   í¬ê¸°: (166915, 66)\n",
            "   ì¸ë±ìŠ¤ ë²”ìœ„: 0 ~ 97146\n",
            "   ì²« 5ê°œ í–‰ì˜ tmê³¼ branch_id:\n",
            "     [0] 2024-01-01 00:00:00 - A\n",
            "     [1] 2024-05-01 00:00:00 - A\n",
            "     [2] 2024-01-01 01:00:00 - A\n",
            "     [3] 2024-05-01 01:00:00 - A\n",
            "     [4] 2024-01-01 02:00:00 - A\n",
            "\n",
            "ğŸ“‹ ì›ë³¸ êµ¬ì¡° ê¸°ë°˜ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±:\n",
            "   âœ… ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ: (166915, 7)\n",
            "\n",
            "ğŸ”„ ê° í–‰ë³„ ì˜ˆì¸¡ê°’ ì •í™• ë§¤í•‘ ì¤‘...\n",
            "   ğŸ“Š heating ê·¸ë£¹ ë§¤í•‘ ì¤€ë¹„:\n",
            "     âœ… 97,147ê°œ í‚¤-ê°’ ë§¤í•‘ ìƒì„±\n",
            "   ğŸ“Š non_heating ê·¸ë£¹ ë§¤í•‘ ì¤€ë¹„:\n",
            "     âœ… 69,768ê°œ í‚¤-ê°’ ë§¤í•‘ ìƒì„±\n",
            "\n",
            "ğŸ¯ ì›ë³¸ ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì˜ˆì¸¡ê°’ í• ë‹¹:\n",
            "   ì²˜ë¦¬ ì¤‘: 0/166,915 (0.0%)\n",
            "   ì²˜ë¦¬ ì¤‘: 20,000/166,915 (12.0%)\n",
            "   ì²˜ë¦¬ ì¤‘: 40,000/166,915 (24.0%)\n",
            "   ì²˜ë¦¬ ì¤‘: 60,000/166,915 (35.9%)\n",
            "   ì²˜ë¦¬ ì¤‘: 80,000/166,915 (47.9%)\n",
            "   ì²˜ë¦¬ ì¤‘: 100,000/166,915 (59.9%)\n",
            "   ì²˜ë¦¬ ì¤‘: 120,000/166,915 (71.9%)\n",
            "   ì²˜ë¦¬ ì¤‘: 140,000/166,915 (83.9%)\n",
            "   ì²˜ë¦¬ ì¤‘: 160,000/166,915 (95.9%)\n",
            "\n",
            "ğŸ“Š ë§¤í•‘ ì™„ë£Œ í†µê³„:\n",
            "   ì „ì²´ í–‰: 166,915ê°œ\n",
            "   ë‚œë°©ì‹œì¦Œ ë§¤í•‘: 97,147ê°œ\n",
            "   ë¹„ë‚œë°©ì‹œì¦Œ ë§¤í•‘: 69,768ê°œ\n",
            "   ë§¤í•‘ ì‹¤íŒ¨: 0ê°œ\n",
            "   ì„±ê³µë¥ : 100.0%\n",
            "\n",
            "ğŸ”§ ë§¤í•‘ í›„ ë‚¨ì€ ì˜ê°’: 392ê°œ\n",
            "   ğŸ’¡ ì˜ê°’ í›„ì²˜ë¦¬ ì¤‘...\n",
            "   âœ… 392ê°œ ì˜ê°’ì„ ë™ì¼ ì¡°ê±´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´\n",
            "\n",
            "ğŸ“Š ìµœì¢… ê²°ê³¼:\n",
            "   ì „ì²´ ë°ì´í„°: 166,915ê°œ\n",
            "   ì˜ê°’: 0ê°œ (0.0%)\n",
            "   í‰ê·  ì˜ˆì¸¡ê°’: 92.99\n",
            "\n",
            "ğŸ“ O~S ì§€ì‚¬ë³„ ìµœì¢… ìƒíƒœ:\n",
            "   âœ… ì§€ì‚¬ O: í‰ê·   67.74 (ë‚œë°© 97.7, ë¹„ë‚œë°© 25.9)\n",
            "      ì˜ê°’: 0ê°œ / 8,785ê°œ (0.0%)\n",
            "   âœ… ì§€ì‚¬ P: í‰ê·   96.11 (ë‚œë°© 124.0, ë¹„ë‚œë°© 57.2)\n",
            "      ì˜ê°’: 0ê°œ / 8,785ê°œ (0.0%)\n",
            "   âœ… ì§€ì‚¬ Q: í‰ê·   54.50 (ë‚œë°© 78.1, ë¹„ë‚œë°© 21.6)\n",
            "      ì˜ê°’: 0ê°œ / 8,785ê°œ (0.0%)\n",
            "   âœ… ì§€ì‚¬ R: í‰ê·   14.37 (ë‚œë°© 18.3, ë¹„ë‚œë°© 8.9)\n",
            "      ì˜ê°’: 0ê°œ / 8,785ê°œ (0.0%)\n",
            "   âœ… ì§€ì‚¬ S: í‰ê·   12.54 (ë‚œë°© 15.8, ë¹„ë‚œë°© 8.1)\n",
            "      ì˜ê°’: 0ê°œ / 8,785ê°œ (0.0%)\n",
            "\n",
            "ğŸ” ì›ë³¸ ìˆœì„œ ìœ ì§€ í™•ì¸:\n",
            "   ì²« 10ê°œ í–‰ì˜ ì‹œê°„ê³¼ ì§€ì‚¬:\n",
            "     [0] 2024-01-01 00:00:00 - A âœ…\n",
            "     [1] 2024-05-01 00:00:00 - A âœ…\n",
            "     [2] 2024-01-01 01:00:00 - A âœ…\n",
            "     [3] 2024-05-01 01:00:00 - A âœ…\n",
            "     [4] 2024-01-01 02:00:00 - A âœ…\n",
            "     [5] 2024-05-01 02:00:00 - A âœ…\n",
            "     [6] 2024-01-01 03:00:00 - A âœ…\n",
            "     [7] 2024-05-01 03:00:00 - A âœ…\n",
            "     [8] 2024-01-01 04:00:00 - A âœ…\n",
            "     [9] 2024-05-01 04:00:00 - A âœ…\n",
            "\n",
            "ğŸ’¾ ì›ë³¸ êµ¬ì¡° ê¸°ë°˜ ê²°ê³¼ ì €ì¥:\n",
            "   ğŸ“ ìƒì„¸ ê²°ê³¼: original_structure_predictions.csv\n",
            "   ğŸ“ ì œì¶œìš©: original_structure_submission.csv\n",
            "\n",
            "ğŸ”„ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸:\n",
            "âœ… result_dfì™€ submission_df ì—…ë°ì´íŠ¸ ì™„ë£Œ!\n",
            "\n",
            "ğŸ‰ ì›ë³¸ test_df êµ¬ì¡° ê¸°ë°˜ ì˜ˆì¸¡ê°’ ë§¤í•‘ ì™„ë£Œ!\n",
            "ğŸ“‹ ìµœì¢… ê²°ê³¼:\n",
            "   âœ… ì›ë³¸ ì¸ë±ìŠ¤ ìˆœì„œ ì™„ë²½ ìœ ì§€\n",
            "   âœ… ê° í–‰ì— ì •í™•í•œ ì˜ˆì¸¡ê°’ ë§¤í•‘\n",
            "   âœ… ìš”ì²­ ì»¬ëŸ¼ êµ¬ì¡°: tm, branch_id, heating_season, stacking_prediction, prophet_prediction, catboost_prediction, hour\n",
            "   ğŸ¯ ìµœì¢… ì œì¶œ íŒŒì¼: original_structure_submission.csv\n",
            "\n",
            "ğŸ“‹ ìµœì¢… ê²°ê³¼ ìƒ˜í”Œ (ì²« 5ê°œ í–‰):\n",
            "                    tm branch_id  heating_season  stacking_prediction  \\\n",
            "0  2024-01-01 00:00:00         A               1                237.1   \n",
            "0  2024-05-01 00:00:00         A               0                 68.9   \n",
            "1  2024-01-01 01:00:00         A               1                222.2   \n",
            "1  2024-05-01 01:00:00         A               0                 59.6   \n",
            "2  2024-01-01 02:00:00         A               1                214.4   \n",
            "\n",
            "   prophet_prediction  catboost_prediction  hour  \n",
            "0               251.7                230.2     0  \n",
            "0                65.3                 67.0     0  \n",
            "1               239.0                215.4     1  \n",
            "1                60.7                 57.3     1  \n",
            "2               231.0                207.8     2  \n"
          ]
        }
      ],
      "source": [
        "# ğŸ¯ ì›ë³¸ test_df êµ¬ì¡°ì— ë§ì¶˜ ì˜ˆì¸¡ê°’ ì •í™• ë§¤í•‘\n",
        "print(\"ğŸ¯ ì›ë³¸ test_df êµ¬ì¡° ìœ ì§€í•˜ë©° ì˜ˆì¸¡ê°’ ì •í™• ë§¤í•‘\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. ì›ë³¸ test_df êµ¬ì¡° í™•ì¸\n",
        "print(\"ğŸ“Š ì›ë³¸ test_df êµ¬ì¡°:\")\n",
        "print(f\"   í¬ê¸°: {test_df.shape}\")\n",
        "print(f\"   ì¸ë±ìŠ¤ ë²”ìœ„: {test_df.index.min()} ~ {test_df.index.max()}\")\n",
        "print(f\"   ì²« 5ê°œ í–‰ì˜ tmê³¼ branch_id:\")\n",
        "for i in range(5):\n",
        "    print(f\"     [{i}] {test_df.iloc[i]['tm']} - {test_df.iloc[i]['branch_id']}\")\n",
        "\n",
        "# 2. ì›ë³¸ test_df ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "print(f\"\\nğŸ“‹ ì›ë³¸ êµ¬ì¡° ê¸°ë°˜ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±:\")\n",
        "\n",
        "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ë³µì‚¬\n",
        "result_df_original = test_df[['tm', 'branch_id']].copy()\n",
        "\n",
        "# heating_season ì»¬ëŸ¼ ì¶”ê°€ (ì—†ë‹¤ë©´ ìƒì„±)\n",
        "if 'heating_season' in test_df.columns:\n",
        "    result_df_original['heating_season'] = test_df['heating_season']\n",
        "else:\n",
        "    print(\"   ğŸ”§ heating_season ì»¬ëŸ¼ ìƒì„± ì¤‘...\")\n",
        "    result_df_original['heating_season'] = pd.to_datetime(result_df_original['tm']).dt.month.isin([10,11,12,1,2,3,4]).astype(int)\n",
        "\n",
        "# ì˜ˆì¸¡ê°’ ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
        "result_df_original['stacking_prediction'] = 0.0\n",
        "result_df_original['prophet_prediction'] = 0.0\n",
        "result_df_original['catboost_prediction'] = 0.0\n",
        "\n",
        "# hour ì»¬ëŸ¼ ì¶”ê°€\n",
        "result_df_original['hour'] = pd.to_datetime(result_df_original['tm']).dt.hour\n",
        "\n",
        "print(f\"   âœ… ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ: {result_df_original.shape}\")\n",
        "\n",
        "# 3. ê° í–‰ë³„ë¡œ ì •í™•í•œ ì˜ˆì¸¡ê°’ ë§¤í•‘\n",
        "print(f\"\\nğŸ”„ ê° í–‰ë³„ ì˜ˆì¸¡ê°’ ì •í™• ë§¤í•‘ ì¤‘...\")\n",
        "\n",
        "# ë§¤í•‘ í†µê³„\n",
        "mapping_stats = {\n",
        "    'total_rows': len(result_df_original),\n",
        "    'heating_mapped': 0,\n",
        "    'non_heating_mapped': 0,\n",
        "    'failed_mapping': 0\n",
        "}\n",
        "\n",
        "# ê° ê·¸ë£¹ë³„ë¡œ ì˜ˆì¸¡ê°’ ë§¤í•‘ ì¤€ë¹„\n",
        "group_prediction_maps = {}\n",
        "\n",
        "for group_name in ['heating', 'non_heating']:\n",
        "    if group_name in test_predictions and group_name in test_groups:\n",
        "        \n",
        "        print(f\"   ğŸ“Š {group_name} ê·¸ë£¹ ë§¤í•‘ ì¤€ë¹„:\")\n",
        "        \n",
        "        # ê·¸ë£¹ ë°ì´í„°ì™€ ì˜ˆì¸¡ê°’\n",
        "        group_data = test_groups[group_name]\n",
        "        group_stacking = np.array(test_predictions[group_name])\n",
        "        group_individual = individual_predictions.get(group_name, {})\n",
        "        \n",
        "        # ê·¸ë£¹ ë°ì´í„°ì˜ ê° í–‰ì„ (tm, branch_id) í‚¤ë¡œ ë§¤í•‘\n",
        "        group_map = {}\n",
        "        \n",
        "        for i, (idx, row) in enumerate(group_data.iterrows()):\n",
        "            if i < len(group_stacking):\n",
        "                key = (row['tm'], row['branch_id'])\n",
        "                \n",
        "                group_map[key] = {\n",
        "                    'stacking': group_stacking[i],\n",
        "                    'prophet': group_individual.get('prophet', [0])[i] if i < len(group_individual.get('prophet', [])) else 0,\n",
        "                    'catboost': group_individual.get('catboost', [0])[i] if i < len(group_individual.get('catboost', [])) else 0\n",
        "                }\n",
        "        \n",
        "        group_prediction_maps[group_name] = group_map\n",
        "        print(f\"     âœ… {len(group_map):,}ê°œ í‚¤-ê°’ ë§¤í•‘ ìƒì„±\")\n",
        "\n",
        "# 4. ì›ë³¸ test_dfì˜ ê° í–‰ì— ëŒ€í•´ ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "print(f\"\\nğŸ¯ ì›ë³¸ ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì˜ˆì¸¡ê°’ í• ë‹¹:\")\n",
        "\n",
        "for idx in range(len(result_df_original)):\n",
        "    if idx % 20000 == 0:  # ì§„í–‰ ìƒí™© í‘œì‹œ\n",
        "        print(f\"   ì²˜ë¦¬ ì¤‘: {idx:,}/{len(result_df_original):,} ({idx/len(result_df_original)*100:.1f}%)\")\n",
        "    \n",
        "    # í˜„ì¬ í–‰ì˜ ì •ë³´\n",
        "    tm = result_df_original.iloc[idx]['tm']\n",
        "    branch_id = result_df_original.iloc[idx]['branch_id']\n",
        "    heating_season = result_df_original.iloc[idx]['heating_season']\n",
        "    \n",
        "    # ê·¸ë£¹ ê²°ì •\n",
        "    group_name = 'heating' if heating_season == 1 else 'non_heating'\n",
        "    \n",
        "    # í•´ë‹¹ ê·¸ë£¹ì˜ ë§¤í•‘ì—ì„œ ì˜ˆì¸¡ê°’ ì°¾ê¸°\n",
        "    if group_name in group_prediction_maps:\n",
        "        key = (tm, branch_id)\n",
        "        \n",
        "        if key in group_prediction_maps[group_name]:\n",
        "            predictions = group_prediction_maps[group_name][key]\n",
        "            \n",
        "            # ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "            result_df_original.iloc[idx, result_df_original.columns.get_loc('stacking_prediction')] = predictions['stacking']\n",
        "            result_df_original.iloc[idx, result_df_original.columns.get_loc('prophet_prediction')] = predictions['prophet']\n",
        "            result_df_original.iloc[idx, result_df_original.columns.get_loc('catboost_prediction')] = predictions['catboost']\n",
        "            \n",
        "            # í†µê³„ ì—…ë°ì´íŠ¸\n",
        "            if heating_season == 1:\n",
        "                mapping_stats['heating_mapped'] += 1\n",
        "            else:\n",
        "                mapping_stats['non_heating_mapped'] += 1\n",
        "        else:\n",
        "            mapping_stats['failed_mapping'] += 1\n",
        "\n",
        "print(f\"\\nğŸ“Š ë§¤í•‘ ì™„ë£Œ í†µê³„:\")\n",
        "print(f\"   ì „ì²´ í–‰: {mapping_stats['total_rows']:,}ê°œ\")\n",
        "print(f\"   ë‚œë°©ì‹œì¦Œ ë§¤í•‘: {mapping_stats['heating_mapped']:,}ê°œ\")\n",
        "print(f\"   ë¹„ë‚œë°©ì‹œì¦Œ ë§¤í•‘: {mapping_stats['non_heating_mapped']:,}ê°œ\")\n",
        "print(f\"   ë§¤í•‘ ì‹¤íŒ¨: {mapping_stats['failed_mapping']:,}ê°œ\")\n",
        "\n",
        "total_mapped = mapping_stats['heating_mapped'] + mapping_stats['non_heating_mapped']\n",
        "success_rate = total_mapped / mapping_stats['total_rows'] * 100\n",
        "print(f\"   ì„±ê³µë¥ : {success_rate:.1f}%\")\n",
        "\n",
        "# 5. ìŒìˆ˜ê°’ ì œê±° ë° ë°˜ì˜¬ë¦¼\n",
        "result_df_original['stacking_prediction'] = np.maximum(result_df_original['stacking_prediction'], 0).round(1)\n",
        "result_df_original['prophet_prediction'] = np.maximum(result_df_original['prophet_prediction'], 0).round(1)\n",
        "result_df_original['catboost_prediction'] = np.maximum(result_df_original['catboost_prediction'], 0).round(1)\n",
        "\n",
        "# 6. ë§¤í•‘ ì‹¤íŒ¨í•œ ì˜ê°’ë“¤ ì²˜ë¦¬\n",
        "remaining_zeros = (result_df_original['stacking_prediction'] == 0).sum()\n",
        "print(f\"\\nğŸ”§ ë§¤í•‘ í›„ ë‚¨ì€ ì˜ê°’: {remaining_zeros:,}ê°œ\")\n",
        "\n",
        "if remaining_zeros > 0:\n",
        "    print(\"   ğŸ’¡ ì˜ê°’ í›„ì²˜ë¦¬ ì¤‘...\")\n",
        "    \n",
        "    # ì§€ì‚¬ë³„, ì‹œì¦Œë³„ í‰ê· ìœ¼ë¡œ ëŒ€ì²´\n",
        "    fixed_count = 0\n",
        "    \n",
        "    for branch in result_df_original['branch_id'].unique():\n",
        "        for season in [0, 1]:\n",
        "            # í•´ë‹¹ ì¡°ê±´ì˜ ì˜ê°’ë“¤\n",
        "            zero_mask = (\n",
        "                (result_df_original['branch_id'] == branch) & \n",
        "                (result_df_original['heating_season'] == season) & \n",
        "                (result_df_original['stacking_prediction'] == 0)\n",
        "            )\n",
        "            zero_count = zero_mask.sum()\n",
        "            \n",
        "            if zero_count > 0:\n",
        "                # ê°™ì€ ì¡°ê±´ì˜ 0ì´ ì•„ë‹Œ ê°’ë“¤ì˜ í‰ê· \n",
        "                nonzero_mask = (\n",
        "                    (result_df_original['branch_id'] == branch) & \n",
        "                    (result_df_original['heating_season'] == season) & \n",
        "                    (result_df_original['stacking_prediction'] > 0)\n",
        "                )\n",
        "                nonzero_values = result_df_original[nonzero_mask]['stacking_prediction']\n",
        "                \n",
        "                if len(nonzero_values) > 0:\n",
        "                    avg_val = nonzero_values.mean()\n",
        "                    result_df_original.loc[zero_mask, 'stacking_prediction'] = avg_val\n",
        "                    fixed_count += zero_count\n",
        "    \n",
        "    print(f\"   âœ… {fixed_count:,}ê°œ ì˜ê°’ì„ ë™ì¼ ì¡°ê±´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´\")\n",
        "\n",
        "# 7. ìµœì¢… ê²°ê³¼ í™•ì¸\n",
        "final_zeros = (result_df_original['stacking_prediction'] == 0).sum()\n",
        "print(f\"\\nğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
        "print(f\"   ì „ì²´ ë°ì´í„°: {len(result_df_original):,}ê°œ\")\n",
        "print(f\"   ì˜ê°’: {final_zeros:,}ê°œ ({final_zeros/len(result_df_original)*100:.1f}%)\")\n",
        "print(f\"   í‰ê·  ì˜ˆì¸¡ê°’: {result_df_original['stacking_prediction'].mean():.2f}\")\n",
        "\n",
        "# 8. O~S ì§€ì‚¬ë³„ ê²°ê³¼ í™•ì¸\n",
        "print(f\"\\nğŸ“ O~S ì§€ì‚¬ë³„ ìµœì¢… ìƒíƒœ:\")\n",
        "problem_branches = ['O', 'P', 'Q', 'R', 'S']\n",
        "\n",
        "for branch in problem_branches:\n",
        "    branch_data = result_df_original[result_df_original['branch_id'] == branch]\n",
        "    \n",
        "    if len(branch_data) > 0:\n",
        "        zero_count = (branch_data['stacking_prediction'] == 0).sum()\n",
        "        avg_pred = branch_data['stacking_prediction'].mean()\n",
        "        \n",
        "        # ì‹œì¦Œë³„ í‰ê· \n",
        "        heating_avg = branch_data[branch_data['heating_season'] == 1]['stacking_prediction'].mean()\n",
        "        non_heating_avg = branch_data[branch_data['heating_season'] == 0]['stacking_prediction'].mean()\n",
        "        \n",
        "        status = \"âœ…\" if zero_count == 0 else \"âš ï¸\" if zero_count < len(branch_data) * 0.1 else \"âŒ\"\n",
        "        \n",
        "        print(f\"   {status} ì§€ì‚¬ {branch}: í‰ê·  {avg_pred:6.2f} (ë‚œë°© {heating_avg:.1f}, ë¹„ë‚œë°© {non_heating_avg:.1f})\")\n",
        "        print(f\"      ì˜ê°’: {zero_count:,}ê°œ / {len(branch_data):,}ê°œ ({zero_count/len(branch_data)*100:.1f}%)\")\n",
        "\n",
        "# 9. ì›ë³¸ ìˆœì„œ í™•ì¸\n",
        "print(f\"\\nğŸ” ì›ë³¸ ìˆœì„œ ìœ ì§€ í™•ì¸:\")\n",
        "print(\"   ì²« 10ê°œ í–‰ì˜ ì‹œê°„ê³¼ ì§€ì‚¬:\")\n",
        "for i in range(10):\n",
        "    original_tm = test_df.iloc[i]['tm']\n",
        "    original_branch = test_df.iloc[i]['branch_id']\n",
        "    result_tm = result_df_original.iloc[i]['tm']\n",
        "    result_branch = result_df_original.iloc[i]['branch_id']\n",
        "    \n",
        "    match = \"âœ…\" if (original_tm == result_tm and original_branch == result_branch) else \"âŒ\"\n",
        "    print(f\"     [{i}] {original_tm} - {original_branch} {match}\")\n",
        "\n",
        "# 10. íŒŒì¼ ì €ì¥\n",
        "print(f\"\\nğŸ’¾ ì›ë³¸ êµ¬ì¡° ê¸°ë°˜ ê²°ê³¼ ì €ì¥:\")\n",
        "\n",
        "# ìƒì„¸ ê²°ê³¼ (ì›ë³¸ êµ¬ì¡° ìœ ì§€)\n",
        "original_structure_filename = 'original_structure_predictions.csv'\n",
        "result_df_original.to_csv(original_structure_filename, index=False)\n",
        "print(f\"   ğŸ“ ìƒì„¸ ê²°ê³¼: {original_structure_filename}\")\n",
        "\n",
        "# ì œì¶œìš© íŒŒì¼\n",
        "submission_original = result_df_original[['tm', 'branch_id']].copy()\n",
        "submission_original['heat_demand'] = result_df_original['stacking_prediction']\n",
        "\n",
        "original_submission_filename = 'original_structure_submission.csv'\n",
        "submission_original.to_csv(original_submission_filename, index=False)\n",
        "print(f\"   ğŸ“ ì œì¶œìš©: {original_submission_filename}\")\n",
        "\n",
        "# 11. ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸\n",
        "print(f\"\\nğŸ”„ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸:\")\n",
        "result_df = result_df_original.copy()\n",
        "submission_df = submission_original.copy()\n",
        "\n",
        "print(\"âœ… result_dfì™€ submission_df ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
        "\n",
        "print(f\"\\nğŸ‰ ì›ë³¸ test_df êµ¬ì¡° ê¸°ë°˜ ì˜ˆì¸¡ê°’ ë§¤í•‘ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“‹ ìµœì¢… ê²°ê³¼:\")\n",
        "print(f\"   âœ… ì›ë³¸ ì¸ë±ìŠ¤ ìˆœì„œ ì™„ë²½ ìœ ì§€\")\n",
        "print(f\"   âœ… ê° í–‰ì— ì •í™•í•œ ì˜ˆì¸¡ê°’ ë§¤í•‘\")\n",
        "print(f\"   âœ… ìš”ì²­ ì»¬ëŸ¼ êµ¬ì¡°: tm, branch_id, heating_season, stacking_prediction, prophet_prediction, catboost_prediction, hour\")\n",
        "print(f\"   ğŸ¯ ìµœì¢… ì œì¶œ íŒŒì¼: {original_submission_filename}\")\n",
        "\n",
        "# ìƒ˜í”Œ í™•ì¸\n",
        "print(f\"\\nğŸ“‹ ìµœì¢… ê²°ê³¼ ìƒ˜í”Œ (ì²« 5ê°œ í–‰):\")\n",
        "print(result_df_original[['tm', 'branch_id', 'heating_season', 'stacking_prediction', 'prophet_prediction', 'catboost_prediction', 'hour']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ê¸°ì¡´ CSV íŒŒì¼ ì§€ì‚¬ë³„ ì‹œê°„ìˆœ ì •ë ¬\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "1ï¸âƒ£ ìƒì„¸ ì˜ˆì¸¡ íŒŒì¼ ì •ë ¬\n",
            "\n",
            "ğŸ“ ìƒì„¸ ì˜ˆì¸¡ íŒŒì¼ ì²˜ë¦¬: original_structure_predictions.csv\n",
            "   âœ… íŒŒì¼ ì½ê¸° ì™„ë£Œ: 166,915í–‰ Ã— 7ì—´\n",
            "   ğŸ“‹ ì»¬ëŸ¼: ['tm', 'branch_id', 'heating_season', 'stacking_prediction', 'prophet_prediction', 'catboost_prediction', 'hour']\n",
            "   ğŸ”§ tm ì»¬ëŸ¼ íƒ€ì…: object\n",
            "   âš™ï¸ datetime ë³€í™˜ ì¤‘...\n",
            "   âœ… ë³€í™˜ ì™„ë£Œ: datetime64[ns]\n",
            "   ğŸ“Š ì •ë ¬ ì „ ìƒíƒœ:\n",
            "      ì´ í–‰ìˆ˜: 166,915ê°œ\n",
            "      ê³ ìœ  ì§€ì‚¬: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S']\n",
            "      ì‹œê°„ ë²”ìœ„: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00\n",
            "      ì§€ì‚¬ë³„ ë°ì´í„° ê°œìˆ˜:\n",
            "        ì§€ì‚¬ A: 8,785ê°œ\n",
            "        ì§€ì‚¬ B: 8,785ê°œ\n",
            "        ì§€ì‚¬ C: 8,785ê°œ\n",
            "        ì§€ì‚¬ D: 8,785ê°œ\n",
            "        ì§€ì‚¬ E: 8,785ê°œ\n",
            "        ì§€ì‚¬ F: 8,785ê°œ\n",
            "        ì§€ì‚¬ G: 8,785ê°œ\n",
            "        ì§€ì‚¬ H: 8,785ê°œ\n",
            "        ì§€ì‚¬ I: 8,785ê°œ\n",
            "        ì§€ì‚¬ J: 8,785ê°œ\n",
            "        ì§€ì‚¬ K: 8,785ê°œ\n",
            "        ì§€ì‚¬ L: 8,785ê°œ\n",
            "        ì§€ì‚¬ M: 8,785ê°œ\n",
            "        ì§€ì‚¬ N: 8,785ê°œ\n",
            "        ì§€ì‚¬ O: 8,785ê°œ\n",
            "        ì§€ì‚¬ P: 8,785ê°œ\n",
            "        ì§€ì‚¬ Q: 8,785ê°œ\n",
            "        ì§€ì‚¬ R: 8,785ê°œ\n",
            "        ì§€ì‚¬ S: 8,785ê°œ\n",
            "   ğŸ¯ ì •ë ¬ ì‹¤í–‰: branch_id â†’ tm\n",
            "   âœ… ì •ë ¬ ì™„ë£Œ!\n",
            "   ğŸ” ì •ë ¬ ê²°ê³¼ í™•ì¸ (ì²« 10ê°œ):\n",
            "       0: 2024-01-01 00:00:00 - ì§€ì‚¬A - 237.1\n",
            "       1: 2024-01-01 01:00:00 - ì§€ì‚¬A - 222.2\n",
            "       2: 2024-01-01 02:00:00 - ì§€ì‚¬A - 214.4\n",
            "       3: 2024-01-01 03:00:00 - ì§€ì‚¬A - 210.6\n",
            "       4: 2024-01-01 04:00:00 - ì§€ì‚¬A - 211.3\n",
            "       5: 2024-01-01 05:00:00 - ì§€ì‚¬A - 216.8\n",
            "       6: 2024-01-01 06:00:00 - ì§€ì‚¬A - 218.5\n",
            "       7: 2024-01-01 07:00:00 - ì§€ì‚¬A - 235.9\n",
            "       8: 2024-01-01 08:00:00 - ì§€ì‚¬A - 247.1\n",
            "       9: 2024-01-01 09:00:00 - ì§€ì‚¬A - 247.8\n",
            "   ğŸ“‹ ì§€ì‚¬ë³„ ë°ì´í„° íŒ¨í„´:\n",
            "      ì§€ì‚¬ A: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00 (8,785ê°œ)\n",
            "      ì§€ì‚¬ B: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00 (8,785ê°œ)\n",
            "      ì§€ì‚¬ C: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00 (8,785ê°œ)\n",
            "\n",
            "============================================================\n",
            "2ï¸âƒ£ ì œì¶œìš© íŒŒì¼ ì •ë ¬\n",
            "\n",
            "ğŸ“ ì œì¶œìš© íŒŒì¼ ì²˜ë¦¬: original_structure_submission.csv\n",
            "   âœ… íŒŒì¼ ì½ê¸° ì™„ë£Œ: 166,915í–‰ Ã— 3ì—´\n",
            "   ğŸ“‹ ì»¬ëŸ¼: ['tm', 'branch_id', 'heat_demand']\n",
            "   ğŸ”§ tm ì»¬ëŸ¼ íƒ€ì…: object\n",
            "   âš™ï¸ datetime ë³€í™˜ ì¤‘...\n",
            "   âœ… ë³€í™˜ ì™„ë£Œ: datetime64[ns]\n",
            "   ğŸ“Š ì •ë ¬ ì „ ìƒíƒœ:\n",
            "      ì´ í–‰ìˆ˜: 166,915ê°œ\n",
            "      ê³ ìœ  ì§€ì‚¬: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S']\n",
            "      ì‹œê°„ ë²”ìœ„: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00\n",
            "      ì§€ì‚¬ë³„ ë°ì´í„° ê°œìˆ˜:\n",
            "        ì§€ì‚¬ A: 8,785ê°œ\n",
            "        ì§€ì‚¬ B: 8,785ê°œ\n",
            "        ì§€ì‚¬ C: 8,785ê°œ\n",
            "        ì§€ì‚¬ D: 8,785ê°œ\n",
            "        ì§€ì‚¬ E: 8,785ê°œ\n",
            "        ì§€ì‚¬ F: 8,785ê°œ\n",
            "        ì§€ì‚¬ G: 8,785ê°œ\n",
            "        ì§€ì‚¬ H: 8,785ê°œ\n",
            "        ì§€ì‚¬ I: 8,785ê°œ\n",
            "        ì§€ì‚¬ J: 8,785ê°œ\n",
            "        ì§€ì‚¬ K: 8,785ê°œ\n",
            "        ì§€ì‚¬ L: 8,785ê°œ\n",
            "        ì§€ì‚¬ M: 8,785ê°œ\n",
            "        ì§€ì‚¬ N: 8,785ê°œ\n",
            "        ì§€ì‚¬ O: 8,785ê°œ\n",
            "        ì§€ì‚¬ P: 8,785ê°œ\n",
            "        ì§€ì‚¬ Q: 8,785ê°œ\n",
            "        ì§€ì‚¬ R: 8,785ê°œ\n",
            "        ì§€ì‚¬ S: 8,785ê°œ\n",
            "   ğŸ¯ ì •ë ¬ ì‹¤í–‰: branch_id â†’ tm\n",
            "   âœ… ì •ë ¬ ì™„ë£Œ!\n",
            "   ğŸ” ì •ë ¬ ê²°ê³¼ í™•ì¸ (ì²« 10ê°œ):\n",
            "       0: 2024-01-01 00:00:00 - ì§€ì‚¬A - 237.1\n",
            "       1: 2024-01-01 01:00:00 - ì§€ì‚¬A - 222.2\n",
            "       2: 2024-01-01 02:00:00 - ì§€ì‚¬A - 214.4\n",
            "       3: 2024-01-01 03:00:00 - ì§€ì‚¬A - 210.6\n",
            "       4: 2024-01-01 04:00:00 - ì§€ì‚¬A - 211.3\n",
            "       5: 2024-01-01 05:00:00 - ì§€ì‚¬A - 216.8\n",
            "       6: 2024-01-01 06:00:00 - ì§€ì‚¬A - 218.5\n",
            "       7: 2024-01-01 07:00:00 - ì§€ì‚¬A - 235.9\n",
            "       8: 2024-01-01 08:00:00 - ì§€ì‚¬A - 247.1\n",
            "       9: 2024-01-01 09:00:00 - ì§€ì‚¬A - 247.8\n",
            "   ğŸ“‹ ì§€ì‚¬ë³„ ë°ì´í„° íŒ¨í„´:\n",
            "      ì§€ì‚¬ A: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00 (8,785ê°œ)\n",
            "      ì§€ì‚¬ B: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00 (8,785ê°œ)\n",
            "      ì§€ì‚¬ C: 2024-01-01 00:00:00 ~ 2025-01-01 00:00:00 (8,785ê°œ)\n",
            "\n",
            "============================================================\n",
            "ğŸ’¾ ì •ë ¬ëœ íŒŒì¼ ì €ì¥\n",
            "   âœ… ìƒì„¸ ì˜ˆì¸¡ íŒŒì¼ ì €ì¥: sorted_original_structure_predictions.csv\n",
            "   âœ… ì›ë³¸ íŒŒì¼ ì—…ë°ì´íŠ¸: original_structure_predictions.csv\n",
            "   âœ… ì œì¶œìš© íŒŒì¼ ì €ì¥: sorted_original_structure_submission.csv\n",
            "   âœ… ì›ë³¸ íŒŒì¼ ì—…ë°ì´íŠ¸: original_structure_submission.csv\n",
            "\n",
            "ğŸ”„ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸:\n",
            "   âœ… result_df ì—…ë°ì´íŠ¸ ì™„ë£Œ\n",
            "   âœ… submission_df ì—…ë°ì´íŠ¸ ì™„ë£Œ\n",
            "\n",
            "ğŸ‰ ëª¨ë“  íŒŒì¼ ì •ë ¬ ì™„ë£Œ!\n",
            "ğŸ“ ìƒì„±ëœ íŒŒì¼:\n",
            "   ğŸ“‹ sorted_original_structure_predictions.csv\n",
            "   ğŸ“‹ original_structure_predictions.csv (ì—…ë°ì´íŠ¸ë¨)\n",
            "   ğŸ“‹ sorted_original_structure_submission.csv\n",
            "   ğŸ“‹ original_structure_submission.csv (ì—…ë°ì´íŠ¸ë¨)\n",
            "\n",
            "âœ¨ ì •ë ¬ ë°©ì‹: ì§€ì‚¬ë³„(A,B,C...) â†’ ì‹œê°„ìˆœ(ë‚ ì§œ/ì‹œê°„)\n",
            "ğŸ“Š ëª¨ë“  ì§€ì‚¬ì˜ ëª¨ë“  ì‹œê°„ ë°ì´í„°ê°€ ì§€ì‚¬ë³„ë¡œ ì—°ì† ë°°ì¹˜ë¨\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ğŸ”„ ê¸°ì¡´ CSV íŒŒì¼ë“¤ì„ ì§€ì‚¬ë³„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "print(\"ğŸ”„ ê¸°ì¡´ CSV íŒŒì¼ ì§€ì‚¬ë³„ ì‹œê°„ìˆœ ì •ë ¬\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# íŒŒì¼ ê²½ë¡œ\n",
        "detail_file = 'original_structure_predictions.csv'\n",
        "submission_file = 'original_structure_submission.csv'\n",
        "\n",
        "def sort_csv_by_branch_and_time(file_path, file_type):\n",
        "    \"\"\"CSV íŒŒì¼ì„ ì§€ì‚¬ë³„, ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    \n",
        "    print(f\"\\nğŸ“ {file_type} íŒŒì¼ ì²˜ë¦¬: {file_path}\")\n",
        "    \n",
        "    try:\n",
        "        # 1. íŒŒì¼ ì½ê¸°\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"   âœ… íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(df):,}í–‰ Ã— {len(df.columns)}ì—´\")\n",
        "        print(f\"   ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}\")\n",
        "        \n",
        "        # 2. tm ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜\n",
        "        print(f\"   ğŸ”§ tm ì»¬ëŸ¼ íƒ€ì…: {df['tm'].dtype}\")\n",
        "        if df['tm'].dtype == 'object':\n",
        "            print(\"   âš™ï¸ datetime ë³€í™˜ ì¤‘...\")\n",
        "            df['tm'] = pd.to_datetime(df['tm'])\n",
        "            print(f\"   âœ… ë³€í™˜ ì™„ë£Œ: {df['tm'].dtype}\")\n",
        "        \n",
        "        # 3. ì •ë ¬ ì „ ìƒíƒœ í™•ì¸\n",
        "        print(f\"   ğŸ“Š ì •ë ¬ ì „ ìƒíƒœ:\")\n",
        "        print(f\"      ì´ í–‰ìˆ˜: {len(df):,}ê°œ\")\n",
        "        print(f\"      ê³ ìœ  ì§€ì‚¬: {sorted(df['branch_id'].unique())}\")\n",
        "        print(f\"      ì‹œê°„ ë²”ìœ„: {df['tm'].min()} ~ {df['tm'].max()}\")\n",
        "        \n",
        "        # ì§€ì‚¬ë³„ ë°ì´í„° ê°œìˆ˜\n",
        "        branch_counts = df['branch_id'].value_counts().sort_index()\n",
        "        print(f\"      ì§€ì‚¬ë³„ ë°ì´í„° ê°œìˆ˜:\")\n",
        "        for branch, count in branch_counts.items():\n",
        "            print(f\"        ì§€ì‚¬ {branch}: {count:,}ê°œ\")\n",
        "        \n",
        "        # 4. ì§€ì‚¬ë³„, ì‹œê°„ìˆœ ì •ë ¬\n",
        "        print(f\"   ğŸ¯ ì •ë ¬ ì‹¤í–‰: branch_id â†’ tm\")\n",
        "        df_sorted = df.sort_values(['branch_id', 'tm'], ascending=[True, True])\n",
        "        df_sorted = df_sorted.reset_index(drop=True)\n",
        "        print(f\"   âœ… ì •ë ¬ ì™„ë£Œ!\")\n",
        "        \n",
        "        # 5. ì •ë ¬ ê²°ê³¼ í™•ì¸\n",
        "        print(f\"   ğŸ” ì •ë ¬ ê²°ê³¼ í™•ì¸ (ì²« 10ê°œ):\")\n",
        "        for i in range(min(10, len(df_sorted))):\n",
        "            tm = df_sorted.iloc[i]['tm']\n",
        "            branch = df_sorted.iloc[i]['branch_id']\n",
        "            if 'heat_demand' in df_sorted.columns:\n",
        "                value = df_sorted.iloc[i]['heat_demand']\n",
        "                print(f\"      {i:2d}: {tm} - ì§€ì‚¬{branch} - {value:.1f}\")\n",
        "            elif 'stacking_prediction' in df_sorted.columns:\n",
        "                value = df_sorted.iloc[i]['stacking_prediction']\n",
        "                print(f\"      {i:2d}: {tm} - ì§€ì‚¬{branch} - {value:.1f}\")\n",
        "            else:\n",
        "                print(f\"      {i:2d}: {tm} - ì§€ì‚¬{branch}\")\n",
        "        \n",
        "        # 6. ì§€ì‚¬ë³„ íŒ¨í„´ í™•ì¸\n",
        "        print(f\"   ğŸ“‹ ì§€ì‚¬ë³„ ë°ì´í„° íŒ¨í„´:\")\n",
        "        for branch in sorted(df_sorted['branch_id'].unique())[:3]:  # ì²˜ìŒ 3ê°œ ì§€ì‚¬ë§Œ\n",
        "            branch_data = df_sorted[df_sorted['branch_id'] == branch]\n",
        "            first_time = branch_data['tm'].iloc[0]\n",
        "            last_time = branch_data['tm'].iloc[-1]\n",
        "            print(f\"      ì§€ì‚¬ {branch}: {first_time} ~ {last_time} ({len(branch_data):,}ê°œ)\")\n",
        "        \n",
        "        return df_sorted\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"   âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return None\n",
        "\n",
        "# 1. ìƒì„¸ ì˜ˆì¸¡ íŒŒì¼ ì •ë ¬\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"1ï¸âƒ£ ìƒì„¸ ì˜ˆì¸¡ íŒŒì¼ ì •ë ¬\")\n",
        "sorted_detail_df = sort_csv_by_branch_and_time(detail_file, \"ìƒì„¸ ì˜ˆì¸¡\")\n",
        "\n",
        "# 2. ì œì¶œìš© íŒŒì¼ ì •ë ¬  \n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"2ï¸âƒ£ ì œì¶œìš© íŒŒì¼ ì •ë ¬\")\n",
        "sorted_submission_df = sort_csv_by_branch_and_time(submission_file, \"ì œì¶œìš©\")\n",
        "\n",
        "# 3. ì •ë ¬ëœ íŒŒì¼ ì €ì¥\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ’¾ ì •ë ¬ëœ íŒŒì¼ ì €ì¥\")\n",
        "\n",
        "if sorted_detail_df is not None:\n",
        "    # ìƒˆë¡œìš´ íŒŒì¼ëª… ìƒì„±\n",
        "    sorted_detail_file = 'sorted_' + detail_file\n",
        "    sorted_detail_df.to_csv(sorted_detail_file, index=False)\n",
        "    print(f\"   âœ… ìƒì„¸ ì˜ˆì¸¡ íŒŒì¼ ì €ì¥: {sorted_detail_file}\")\n",
        "    \n",
        "    # ì›ë³¸ íŒŒì¼ë„ ë®ì–´ì“°ê¸°\n",
        "    sorted_detail_df.to_csv(detail_file, index=False)\n",
        "    print(f\"   âœ… ì›ë³¸ íŒŒì¼ ì—…ë°ì´íŠ¸: {detail_file}\")\n",
        "\n",
        "if sorted_submission_df is not None:\n",
        "    # ìƒˆë¡œìš´ íŒŒì¼ëª… ìƒì„±\n",
        "    sorted_submission_file = 'sorted_' + submission_file\n",
        "    sorted_submission_df.to_csv(sorted_submission_file, index=False)\n",
        "    print(f\"   âœ… ì œì¶œìš© íŒŒì¼ ì €ì¥: {sorted_submission_file}\")\n",
        "    \n",
        "    # ì›ë³¸ íŒŒì¼ë„ ë®ì–´ì“°ê¸°\n",
        "    sorted_submission_df.to_csv(submission_file, index=False)\n",
        "    print(f\"   âœ… ì›ë³¸ íŒŒì¼ ì—…ë°ì´íŠ¸: {submission_file}\")\n",
        "\n",
        "# 4. ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ìˆë‹¤ë©´)\n",
        "print(f\"\\nğŸ”„ ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸:\")\n",
        "if sorted_detail_df is not None:\n",
        "    try:\n",
        "        result_df = sorted_detail_df.copy()\n",
        "        print(\"   âœ… result_df ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
        "    except:\n",
        "        print(\"   âš ï¸ result_df ì—…ë°ì´íŠ¸ ë¶ˆê°€ (ë³€ìˆ˜ ì—†ìŒ)\")\n",
        "\n",
        "if sorted_submission_df is not None:\n",
        "    try:\n",
        "        submission_df = sorted_submission_df.copy()\n",
        "        print(\"   âœ… submission_df ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
        "    except:\n",
        "        print(\"   âš ï¸ submission_df ì—…ë°ì´íŠ¸ ë¶ˆê°€ (ë³€ìˆ˜ ì—†ìŒ)\")\n",
        "\n",
        "# 5. ìµœì¢… ìš”ì•½\n",
        "print(f\"\\nğŸ‰ ëª¨ë“  íŒŒì¼ ì •ë ¬ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“ ìƒì„±ëœ íŒŒì¼:\")\n",
        "if sorted_detail_df is not None:\n",
        "    print(f\"   ğŸ“‹ sorted_{detail_file}\")\n",
        "    print(f\"   ğŸ“‹ {detail_file} (ì—…ë°ì´íŠ¸ë¨)\")\n",
        "if sorted_submission_df is not None:\n",
        "    print(f\"   ğŸ“‹ sorted_{submission_file}\")\n",
        "    print(f\"   ğŸ“‹ {submission_file} (ì—…ë°ì´íŠ¸ë¨)\")\n",
        "\n",
        "print(f\"\\nâœ¨ ì •ë ¬ ë°©ì‹: ì§€ì‚¬ë³„(A,B,C...) â†’ ì‹œê°„ìˆœ(ë‚ ì§œ/ì‹œê°„)\")\n",
        "print(f\"ğŸ“Š ëª¨ë“  ì§€ì‚¬ì˜ ëª¨ë“  ì‹œê°„ ë°ì´í„°ê°€ ì§€ì‚¬ë³„ë¡œ ì—°ì† ë°°ì¹˜ë¨\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
